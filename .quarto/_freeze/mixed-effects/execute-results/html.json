{
  "hash": "b005007d18650657d0df7116e5f72fb2",
  "result": {
    "engine": "knitr",
    "markdown": "# Linear mixed-effects (1)\n\n\n\n\n\n\n\n\n\n:::{.info}\n\nSome ideas in this chapter come from the textbook [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/) by @McElreath_2020, This chapter also borrows extensively from Tristan Mahr's [excellent blog post on partial pooling](https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/).\n\n:::\n\n## The `sleepstudy` dataset\n\nIn this chapter, we'll be working with some real data from a study looking at the effects of sleep deprivation on psychomotor performance [@Belenky_et_al_2003]. Data from this study is included as the built-in dataset `sleepstudy` in the `lme4` package for R [@Bates_et_al_2015].\n\nLet's start by looking at the documentation for the `sleepstudy` dataset. After loading the **`lme4`** package, you can access the documentation by typing `?sleepstudy` in the console.\n\n```\nsleepstudy                package:lme4                 R Documentation\n\nReaction times in a sleep deprivation study\n\nDescription:\n\n     The average reaction time per day (in milliseconds) for subjects\n     in a sleep deprivation study.\n\n     Days 0-1 were adaptation and training (T1/T2), day 2 was baseline\n     (B); sleep deprivation started after day 2.\n\nFormat:\n\n     A data frame with 180 observations on the following 3 variables.\n\n     ‘Reaction’ Average reaction time (ms)\n\n     ‘Days’ Number of days of sleep deprivation\n\n     ‘Subject’ Subject number on which the observation was made.\n\nDetails:\n\n     These data are from the study described in Belenky et al.  (2003),\n     for the most sleep-deprived group (3 hours time-in-bed) and for\n     the first 10 days of the study, up to the recovery period.  The\n     original study analyzed speed (1/(reaction time)) and treated day\n     as a categorical rather than a continuous predictor.\n\nReferences:\n\n     Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L.\n     Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and\n     Thomas J. Balkin (2003) Patterns of performance degradation and\n     restoration during sleep restriction and subsequent recovery: a\n     sleep dose-response study. _Journal of Sleep Research_ *12*, 1-12.\n```\n\nThese data meet our definition of multilevel data due to repeated measurements on the same dependent variable (mean RT) for the same participants over ten days. Multilevel data of this type is extremely common in psychology. Unfortunately, most statistics textbooks commonly used in psychology courses don't sufficiently discuss multilevel data, beyond paired t-tests and repeated-measures ANOVA. The `sleepstudy` dataset is interesting because it is multilevel but has a continuous predictor, and thus does not fit will with t-test or ANOVA, because both of these approaches are for categorical predictors. There are ways you could make the data fit into one of these frameworks, but not without losing information or possibly violating assumptions.\n\nIt is a shame that psychology students don't really learn much about analyzing multilevel data. Think about studies you have read recently in psychology or neuroscience. How many of them take a single measurement on the DV from each participant? Very few, if any. Nearly all take multiple measurements, for one or more of the following reasons: (1) the researchers are measuring the same participants across levels of a factor in a within-subject design; (2) they are interested in assessing change over time; or (3) they are measuring responses to multiple stimuli. Multilevel data is so common that multilevel analysis should be taught as the **default** approach in psychology. Learning about multilevel analysis can be challenging, but you already know much of what you need by having learned about correlation and regression. You will see that it is just an extension of simple regression.\n\nLet's take a closer look at the `sleepstudy` data. The dataset contains eighteen participants from the three-hour sleep condition. Each day, over 10 days, participants performed a ten-minute \"psychomotor vigilance test\" where they had to monitor a display and press a button as quickly as possible each time a stimulus appeared. The dependent measure in the dataset is the participant's average response time (RT) on the task for that day.\n\nA good way to start every analysis is to plot the data.  @fig-one-subject shows data for a single subject. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"lme4\")\nlibrary(\"tidyverse\")\n\njust_308 <- sleepstudy |>\n  filter(Subject == \"308\")\n\nggplot(just_308, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9)\n```\n\n::: {.cell-output-display}\n![Data from a single subject in Belenky et al. (2003)](mixed-effects_files/figure-html/fig-one-subject-1.png){#fig-one-subject width=672}\n:::\n:::\n\n\n\n\n::: {#nte-facet .callout-note collapse=\"true\"}\n## Exercise 4.1\n\nUse ggplot to recreate @fig-plot-solution below, which shows data for all 18 subjects.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Data from Belenky et al. (2003)](mixed-effects_files/figure-html/fig-plot-solution-1.png){#fig-plot-solution width=672}\n:::\n:::\n\n\n\n\nIt looks like RT is increasing with each additional day of sleep deprivation, starting from day 2 and increasing until day 10.\n\n::: {.callout-tip collapse=\"true\"}\n## Hint\n\nJust above, you were given the code to make a plot for a single participant. Adapt this code to show all of the participants by getting rid of the `filter()` statement and adding a *`ggplot2`* function that starts with `facet_`.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\nSame as above, except you just add one line: `facet_wrap(~Subject)`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9) +\n  facet_wrap(~Subject)\n```\n:::\n\n\n\n\n:::\n\n:::\n\n## How to model the relationship?\n\nWe wish to understand the effect of sleep deprivation on respnose time, so we will try to model this relationship. To model the data appropriately, first we need to know more about the study design. This is how @Belenky_et_al_2003 describe it (p. 2):\n\n> The first 3 days (T1, T2 and B) were adaptation and training (T1 and T2) and baseline (B) and subjects were required to be in bed from 23:00 to 07:00 h [8 h required time in bed (TIB)]. On the third day (B), baseline measures were taken. Beginning on the fourth day and continuing for a total of 7 days (E1–E7) subjects were in one of four sleep conditions [9 h required TIB (22:00–07:00 h), 7 h required TIB (24:00–07:00 h), 5 h required TIB (02:00–07:00 h), or 3 h required TIB (04:00–07:00 h)], effectively one sleep augmentation condition, and three sleep restriction conditions.\n\nThere were seven nights of sleep restriction, with the first night of restriction occurring after the third day. The first two days, coded as `0`, `1`, were adaptation and training. The day coded as `2`, where the baseline measurement was taken, should be the place where we start our analysis. If we include the days `0` and `1` in our analysis, this might bias our results, since any changes in performance during the first two days have to do with training, not sleep restriction. Exercise 4.2 shows how to do this. The resulting data is stored in the data frame `sleep2`, which we will use in the analysis.\n\n::: {.callout-note collapse=\"true\"}\n## Exercise 4.2\n\nRemove from the dataset observations where `Days` is coded `0` or `1`, and then make a new variable `days_deprived` from the `Days` variable so that the sequence starts at day 2, with day 2 being re-coded as day 0, day 3 as day 1, day 4 as day 2, etc. This new variable now tracks the number of days of sleep deprivation. Store the new table as `sleep2`.\n\n::: {.callout-tip collapse=\"true\"}\n## Hint\n\nYou will need to get rid of observations where `Days` is less than `2`, and then create a new variable, `days_deprived`, which is a transformation of the variable `Days`.\n\nYou can do this by using two of the following six one-table dplyr verbs on the data: `select()`, `mutate()`, `group_by()`, `filter()`, `arrange()`, `summarise()`.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## Solution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep2 <- sleepstudy |>\n  filter(Days >= 2L) |>\n  mutate(days_deprived = Days - 2L)\n```\n:::\n\n\n\n\nIt is always a good idea to double check that the code works as intended. First, look at it:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(sleep2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Reaction Days Subject days_deprived\n1 250.8006    2     308             0\n2 321.4398    3     308             1\n3 356.8519    4     308             2\n4 414.6901    5     308             3\n5 382.2038    6     308             4\n6 290.1486    7     308             5\n```\n\n\n:::\n:::\n\n\n\n\nAnd check that `Days` and `days_deprived` match up.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep2 |>\n  count(days_deprived, Days)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  days_deprived Days  n\n1             0    2 18\n2             1    3 18\n3             2    4 18\n4             3    5 18\n5             4    6 18\n6             5    7 18\n7             6    8 18\n8             7    9 18\n```\n\n\n:::\n:::\n\n\n\n\nLooks good. Note that the variable `n` in generated by `count()` and tells you how many rows there are for each unique combination of `Days` and `days_deprived`. In this case, there were 18, one row for each participant.\n\n:::\n\n:::\n\nNow let's re-plot the data looking at just these eight data points from Day 0 to Day 7. We've just copied the code from above, substituting `sleep2` for `sleepstudy` and using `days_deprived` for our `x` variable.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n```\n\n::: {.cell-output-display}\n![Data from Belenky et al. (2003), showing reaction time at baseline (0) and after each day of sleep deprivation.](mixed-effects_files/figure-html/fig-plot-solution2-1.png){#fig-plot-solution2 width=672}\n:::\n:::\n\n\n\n\nTake a moment to think about how me might model the relationship between `days_deprived` and `Reaction`. Does reaction time increase or decrease with increasing sleep deprivation? Is the relationship roughly stable or does it change with time?\n\nWith only one exception (subject 335) it looks like reaction time increases with each additional day of sleep deprivation. It looks like we could fit a line to each participant's data. Recall the general equation for a line is of the form **y = y-intercept + slope $\\times$ x**. In regression, the we usually express a linear relationship with the formula\n\n$$Y_i = \\beta_0 + \\beta_1 X_i + e_i$$\n\nwhere $\\beta_0$ is the y-intercept and $\\beta_1$ is the slope. We estimate these two [:regression coefficients](glossary.qmd#regression-coefficient) from the data.\n\nThe lines will all differ in intercept (mean RT at day zero, before the sleep deprivation began) and slope (the change in RT with each additional day of sleep deprivation). But should we fit the same line to everyone? Or a totally different line for each subject? Or something somewhere in between? \n\nLet's start by considering three different approaches we might take. Following McElreath, we will distinguish these approaches by calling them [complete pooling](#complete-pooling), [no pooling](#no-pooling), and [partial pooling](#partial-pooling).\n\n### Complete pooling: One size fits all {#complete-pooling}\n\nThe **complete pooling** approach is a \"one-size-fits-all\" model: it estimates a single intercept and slope for the entire dataset, ignoring the fact that different subjects might vary in their intercepts or slopes. If that sounds like a bad approach, it is; but you know this because you've already visualized the data and noted that the pattern for each participant would seem to require different y-intercept and slope values.\n\nFitting one line is called the \"complete pooling\" approach because we pool together data from all subjects to get single estimates for an overall intercept and slope. The GLM for this approach is simply\n\n$$Y_{sd} = \\beta_0 + \\beta_1 X_{sd} + e_{sd}$$\n\n$$e_{sd} \\sim N\\left(0, \\sigma^2\\right)$$\n\nwhere $Y_{sd}$ is the mean RT for subject $s$ on day $d$, $X_{sd}$ is the value of `days_deprived` associated with that case (0-7), and $e_{sd}$ is the error.\n\nWe would fit such a model in R using the `lm()` function, e.g.:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncp_model <- lm(Reaction ~ days_deprived, sleep2)\n\nsummary(cp_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Reaction ~ days_deprived, data = sleep2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.284  -26.732    2.143   27.734  140.453 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    267.967      7.737  34.633  < 2e-16 ***\ndays_deprived   11.435      1.850   6.183 6.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 50.85 on 142 degrees of freedom\nMultiple R-squared:  0.2121,\tAdjusted R-squared:  0.2066 \nF-statistic: 38.23 on 1 and 142 DF,  p-value: 6.316e-09\n```\n\n\n:::\n:::\n\n\n\n\nAccording to this model, the predicted mean response time on Day 0 is about 268 milliseconds, with an increase of about 11 milliseconds per day of deprivation, on average.  We can't trust the standard errors for our regression coefficients, however, because we are assuming that all of the observations are independent (technically, that the residuals are). However, we can be pretty sure this is a bad assumption.\n\nLet's add the model predictions to the graph that we created above. We can use `geom_abline()` to do so, specifying the intercept and slope for the line using the regression coefficients from the model fit, `coef(cp_model)`, which returns a two-element vector with intercept and slope, respectively.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(cp_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) days_deprived \n    267.96742      11.43543 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_abline(intercept = coef(cp_model)[1],\n              slope = coef(cp_model)[2],\n              color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n```\n\n::: {.cell-output-display}\n![Data plotted against predictions from the complete pooling model.](mixed-effects_files/figure-html/fig-cp-model-plot-1.png){#fig-cp-model-plot width=672}\n:::\n:::\n\n\n\n\nThe model fits the data badly. We need a different approach.\n\n### No pooling {#no-pooling}\n\nPooling all the information to get just one intercept and one slope estimate is inappropriate. Another approach would be to fit separate lines for each participant. This means that the estimates for each participant will be completely uninformed by the estimates for the other participants. In other words, we separately estimate 18 individual intercept/slope pairs.\n\nThis model could be implemented in two ways: (1) by running separate regressions for each participant or (2) by running fixed-effects regression. We'll do the latter, so that everything is in one big model. We know how to do this already: we add in dummy codes for the `Subject` factor. We have 18 levels of this factor, so we'd need 17 dummy codes. Fortunately, R saves us from the trouble of creating the 17 variables we would need by hand. All we need to do is include `Subject` as a predictor in the model, and interact this categorical predictor with `days_deprived` to allow intercepts and slopes to vary.\n\n::: {.callout-caution}\n\nThe variable `Subject` in the `sleep2` dataset is nominal. We just use numbers as labels to preserve anonymity, without intending to imply that Subject 310 is one point better than Subject 309 and two points better than 308. Make sure that you define it as a factor so that it is not included as a continuous variable!\n\nWe can test whether something is a factor in various ways. One is to use `summary()` on the table.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep2 |> summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Reaction          Days         Subject   days_deprived \n Min.   :203.0   Min.   :2.00   308    : 8   Min.   :0.00  \n 1st Qu.:265.2   1st Qu.:3.75   309    : 8   1st Qu.:1.75  \n Median :303.2   Median :5.50   310    : 8   Median :3.50  \n Mean   :308.0   Mean   :5.50   330    : 8   Mean   :3.50  \n 3rd Qu.:347.7   3rd Qu.:7.25   331    : 8   3rd Qu.:5.25  \n Max.   :466.4   Max.   :9.00   332    : 8   Max.   :7.00  \n                                (Other):96                 \n```\n\n\n:::\n:::\n\n\n\n\nHere you can see that it is not treated as a number because rather than giving you distributional information (means, etc.) it tells you how many observations there are at each level.\n\nYou can also test it directly:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep2 |> pull(Subject) |> is.factor()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n\nIf something is not a factor, you can make it one be re-defining it using the `factor()` function.\n\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnp_model <- lm(Reaction ~ days_deprived + Subject +\n                 days_deprived:Subject,\n               data = sleep2)\n\nsummary(np_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Reaction ~ days_deprived + Subject + days_deprived:Subject, \n    data = sleep2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-106.521   -8.541    1.143    8.889  128.545 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              288.2175    16.4772  17.492  < 2e-16 ***\ndays_deprived             21.6905     3.9388   5.507 2.49e-07 ***\nSubject309               -87.9262    23.3023  -3.773 0.000264 ***\nSubject310               -62.2856    23.3023  -2.673 0.008685 ** \nSubject330               -14.9533    23.3023  -0.642 0.522422    \nSubject331                 9.9658    23.3023   0.428 0.669740    \nSubject332                27.8157    23.3023   1.194 0.235215    \nSubject333                -2.7581    23.3023  -0.118 0.906000    \nSubject334               -50.2051    23.3023  -2.155 0.033422 *  \nSubject335               -25.3429    23.3023  -1.088 0.279207    \nSubject337                24.6143    23.3023   1.056 0.293187    \nSubject349               -59.2183    23.3023  -2.541 0.012464 *  \nSubject350               -40.2023    23.3023  -1.725 0.087343 .  \nSubject351               -24.2467    23.3023  -1.041 0.300419    \nSubject352                43.0655    23.3023   1.848 0.067321 .  \nSubject369               -21.5040    23.3023  -0.923 0.358154    \nSubject370               -53.3072    23.3023  -2.288 0.024107 *  \nSubject371               -30.4896    23.3023  -1.308 0.193504    \nSubject372                 2.4772    23.3023   0.106 0.915535    \ndays_deprived:Subject309 -17.3334     5.5703  -3.112 0.002380 ** \ndays_deprived:Subject310 -17.7915     5.5703  -3.194 0.001839 ** \ndays_deprived:Subject330 -13.6849     5.5703  -2.457 0.015613 *  \ndays_deprived:Subject331 -16.8231     5.5703  -3.020 0.003154 ** \ndays_deprived:Subject332 -19.2947     5.5703  -3.464 0.000765 ***\ndays_deprived:Subject333 -10.8151     5.5703  -1.942 0.054796 .  \ndays_deprived:Subject334  -3.5745     5.5703  -0.642 0.522423    \ndays_deprived:Subject335 -25.8995     5.5703  -4.650 9.47e-06 ***\ndays_deprived:Subject337   0.7518     5.5703   0.135 0.892895    \ndays_deprived:Subject349  -5.2644     5.5703  -0.945 0.346731    \ndays_deprived:Subject350   1.6007     5.5703   0.287 0.774382    \ndays_deprived:Subject351 -13.1681     5.5703  -2.364 0.019867 *  \ndays_deprived:Subject352 -14.4019     5.5703  -2.585 0.011057 *  \ndays_deprived:Subject369  -7.8948     5.5703  -1.417 0.159273    \ndays_deprived:Subject370  -1.0495     5.5703  -0.188 0.850912    \ndays_deprived:Subject371  -9.3443     5.5703  -1.678 0.096334 .  \ndays_deprived:Subject372 -10.6041     5.5703  -1.904 0.059613 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.53 on 108 degrees of freedom\nMultiple R-squared:  0.849,\tAdjusted R-squared:  0.8001 \nF-statistic: 17.35 on 35 and 108 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\nWhat this model has done is take one subject to be the baseline (specifically, subject 308), and represent each subject in terms of offsets from that baseline. You saw this already when we talked about [continuous-by-categorical interactions](05-interactions-course-notes.html#cont-by-cat).\n\n::: {.callout-note collapse=\"true\"}\n## Exercise 4.3\n\nAnswer the questions below about values estimated by the model.\n\n- What is the intercept for subject 308? \n- What is the slope for subject 308? \n- What is the intercept for subject 335? \n- What is the slope for subject 335? \n\n<!-- \nr fitb(coef(.np)[\"(Intercept)\"], width = 8, num = TRUE, tol = .001)\n\nr fitb(coef(.np)[\"days_deprived\"], num = TRUE, width = 8, tol = .001)\n\nr fitb(coef(.np)[\"(Intercept)\"] + coef(.np)[\"Subject335\"], width = 8, num = TRUE, tol = .001)\n\nr fitb(coef(.np)[\"days_deprived\"] + coef(.np)[\"days_deprived:Subject335\"], num = TRUE, width = 8, tol = .001)\n-->\n\n::: {.callout-tip collapse=\"true\"}\n## Answers and explanation\n\nThe baseline subject is 308; the default in R is to sort the levels of the factor alphabetically and chooses the first one as the baseline. This means that the intercept and slope for 308 are given by `(Intercept)` and `days_deprived` respectively, because all of the other 17 dummy variables will be zero for subject 308.\n\nAll of the regression coefficients for the other subjects are represented as *offsets* from this baseline subject. If we want to calculate the intercept and slope for a given subject, we just add in the corresponding offsets. So, the answers are\n\n* intercept for 308: <u>288.217</u>\n* slope for 308: 21.69\n\n* intercept for 335: `(Intercept)` + `Subject335` = 288.217 + -25.343 = <u>262.874</u>\n* slope for 335: `days_deprived` + `days_deprived:Subject335` = 21.69 + -25.899 = <u>-4.209</u>\n\n:::\n\n:::\n\nIn the \"no pooling\" model, there is no *overall* population intercept and slope that is being estimated; in this case, `(Intercept)` and `days_deprived` are estimates of the intercept and slope for subject 308, which was (arbitrarily) chosen as the baseline subject. To get population estimates, we could introduce a second stage of analysis where we calculate means of the individual intercepts and slopes. Let's use the model estimates to calculate the intercepts and slopes for each subject.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_intercepts <- c(coef(np_model)[\"(Intercept)\"],\n                    coef(np_model)[3:19] + coef(np_model)[\"(Intercept)\"])\n\nall_slopes  <- c(coef(np_model)[\"days_deprived\"],\n                 coef(np_model)[20:36] + coef(np_model)[\"days_deprived\"])\n\nids <- sleep2 |>\n  pull(Subject) |>\n  levels()\n\n# make a tibble with the data extracted above\nnp_coef <- tibble(Subject = factor(ids),\n                  intercept = all_intercepts,\n                  slope = all_slopes)\n\nnp_coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 3\n   Subject intercept slope\n   <fct>       <dbl> <dbl>\n 1 308          288. 21.7 \n 2 309          200.  4.36\n 3 310          226.  3.90\n 4 330          273.  8.01\n 5 331          298.  4.87\n 6 332          316.  2.40\n 7 333          285. 10.9 \n 8 334          238. 18.1 \n 9 335          263. -4.21\n10 337          313. 22.4 \n11 349          229. 16.4 \n12 350          248. 23.3 \n13 351          264.  8.52\n14 352          331.  7.29\n15 369          267. 13.8 \n16 370          235. 20.6 \n17 371          258. 12.3 \n18 372          291. 11.1 \n```\n\n\n:::\n:::\n\n\n\n\nLet's see how well this model fits our data (@fig-np-model-fits).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_abline(data = np_coef,\n              mapping = aes(intercept = intercept,\n                            slope = slope),\n              color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n```\n\n::: {.cell-output-display}\n![Data plotted against fits from the no-pooling approach.](mixed-effects_files/figure-html/fig-np-model-fits-1.png){#fig-np-model-fits width=672}\n:::\n:::\n\n\n\n\nThis is much better than the complete pooling model. If we want to test the null hypothesis that the fixed slope is zero, we could do so using a one-sample test.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnp_coef |>\n  pull(slope) |>\n  t.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  pull(np_coef, slope)\nt = 6.1971, df = 17, p-value = 9.749e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  7.542244 15.328613\nsample estimates:\nmean of x \n 11.43543 \n```\n\n\n:::\n:::\n\n\n\n\nThis tells us that the mean slope of\n11.435\nis significantly different from zero,\nt\\(17\\) = 6\\.197, $p < .001$.\n\n### Partial pooling using mixed-effects models\n\nNeither the complete nor the no-pooling approach is satisfactory. It would be desirable to improve our estimates for individual participants by taking advantage of what we know about the other participants. This will help us better distinguish signal from error for each participant and improve generalization to the population. As the web app below will show, this becomes particularly important when we have unbalanced or missing data.\n\nIn the no-pooling model, we treated `Subject` as a [:fixed factor](glossary.qmd#fixed-factor). Each pair of intercept and slope estimates is determined by that subject's data alone. However, we are not interested in these 18 subjects in and of themselves; rather, we are interested in them as examples drawn from a larger population of potential subjects. This subjects-as-fixed-effects approach is suboptimal if your goal is to generalize to new participants in the population of interest. \n\nPartial pooling happens when you treat a factor as a random instead of fixed in your analysis. A [:random factor](glossary.qmd#random-factor) is a factor whose levels are considered to represent a proper subset of all the levels in the population. Usually, you treat a factor as random when the levels you have in the data are the result of sampling, and you want to generalize beyond those levels. In this case, we have eighteen unique subjects and thus, eighteen levels of the `Subject` factor, and would like to say something general about effects of sleep deprivation on the population of potential subjects.\n\n<!-- Just as there are two types of factors---fixed and random---there are two types of effects associated with them -->\n\nA way to include random factors in your analysis is to use a [:linear mixed-effects model](glossary.qmd#linear-mixed-effects-model). When you do this, estimates at each level of the factor (i.e., for each subject) become informed by information at other levels (i.e., for other subjects). Rather than estimating the intercept and slope for each participant without considering the estimates for other subjects, the model estimates values for the population, and pulls the estimates for individual subjects toward those values, a statistical phenomenon known as \"shrinkage\". \n\nThe multilevel model is below. It is important that you understand the math and what it means. It looks complicated at first, but there's really nothing below that you haven't seen before. We'll explain everything step by step.\n\n**Level 1:**\n\n\\begin{equation}\nY_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + e_{sd}\n\\end{equation}\n\n**Level 2:**\n\n\\begin{equation}\n\\beta_{0s} = \\gamma_{0} + S_{0s}\n\\end{equation}\n\n\\begin{equation}\n\\beta_{1s} = \\gamma_{1} + S_{1s}\n\\end{equation}\n\n*Variance Components:*\n\n\\begin{equation}\n \\langle S_{0s}, S_{1s} \\rangle \\sim N\\left(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}\\right) \n\\end{equation}\n\n\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\\begin{array}{cc}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n         \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\\\\n         \\end{array}\\right) \n\\end{equation}\n\n\\begin{equation}\ne_{sd} \\sim N\\left(0, \\sigma^2\\right)\n\\end{equation}\n\nIn case you get lost, here's a table with an explanation for all of the variables in the set of equations above.\n\n| Variable                 | Type      | Description                                                           |\n|:-------------------------|:----------|:----------------------------------------------------------------------|\n| $Y_{sd}$               | observed | Value of `Reaction` for subject $s$ on day $d$                    |\n| $X_{sd}$               | observed | Value of `days_deprived` (0-7) for subject $s$ on day $d$                  |\n| $\\beta_{0s}$           | derived  | level 1 intercept parameter                                           |\n| $\\beta_{1s}$           | derived  | level 1 slope parameter                                               |\n| $e_{sd}$               | derived  | Error for subject $s$, day $d$ |\n| $\\gamma_0$             | fixed    | Grand intercept (\"gamma\")                                             |\n| $\\gamma_1$             | fixed    | Grand slope (\"gamma\")                                                 |\n| $S_{0s}$               | derived  | Random intercept (offset) for subject $s$                           |\n| $S_{1s}$               | derived  | Random slope (offset) for subject $s$                               |\n| $\\mathbf{\\Sigma}$      | random   | Variance-covariance matrix                                            |\n| ${\\tau_{00}}^2$        | random   | Variance of random intercepts                                         |\n| $\\rho$                 | random   | Random correlation between intercepts and slopes                      |\n| ${\\tau_{11}}^2$        | random   | Variance of random slopes                                             |\n| $\\sigma^2$             | random   | Error variance                                                        |\n\nNote the \"Status\" column of the table contains values *fixed*, *random*, and *derived*. Although *fixed* and *random* are standard terms, *derived* is not; I have introduced it to help you think about what these different variables mean in the context of the model and to help you distinguish variables that are directly estimated from variables that are not.\n\nLet's begin with the Level 1 equation of our model, which represents the general relationship between the predictors and response variable. It captures the functional form of the main relationship between reaction time $Y_{sd}$ and sleep deprivation $X_{sd}$: a straight line with intercept $\\beta_{0s}$ and slope $\\beta_{1s}$. Now $\\beta_{0s}$ and $\\beta_{1s}$ makes it look like the complete pooling model, where we estimated a single intercept and single slope for the entire dataset; however, we're not actually estimating these directly. Instead, we're going to think of $\\beta_{0s}$ and $\\beta_{1s}$ as derived: they are wholly defined by variables at Level 2 of the model.\n\nLevel 2 of the model, defined by two equations, represents relationships at the participant level. Here, we define the intercept $\\beta_{0s}$ in terms of a fixed effect $\\gamma_0$ and a *random intercept* $S_{0s}$; likewise, we define the slope $\\beta_{1s}$ in terms of a fixed slope $\\gamma_1$ and a *random slope* $S_{1s}$.\n\nThe final equations represent the *Variance Components* of the model.  We'll get into this more in detail below.\n\nLet's substitute the Level 2 equations into Level 1 to see the advantages of representing things in the multilevel way.\n\n\\begin{equation}\nY_{sd} = \\gamma_{0} + S_{0s} + \\left(\\gamma_{1} + S_{1s}\\right) X_{sd} + e_{sd}\n\\end{equation}\n\nWhile this \"combined\" formula syntax is easy enough to understand in this particular case, the multilevel form more clearly allows us to see the functional form of the model: a straight line. We could easily change the functional form to, for instance, capture non-linear trends:\n\n$$Y_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + \\beta_{2s} X_{sd}^2 + e_{sd}$$\n\nThis functional form gets obscured in the combined syntax.  The multilevel syntax also makes it easy to see which terms go with the intercept and which terms go with the slope. Also, as designs get more compilicated---for example, if we were to assign participants to different experimental conditions, thus introducing a further predictors at Level 2---the combined equations get harder and harder to parse and reason about.\n\nFixed effect parameters like $\\gamma_0$ and $\\gamma_1$ are estimated from the data, and reflect stable properties of the population. In this example, $\\gamma_0$ is the population intercept and $\\gamma_1$ is the population slope. You can think of these **fixed-effects parameters** as representing the **average intercept and slope** in the population. These are \"fixed\" in the sense that we assume that they reflect the true underlying values in the population; they are not assumed to vary from sample to sample.  These fixed effects parameters are often of prime theoretical interest; we want to measure them and their standard errors in a manner that is as unbiased and precise as the data allow. In experimental settings they are often the targets of hypothesis tests.\n\nRandom effects like $S_{0i}$ and $S_{1i}$ allow intercepts and slopes (respectively) to vary over subjects.  These random effects are *offsets*: deviations from the population 'grand mean' values. Some subjects will just be slower responders than others, such that they will have a higher intercept (mean RT) on day 0 than the population's estimated value of $\\hat{\\gamma_0}$. These slower-than-average subjects will have positive $S_{0i}$ values; faster-than-average subjects will have negative $S_{0i}$ values. Likewise, some subjects will show stronger effects of sleep deprivation (steeper slope) than the estimated population effect, $\\hat{\\gamma_1}$, which implies a positive offset $S_{1s}$, while others may show weaker effects or close to none at all (negative offset).\n\nEach participant can be represented as a vector pair $\\langle S_{0i}, S_{1i} \\rangle$.  If the subjects in our sample comprised the entire population, we would be justified in treating them as fixed and estimating their values, as in the \"no-pooling\" approach above. This is not the case here. In recognition of the fact they are sampled, we are going to treat subjects as a random factor rather than a fixed factor. Instead of estimating the values for the subjects we happened to pick, we will estimate **the covariance matrix that represents the  bivariate distribution from which these pairs of values are drawn**. By doing this, we allow the subjects in the sample to inform us about characteristics of the population.\n\n### The variance-covariance matrix\n\n\\begin{equation}\n \\langle S_{0s}, S_{1s} \\rangle \\sim N\\left(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}\\right) \n\\end{equation}\n\n\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\\begin{array}{cc}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n         \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\\\\n         \\end{array}\\right) \n\\end{equation}\n\nEquations in the *Variance Components* characterize our estimates of variability. The first equation states our assumption that the random intercept / random slope pairs $\\langle S_{0s}, S_{1s} \\rangle$ are drawn from a bivariate normal distribution centered at the origin $\\langle 0, 0 \\rangle$ with variance-covariance matrix $\\mathbf{\\Sigma}$.\n\nThe variance-covariance matrix is key: it determines the probability of drawing random effect pairs $\\langle S_{0s}, S_{1s} \\rangle$ from the population.  You have seen these before, in the [chapter on correlation and regression](corr-and-reg.qmd). The covariance matrix is always a square matrix (equal numbers of columns and rows).  On the main diagonal (upper left and bottom right cells) it has random effect variances ${\\tau_{00}}^2$ and ${\\tau_{11}}^2$. ${\\tau_{00}}^2$ is the random intercept variance, which captures how much subjects vary in their mean response time on Day 0, before any sleep deprivation. ${\\tau_{11}}^2$ is the random slope variance, which captures how much subjects vary in their susceptibility to the effects of sleep deprivation.\n\nThe cells in the off-diagonal contain covariances, but this information is represented redundantly in the matrix; the lower left element is identical to the upper right element; both capture the covariance between random intercepts and slopes, as expressed by $\\rho\\tau_{00}\\tau_{11}$. In this equation $\\rho$ is the correlation between the intercept and slope. So, all the information in the matrix can be captured by just three parameters: $\\tau_{00}$, $\\tau_{11}$, and $\\rho$.\n\n## Estimating linear mixed-effects models\n\nTo estimate parameters, we are going to use the `lmer()` function of the lme4 package (Bates, Mächler, Bolker, & Walker, 2015).  The basic syntax of `lmer()` is\n\n```\nlmer(formula, data, ...)\n```\n\nwhere `formula` expresses the structure of the underlying model in a compact format and `data` is the data frame where the variables mentioned in the formula can be found.\n\nThe general format of the model formula for N fixed effects (`fix`) and K random effects (`ran`) is\n\n`DV ~ fix1 + fix2 + ... + fixN + (ran1 + ran2 + ... + ranK | random_factor1)`\n\nInteractions between factors A and B can be specified using either `A * B` (interaction and main effects) or `A:B` (just the interaction).\n\nA key difference from standard R model syntax is the presence of a random effect term, which is enclosed in parentheses, e.g., `(ran1 + ran2 + ... + ranK | random_factor)`.  Each bracketed expression represents random effects associated with a single random factor. You can have more than one random effects term in a single formula, as we will see when we talk about crossed random factors. You should think of the random effects terms as providing `lmer()` with **instructions on how to construct variance-covariance matrices**.\n\nOn the left side of the bar `|` you put the effects you want to allow to vary over the levels of the random factor named on the right side. Usually, the right-side variable is one whose values uniquely identify individual subjects (e.g., `subject_id`).\n\nConsider the following possible model formulas for the `sleep2` data and the variance-covariance matrices they construct.\n\n|   | model                        | syntax                                      |\n|---+------------------------------+---------------------------------------------|\n| 1 | random intercepts only       | `Reaction ~ days_deprived + (1 | Subject)`         |\n| 2 | random intercepts and slopes | `Reaction ~ days_deprived + (1 + days_deprived | Subject)`  |\n| 3 | model 2 alternate syntax     | `Reaction ~ days_deprived + (days_deprived | Subject)`      |\n| 4 | random slopes only           | `Reaction ~ days_deprived + (0 + days_deprived | Subject)`  |\n| 5 | model 2 + zero-covariances   | `Reaction ~ days_deprived + (days_deprived || Subject)` |\n\n*Model 1:*\n\n\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    {\\tau_{00}}^2 & 0 \\\\\n                0 & 0 \\\\\n  \\end{array}\\right) \n\\end{equation*}\n\n*Models 2 and 3:*\n\n\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n             {\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n    \\rho\\tau_{00}\\tau_{11} &          {\\tau_{11}}^2 \\\\\n  \\end{array}\\right) \n\\end{equation*}\n\n*Model 4:*\n\n\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    0 &             0 \\\\\n    0 & {\\tau_{11}}^2 \\\\\n  \\end{array}\\right) \n\\end{equation*}\n\n*Model 5:*\n\n\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    {\\tau_{00}}^2 &             0 \\\\\n                0 & {\\tau_{11}}^2 \\\\\n  \\end{array}\\right) \n\\end{equation*}\n\nThe most reasonable model for these data is Model 2, so we'll stick with that.\n\nLet's fit the model, storing the result in object `pp_mod`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_mod <- lmer(Reaction ~ days_deprived + (days_deprived | Subject),\n               data = sleep2)\n\nsummary(pp_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ days_deprived + (days_deprived | Subject)\n   Data: sleep2\n\nREML criterion at convergence: 1404.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0157 -0.3541  0.0069  0.4681  5.0732 \n\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n Residual               651.60   25.526       \nNumber of obs: 144, groups:  Subject, 18\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    267.967      8.266  32.418\ndays_deprived   11.435      1.845   6.197\n\nCorrelation of Fixed Effects:\n            (Intr)\ndays_deprvd -0.062\n```\n\n\n:::\n:::\n\n\n\n\nBefore discussing how to interpret the output, let's first plot the data against our model predictions. We can get model predictions using the `predict()` function (see `?predict.merMod` for information about use with mixed-effects models).\n\nFirst, create a new data frame with predictor values for `Subject` and `days_deprived`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata <- crossing(\n  Subject = sleep2 |>\n    pull(Subject) |>\n    levels() |>\n    factor(),\n  days_deprived = 0:7)\n\nhead(newdata, 17)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17 × 2\n   Subject days_deprived\n   <fct>           <int>\n 1 308                 0\n 2 308                 1\n 3 308                 2\n 4 308                 3\n 5 308                 4\n 6 308                 5\n 7 308                 6\n 8 308                 7\n 9 309                 0\n10 309                 1\n11 309                 2\n12 309                 3\n13 309                 4\n14 309                 5\n15 309                 6\n16 309                 7\n17 310                 0\n```\n\n\n:::\n:::\n\n\n\n\nThen, run this through `predict()`. Typically we will add the prediction in as a new variable in the data frame of new data, giving it the same name as our DV (`Reaction`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewdata2 <- newdata |>\n  mutate(Reaction = predict(pp_mod, newdata))\n```\n:::\n\n\n\n\nNow we are ready to plot.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_line(data = newdata2,\n            color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n```\n\n::: {.cell-output-display}\n![Data plotted against predictions from a partial pooling approach.](mixed-effects_files/figure-html/fig-pp-plot-1.png){#fig-pp-plot width=672}\n:::\n:::\n\n\n\n\n## Interpreting `lmer()` output and extracting estimates\n\nThe call to `lmer()` returns a fitted model object of class \"lmerMod\". To find out more about the `lmerMod` class, which is in turn a specialized version of the `merMod` class, see `?lmerMod-class`.\n\n\n\n\n\n\n\n\n\n### Fixed effects\n\nThe section of the output called `Fixed effects:` should look familiar; it is similar to what you would see in the output for a simple linear model fit by `lm()`.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    267.967      8.266  32.418\ndays_deprived   11.435      1.845   6.197\n```\n\n\n:::\n:::\n\n\n\n\nThis indicates that the estimated mean reaction time for participants at Day 0 was about\n268 milliseconds, \nwith each day of sleep deprivation adding an additional \n11 milliseconds \nto the response time, on average.\n\nIf we need to get the fixed effects from the model, we can extract them using `fixef()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(pp_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) days_deprived \n    267.96742      11.43543 \n```\n\n\n:::\n:::\n\n\n\n\nThe standard errors give us estimates of the variability for these parameters due to sampling error.  You could use these to calculate the $t$-values or derive confidence intervals. Extract them using `vcov(pp_mod)` which gives a variance-covariance matrix (*not* the one associated with the random effects), pull out the diagonal using `diag()` and then take the square root using `sqrt()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(diag(vcov(pp_mod)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) days_deprived \n     8.265896      1.845293 \n```\n\n\n:::\n\n```{.r .cell-code}\n# OR, equivalently using pipes:\n# vcov(pp_mod) |> diag() |> sqrt()\n```\n:::\n\n\n\n\nNote that these $t$ values do not appear with $p$ values, as is customary in simpler modeling frameworks. There are multiple approaches for getting $p$ values from mixed-effects models, with advantages and disadvantages to each; see @Luke_2017 for a survey of options. The $t$ values do not appear with degrees of freedom, because the degrees of freedom in a mixed-effects model are not well-defined. Often people will treat them as Wald $z$ values, i.e., as observations from the standard normal distribution. Because the $t$ distribution asymptotes the standard normal distribution as the number of observations goes to infinity, this \"t-as-z\" practice is legitimate if you have a large enough set of observations.\n\nTo calculate the Wald $z$ values, just divide the fixed effect estimate by its standard error:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntvals <- fixef(pp_mod) / sqrt(diag(vcov(pp_mod)))\n\ntvals\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) days_deprived \n    32.418437      6.197082 \n```\n\n\n:::\n:::\n\n\n\n\nYou can get the associated $p$-values using the following formula:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n2 * (1 - pnorm(abs(tvals)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) days_deprived \n  0.00000e+00   5.75197e-10 \n```\n\n\n:::\n:::\n\n\n\n\nThis gives us strong evidence against the null hypothesis $H_0: \\gamma_1 = 0$. Sleep deprivation does appear to increase response time.\n\nYou can get confidence intervals for the estimates using `confint()` (this technique uses the *parametric bootstrap*).  `confint()` is a generic function, so to get help on this function, use `?confint.merMod`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(pp_mod)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nComputing profile confidence intervals ...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    2.5 %      97.5 %\n.sig01         19.0979934  46.3366599\n.sig02         -0.4051073   0.8058951\n.sig03          4.0079284  10.2487351\n.sigma         22.4666029  29.3494509\n(Intercept)   251.3443396 284.5904989\ndays_deprived   7.7245247  15.1463328\n```\n\n\n:::\n:::\n\n\n\n\n### Random effects\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n Residual               651.60   25.526       \nNumber of obs: 144, groups:  Subject, 18\n```\n\n\n:::\n:::\n\n\n\n\nThe random effects part of the `summary()` output is less familiar. What you find here is a table with information about the variance components: the variance-covariance matrix (or matrices, if you have multiple random factors) and the residual variance.\n\nLet's start with the `Residual` line. This tells us that the residual variance, $\\sigma^2$, was estimated at about\n651.6. The value in the next column, \n25.526, is just the standard deviation, $\\sigma$, which is the square root of the variance.\n\nWe extract the residual standard deviation using the `sigma()` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma(pp_mod) # residual\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25.5264\n```\n\n\n:::\n:::\n\n\n\n\nThe two lines above the `Residual` line give us information about the variance-covariance matrix for the `Subject` random factor.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n```\n\n\n:::\n:::\n\n\n\n\nThe values in the `Variance` column gives us the main diagonal of the matrix, and the `Std.Dev.` values are just the square roots of these values. The `Corr` column tells us the correlation between the intercept and slope.\n\nWe can extract these values from the fitted object `pp_mod` using the `VarCorr()` function. This returns a named list, with one element for each random factor. We have `Subject` as our only random factor, so the list will just be of length 1.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# variance-covariance matrix for random factor Subject\nVarCorr(pp_mod)[[\"Subject\"]] # equivalently: VarCorr(pp_mod)[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              (Intercept) days_deprived\n(Intercept)      958.3517      37.20460\ndays_deprived     37.2046      45.77766\nattr(,\"stddev\")\n  (Intercept) days_deprived \n    30.957255      6.765919 \nattr(,\"correlation\")\n              (Intercept) days_deprived\n(Intercept)     1.0000000     0.1776263\ndays_deprived   0.1776263     1.0000000\n```\n\n\n:::\n:::\n\n\n\n\nThe first few lines are a printout of the variance covariance matrix. You can see the variances in the main diagonal. We can get these with:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag(VarCorr(pp_mod)[[\"Subject\"]]) # just the variances\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept) days_deprived \n    958.35165      45.77766 \n```\n\n\n:::\n:::\n\n\n\n\nWe can get the correlation between the intecepts and slopes in two ways. First, by extracting the `\"correlation\"` attribute and then pulling out the element in row 1 column 2 (`[1, 2]`):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattr(VarCorr(pp_mod)[[\"Subject\"]], \"correlation\")[1, 2] # the correlation\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1776263\n```\n\n\n:::\n:::\n\n\n\n\nOr we can directly compute the value from the variance-covariance matrix itself.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# directly compute correlation from variance-covariance matrix\nmx <- VarCorr(pp_mod)[[\"Subject\"]]\n\n## if cov = rho * t00 * t11, then\n## rho = cov / (t00 * t11).\nmx[1, 2] / (sqrt(mx[1, 1]) * sqrt(mx[2, 2]))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1776263\n```\n\n\n:::\n:::\n\n\n\n\nWe can pull out the estimated random effects (BLUPS) using `ranef()`. Like `VarCorr()` , the result is a named list, with each element corresponding to a single random factor.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nranef(pp_mod)[[\"Subject\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    (Intercept) days_deprived\n308  24.4992891     8.6020000\n309 -59.3723102    -8.1277534\n310 -39.4762764    -7.4292365\n330   1.3500428    -2.3845976\n331  18.4576169    -3.7477340\n332  30.5270040    -4.8936899\n333  13.3682027     0.2888639\n334 -18.1583020     3.8436686\n335 -16.9737887   -12.0702333\n337  44.5850842    10.1760837\n349 -26.6839022     2.1946699\n350  -5.9657957     8.1758613\n351  -5.5710355    -2.3718494\n352  46.6347253    -0.5616377\n369   0.9616395     1.7385130\n370 -18.5216778     5.6317534\n371  -7.3431320     0.2729282\n372  17.6826159     0.6623897\n```\n\n\n:::\n:::\n\n\n\n\nThere are other extractor functions that are useful. See `?merMod-class` for details.\n\nWe can get fitted values from the model using `fitted()` and residuals using `residuals()`.  (These functions take into account \"the conditional modes of the random effects\", i.e., the BLUPS).\n \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmutate(sleep2,\n       fit = fitted(pp_mod),\n       resid = residuals(pp_mod)) %>%\n  group_by(Subject) %>%\n  slice(c(1,10)) %>%\n  print(n = +Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 × 6\n# Groups:   Subject [18]\n   Reaction  Days Subject days_deprived   fit  resid\n      <dbl> <dbl> <fct>           <dbl> <dbl>  <dbl>\n 1     251.     2 308                 0  292. -41.7 \n 2     203.     2 309                 0  209.  -5.62\n 3     234.     2 310                 0  228.   5.83\n 4     284.     2 330                 0  269.  14.5 \n 5     302.     2 331                 0  286.  15.4 \n 6     273.     2 332                 0  298. -25.5 \n 7     277.     2 333                 0  281.  -4.57\n 8     243.     2 334                 0  250.  -6.44\n 9     254.     2 335                 0  251.   3.50\n10     292.     2 337                 0  313. -20.9 \n11     239.     2 349                 0  241.  -2.36\n12     256.     2 350                 0  262.  -5.80\n13     270.     2 351                 0  262.   7.50\n14     327.     2 352                 0  315.  12.3 \n15     257.     2 369                 0  269. -11.7 \n16     239.     2 370                 0  249. -10.5 \n17     278.     2 371                 0  261.  17.3 \n18     298.     2 372                 0  286.  11.9 \n```\n\n\n:::\n:::\n\n\n\n\nFinally, we can get predictions for new data using `predict()`, as we did above. Below we use `predict()` to imagine what might have happened had we continued our study for three extra days.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create the table with new predictor values\nndat <- crossing(Subject = sleep2 %>% pull(Subject) %>% levels() %>% factor(),\n                 days_deprived = 8:10) %>%\n  mutate(Reaction = predict(pp_mod, newdata = .))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_line(data = bind_rows(newdata2, ndat),\n            color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:10) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n```\n\n::: {.cell-output-display}\n![Data against model with extrapolation.](mixed-effects_files/figure-html/fig-extrap-plot-1.png){#fig-extrap-plot width=672}\n:::\n:::\n\n\n\n\n## Multi-level app\n\n[Try out the multi-level web app](https://talklab.psy.gla.ac.uk/app/multilevel-site/){target=\"_blank\"} to sharpen your understanding of the three different approaches to multi-level modeling.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}