[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Models in Psychology and Neuroscience: A Survey",
    "section": "",
    "text": "Preface\nThis course offers a survey of common approaches to statistical modeling found in psychology and neuroscience, with three key goals in mind: flexibility, generalizability, and reproducibility. We focus on analytical techniques that are flexible, in the sense that they can be adapted to different types of study designs with different types of data. We seek to structure analyses to support claims that are generalizable to the larger population of people (and possibly stimuli) from which our sample was drawn. Finally, we aim to analyse data in ways that are reproducible by writing plain text scripts in R. In this way, the full set of procedures have been documented in an unambiguous way to enable others (including our future selves!) to easily reproduce our analysis from the raw data to the results and perhaps even up to generating the research report.\nThis textbook accompanies a one-semester survey course whose aim is to introduce students to the most common analytical approaches in the field. But psychology and neuroscience are broad research areas with many different traditions and approaches, reflecting the complexity of the subject matter. Fortunately, all of these approaches are built on the foundation of linear regression. My hope is to provide a solid foundation in regression so that you know enough to go further on your own with any technique you are interested in. Be under no illusion that we give adequate treatment to techniques introduced in this textbook beyond multiple regression. For each approach, I provide guidance for learning more at the end of the corresponding chapter.\nThe textbook focuses on the practical implementation of concepts introduced in lectures of my course. The textbook itself does not provide much conceptual discussion. Nevertheless, it may still be useful for people who already understand these concepts but are looking to learn their implementation in R statistical programming.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Statistical Models in Psychology and Neuroscience: A Survey",
    "section": "About this book",
    "text": "About this book\nThe material in this course forms the basis for a one-semester course for third-year undergradautes taught by Dale Barr at the University of Glasgow School of Psychology and Neuroscience. It is part of the PsyTeachR series of course materials developed by University of Glasgow Psychology staff.\nThis textbook is meant to be interactive. Each chapter contains embedded exercises as well as web applications to help students better understand the content. The interactive content will only work if you access this material through a web browser. Printing out the material is not recommended.\n\nThe main tool that we will be using is the statistical programming environment R. To follow along with the code in this textbook you will need R version 4.2.0 or later. It is also recommended that you install the add-on packages tidyverse, lme4, psych, corrr, and lavaan.\nFor anyone starting out with R, you will need to choose an :Integrated Development Environment. For beginners, the RStudio Desktop is a good choice.\n\nHow to use this book\nThis book has ‘dark’ and ‘light’ modes that you can toggle between to suit your reading preferences. Look for the toggle switch next to where the book title is displayed in your browser.\nWhen discussing statistical modeling, some use of technical terminology is unavoidable. This book contains a glossary where you can find definitions of common terms. I have used the Nutshell web tool which allows you to “expand” definitions to appear within the body of the web page, to avoid flipping back and forth between the main text and the glossary. Whereever you see an underlined term that has two dots to the top left of it, like this term—:Nutshell—you can click on the term to expand the definition (try it!).\n\n\nHow to cite this book\nBarr, Dale J. (2024). Statistical models in psychology and neuroscience: A survey. Version 0.9.0. Retrieved from https://psyteachr.github.io/stat-models-v2.\n\n\n\nFree to re-use and remix!\nYou are free to re-use and modify the material in this textbook for your own purposes with the stipulation that you cite the original work. Please note additional terms of the Creative Commons CC-BY-SA 4.0 license governing re-use of this material.\n\n\nHow this book was made\nThis book was authored using the Quarto publishing system. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro-placeholder.html",
    "href": "intro-placeholder.html",
    "title": "1  Introduction",
    "section": "",
    "text": "(under construction)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html",
    "href": "corr-and-reg.html",
    "title": "2  Correlation and Regression",
    "section": "",
    "text": "2.1 Correlation\nA correlation coefficient quantifies the strength and direction of an association between two variables. It is usually represented by the symbol \\(r\\) or \\(\\rho\\) (Greek letter “rho”). The correlation coefficient ranges between -1 and 1, with 0 corresponding to no relationship, positive values reflecting a positive relationship (as one variable increases, so does the other), and negative values reflecting a negative relationship (as one variable increases, the other decreases).\nThe web app below shows 100 random points from a bivariate normal distribution for variables \\(X\\) and \\(Y\\). You can use the sliders to change the parameters of that distribution: the correlation (Greek symbol \\(\\rho\\), “rho”), the standard deviation for \\(X\\) (\\(\\sigma_X\\), “sigma X”), the standard deviation for \\(Y\\) (\\(\\sigma_Y\\)), the mean of \\(X\\) (\\(\\mu_X\\), “mu X”) and the mean of \\(Y\\) (\\(\\mu_Y\\), “mu Y”). These five parameters are all you need to characterize a bivariate normal distribution.\nPlay around with the sliders until you have a conceptual understanding of the various parameters. Use the “new sample” button to get a new set of 100 randomly-generated pairs, and use “reset” to reset the parameter values to their defaults.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html#correlation",
    "href": "corr-and-reg.html#correlation",
    "title": "2  Correlation and Regression",
    "section": "",
    "text": "2.1.1 Bivariate application\n\n\nThere are different correlation coefficients that make different assumptions or that allow you to work with different types of data. To start, let’s consider the one most commonly used: the Pearson product-moment correlation coefficient, which can be applied to :interval or :ratio scale data. When you hear someone talk about a “correlation” without further qualification, they are most likely talking about a Pearson correlation coefficient. If we have the variables \\(X\\) and \\(Y\\) then we might use \\(r_{XY}\\) or \\(\\rho_{XY}\\) as a symbol for the correlation.\nThere are multiple equivalent formulas for calculating a Pearson correlation coefficient. The most important one for us is\n\\[r_{XY} = \\frac{cov_{XY}}{S_X S_Y}\\]\nbecause it can offer some conceptual understanding. The quantity \\(cov_{XY}\\) is the covariance between two variables, \\(X\\) and \\(Y\\), which is defined as\n\\[cov_{XY} = \\frac{\\Sigma (X - \\bar{X})(Y - \\bar{Y})}{N}.\\]\nIn other words, it is the sum of the products of the deviation scores for each \\((X, Y)\\) pair of observations divided by the number of pairs.\nNote that the covariance of a variable with itself is\n\\[cov_{XX} = \\frac{\\Sigma (X - \\bar{X})(X - \\bar{X})}{N} = \\frac{\\Sigma (X - \\bar{X})^2}{N}\\]\nwhich is the formula for the variance of a variable. Taking the square root gives us the formula for the standard deviation:\n\\[SD_X = \\sqrt{\\frac{\\Sigma (X - \\bar{X})^2}{N}}.\\]\nThe above formulas are for calculating these statistics for the sample. When we want to estimate the corresponding parameters for the population we have sampled from, these formulas will have \\(N-1\\) instead of \\(N\\) in their denominators. The R functions cov(), var(), and sd() are used to compute these values on vectors or matrices.\n\n\n\n\n\n\nExercise: Calculate a correlation coefficient for variables in a data frame.\n\n\n\nThe data frame iris in R has measurements of different parts of 50 flowers from three different species of iris. See help(\"iris\") for more information about this dataset.\n\nhead(iris, 6)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nCalculate the correlation between the petal length and petal width of these flowers in R using the cov() and sd() functions in R.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\\[r_{XY} = \\frac{cov_{XY}}{S_X S_Y}\\]\ncov(iris$Petal.Length, iris$Petal.Width) # covariance\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncov(iris$Petal.Length, iris$Petal.Width) /\n  (sd(iris$Petal.Length) * sd(iris$Petal.Width))\n\n[1] 0.9628654\n\n\nLet’s compare this result with the result of the function cor().\n\ncor(iris$Petal.Length, iris$Petal.Width)\n\n[1] 0.9628654\n\n\n\n\n\n\n\n\n\n2.1.2 Covariance matrix\nCovariance matrices become very important once we start talking about multilevel modelling and multivariate analyses.\nA covariance matrix (also known as the variance-covariance matrix) is a mathematical structure that describes the spread of a multivariate distribution. It is multidimensional analogue of the standard deviation.\nTo fully describe a univariate (single variable) normal distribution, you need to know only two parameters: the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). Mathematically, this is often represented as\n\\[X_i \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\]\nwhich is read as, “each case \\(i\\) of the \\(X\\) variable is drawn from a normal distribution with mean of \\(\\mu\\) and variance \\(\\sigma^2\\).\nTo fully describe a bivariate normal distribution, you need five parameters: the means of each of the two variables, their standard deviations, and their correlation. The mathematical way to represent the idea that pairs of observations for case \\(i\\), (\\(X_i\\), \\(Y_i\\)) are drawn from a normal distribution is:\n\\[\\begin{pmatrix} X_i \\\\ Y_i\\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix}\\mu_X \\\\ \\mu_Y\\end{pmatrix}, \\mathbf{\\Sigma}\\right)\\]\nwhere \\(\\mathbf{\\Sigma}\\) is a covariance matrix, defined as\n\\[\\mathbf{\\Sigma} = \\begin{pmatrix}cov_{xx} & cov_{xy} \\\\ cov_{yx} & cov_{yy}\\end{pmatrix} = \\begin{pmatrix}\\rho_{xx}\\sigma_x\\sigma_x & \\rho_{xy}\\sigma_x\\sigma_y \\\\ \\rho_{yx}\\sigma_y\\sigma_x & \\rho_{yy}\\sigma_y\\sigma_y\\end{pmatrix}.\\]\nYou can see that the entries are just a rearrangement of the correlation formula\n\\[\\rho_{xy} = \\frac{cov_{xy}}{\\sigma_x \\sigma_y}.\\]\nwhere we rearrange to solve for \\(cov_{xy}\\)\n\\[cov_{xy} = \\rho_{xy}\\sigma_x \\sigma_y.\\]\nBecause the correlation of a variable with itself (\\(\\rho_{xx}\\), \\(\\rho_{yy}\\)) is always 1, we will often see \\(\\mathbf{\\Sigma}\\) written equivalently as\n\\[\\begin{pmatrix}{\\sigma_x}^2 & \\rho_{xy}\\sigma_x\\sigma_y \\\\ \\rho_{yx}\\sigma_y\\sigma_x & {\\sigma_y}^2\\end{pmatrix}.\\]\nNote two things about the above matrix: first, the variances appear in the main diagonal, and the covariance appear off the main diagonal. Second, a correlation is not directional, and so \\(\\rho_{xy} = \\rho_{yx}\\), which means that the two off-diagonal elements have the same value.\n\n\n\n\n\n\nWarning: \\(\\Sigma\\) doing double duty\n\n\n\nYou may have noticed that the Greek capital letter \\(\\Sigma\\) is used both to represent the instruction to add things together, e.g., \\(\\Sigma X\\) = sum(X), and now it’s being used as a symbol to represent a covariance matrix \\(\\mathbf{\\Sigma}\\). In the latter case, \\(\\Sigma\\) appears in boldface (compare \\(\\mathbf{\\Sigma}\\) to \\(\\Sigma\\)). Usually the context will make things clear.\n\n\n\n\n\n\n\n\nTell me more about matrices\n\n\n\n\n\nIn mathematics, matrices are just generalizations of the concept of a vector: a vector can be thought of as having one dimension, whereas a matrix can have any number of dimensions.\nSo the matrix\n\\[\n\\begin{pmatrix}\n1 & 4 & 7 \\\\\n2 & 5 & 8 \\\\\n3 & 6 & 9 \\\\\n\\end{pmatrix}\n\\]\nis a 3 (row) by 3 (column) matrix containing the column vectors \\(\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ \\end{pmatrix}\\), \\(\\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\\\ \\end{pmatrix}\\), and \\(\\begin{pmatrix} 7 \\\\ 8 \\\\ 9 \\\\ \\end{pmatrix}\\). Conventionally, we refer to matrices in \\(i\\) by \\(j\\) format, with \\(i\\) being the number of rows and \\(j\\) being the number of columns. So a 3x2 matrix has 3 rows and 2 columns, like so.\n\\[\n\\begin{pmatrix}\na & d \\\\\nb & e \\\\\nc & f \\\\\n\\end{pmatrix}\n\\]\nA square matrix is a matrix where the number of rows is equal to the number of columns.\n\n\n\n\n\n\n\n\n\nExercise: Generate a covariance matrix\n\n\n\nGenerate a covariance matrix corresponding to the relation between Petal.Width and Petal.Length for the iris dataset. Name your covariance matrix cvmx.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nr_wl &lt;- cor(iris$Petal.Width, iris$Petal.Length)\nsd_w &lt;- sd(iris$Petal.Width)\nsd_l &lt;- sd(iris$Petal.Length)\n\ncov_wl &lt;- r_wl * sd_w * sd_l\n\n## bind together rows of the matrix using rbind()\ncvmx &lt;- rbind(c(sd_w^2, cov_wl),\n              c(cov_wl, sd_l^2))\n\ncvmx\n\n          [,1]     [,2]\n[1,] 0.5810063 1.295609\n[2,] 1.2956094 3.116278",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html#regression",
    "href": "corr-and-reg.html#regression",
    "title": "2  Correlation and Regression",
    "section": "2.2 Regression",
    "text": "2.2 Regression\nWhen we have two variables, \\(X\\) and \\(Y\\), and our goal is to predict values of \\(Y\\) from \\(X\\), then we usually want to fit a linear regression model; or we might say, we want “to regress \\(Y\\) on \\(X\\).”\nA simple regression model is of the form:\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i.\\]\nIn this equation, \\(\\beta_0\\) and \\(\\beta_1\\) are the y-intercept and slope parameters respectively, and the \\(e_i\\)s are the errors (the part of each \\(Y_i\\) left unexplained by the model). It is conventionally assumed that the \\(e_i\\) values are from a normal distribution with mean of zero and variance \\(\\sigma^2\\); the math-y way of saying this is \\(e_i \\sim \\mathcal{N}(0, \\sigma^2)\\), where \\(\\sim\\) is read as “distributed according to” and \\(\\mathcal{N}(0, \\sigma^2)\\) means “Normal distribution (\\(\\mathcal{N}\\)) with mean of 0 and variance of \\(\\sigma^2\\)”.\n\n2.2.1 Scatterplot\nLet’s return to the iris data. Say we want to predict Petal.Length from Petal.Width. Before proceeding, it is a good idea to make a scatterplot of the data using ggplot2. We can use the colour aesthetic to capture what species a datapoint belongs to.\n\nlibrary(\"tidyverse\")\n\nggplot(iris,\n       aes(x = Petal.Width, y = Petal.Length, colour = Species)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 2.1: Scatterplot of the iris data.\n\n\n\n\n\nThe plot looks like we could nicely fit a line through the cloud of points. Let’s do that.\n\n\n2.2.2 Fitting a model using lm()\nWe use the lm() function (linear model) to fit regression models. It’s a good idea to look at the help file before using a function we’re unfamiliar with (type help(\"lm\") in the console).\nlm                    package:stats                    R Documentation\n\nFitting Linear Models\n\nDescription:\n\n     ‘lm’ is used to fit linear models, including multivariate ones.\n     It can be used to carry out regression, single stratum analysis of\n     variance and analysis of covariance (although aov may provide a\n     more convenient interface for these).\n\nUsage:\n\n     lm(formula, data, subset, weights, na.action,\n        method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,\n        singular.ok = TRUE, contrasts = NULL, offset, ...)\n     \n     ## S3 method for class 'lm'\n     print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)\n     \nArguments:\n\n formula: an object of class ‘\"formula\"’ (or one that can be coerced to\n          that class): a symbolic description of the model to be\n          fitted.  The details of model specification are given under\n          ‘Details’.\n\n    data: an optional data frame, list or environment (or object\n          coercible by as.data.frame to a data frame) containing the\n          variables in the model.  If not found in ‘data’, the\n          variables are taken from ‘environment(formula)’, typically\n          the environment from which ‘lm’ is called.\nUsually the first few arguments are the important ones we need to pay attention to. Here, the key ones are formula and data. We need to provide a symbolic description of the model. Under Details we see\nDetails:\n\n     Models for ‘lm’ are specified symbolically.  A typical model has\n     the form ‘response ~ terms’ where ‘response’ is the (numeric)\n     response vector and terms is a series of terms which specifies a\n     linear predictor for ‘response’.\nSo we specify the model using response ~ terms, but we omit the intercept and the error term of the model because they are always implied. We also don’t name the regression coefficients; they get given the same names as the predictor variables. Thus our formula \\(Y_i = \\beta_0 + \\beta_1 X_i + e_i\\) just becomes Petal.Length ~ Petal.Width. The data argument tells the function the name of the data frame where the variables can be found. The result of the call to lm() is a fitted model object. We want to store the result so that we can perform further computations on the object. So altogether, our call might look like the following.\n\nmod &lt;- lm(Petal.Length ~ Petal.Width, data = iris)\n\nNote that we chose the name mod, but this is arbitrary; we could have used any other variable name.\nTo see the results of the function, we use summary() on the model object.\n\nsummary(mod)\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.33542 -0.30347 -0.02955  0.25776  1.39453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.08356    0.07297   14.85   &lt;2e-16 ***\nPetal.Width  2.22994    0.05140   43.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4782 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n\nWe are most interested in the table of coefficients here, which is the table of the regression coefficients for our model, the y-intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\). We get \\(\\hat{\\beta}_0 = 1.084\\) and \\(\\hat{\\beta}_1 = 2.230\\). So with every 1 unit increase in Petal.Width, Petal.Length increases by 2.230.\nWe can see these value in the output, but we can also pull them out using code with the coef() function, which returns a vector with their values.\n\ncoef(mod)\n\n(Intercept) Petal.Width \n   1.083558    2.229940 \n\n\nRecall that the errors in our model are from a normal distribution with a mean of zero and variance \\(\\sigma^2\\), stated mathematically as \\(e_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\). The “residual standard error” for the model tells us \\(\\hat{\\sigma}\\), the estimated value of \\(\\sigma\\). We can pull this out of the model object using the sigma() function.\n\nsigma(mod)\n\n[1] 0.4782058\n\n\n\n\n2.2.3 Plotting the model fit against the data\nIt’s usually a good idea to plot the model fit against the data to see how well we are doing. Let’s re-create our scatterplot but use geom_abline() to add in a line with the slope and y-intercept for the model, which we can get using coef().\n\nggplot(iris,\n       aes(x = Petal.Width, y = Petal.Length, colour = Species)) +\n  geom_point() +\n  geom_abline(slope = coef(mod)[\"Petal.Width\"],\n              intercept = coef(mod)[\"(Intercept)\"],\n              colour = \"pink\")\n\n\n\n\n\n\n\n\n\n\n2.2.4 Getting other properties of fitted model objects\nThere are three other functions that useful to know. Each of these takes the fitted model object as the first argument.\n\n\n\nfunction\ndescription\n\n\n\n\npredict()\ngenerate predictions from the model\n\n\nfitted()\nget fitted values from the model\n\n\nresiduals()\ncalculate residuals\n\n\n\nFitted values, denoted by \\(\\hat{Y}_i\\), are the predicted values for all the \\(X_i\\) in the data. Predicted values can be for any \\(X_i\\) values, even those not seen in the dataset.\nResiduals represent the error of prediction, and are defined as \\(Y_i - \\hat{Y}_i\\); i.e., the observed value for case \\(i\\) minus the fitted value for case \\(i\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html#relationship-between-correlation-and-regression",
    "href": "corr-and-reg.html#relationship-between-correlation-and-regression",
    "title": "2  Correlation and Regression",
    "section": "2.3 Relationship between correlation and regression",
    "text": "2.3 Relationship between correlation and regression\nWe can calculate regression coefficients from correlation statistics and vice versa. To get regression statistics from correlation statistics, along with the correlation coefficient we need means of X and Y (denoted by \\(\\mu_x\\) and \\(\\mu_y\\) respectively) and their standard deviations (\\(\\hat{\\sigma}_x\\) and \\(\\hat{\\sigma}_y\\)). Let’s see how we can compute regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\).\nFirst, the slope of the regression line \\(\\beta_1\\) equals the correlation coefficient \\(\\rho\\) times the ratio of the standard deviations of \\(Y\\) and \\(X\\).\n\\[\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\]\nIf you play around with the bivariate web app you can verify for yourself that this is the case.\nThe next thing to note is that for mathematical reasons, the regression line is guaranteed to go through the point corresponding to the mean of \\(X\\) and the mean of \\(Y\\), i.e., the point \\((\\mu_x, \\mu_y)\\). (You can think of the regression line “pivoting” around that point depending on the slope). You also know that \\(\\beta_0\\) is the y-intercept, the point where the line crosses the vertical axis at \\(X = 0\\). From this information, and the estimates above, can you figure out the value of \\(\\beta_0\\)?\nWell, for each unit increase in \\(X\\) you have a corresponding change of \\(\\beta_1\\) for \\(Y\\), and you know that the line goes through the points \\((\\mu_x, \\mu_y)\\) as well as the y-intercept \\((0, \\beta_0)\\).\nThink about stepping back unit-by-unit from \\(X = \\mu_x\\) to \\(X = 0\\). At \\(X = \\mu_x\\), \\(Y = \\mu_y\\). Each unit step you take backward in the X dimension, \\(Y\\) will drop by \\(\\beta_1\\) units. When you get to zero, \\(Y\\) will have dropped from \\(\\mu_y\\) to \\(\\mu_y - \\mu_x\\beta_1\\).\nSo the general solution is: \\(\\beta_0 = \\mu_y - \\mu_x\\beta_1\\).\nTo close, here are a few implications from the relationship between correlation and regression.\n\n\\(\\beta_1 = 0\\) is the same as \\(\\rho = 0\\).\n\\(\\beta_1 &gt; 0\\) implies \\(\\rho &gt; 0\\), since standard deviations can’t be negative.\n\\(\\beta_1 &lt; 0\\) implies \\(\\rho &lt; 0\\), for the same reason.\nRejecting the null hypothesis that \\(\\beta_1 = 0\\) is the same as rejecting the null hypothesis that \\(\\rho = 0\\). The p-values that you get for \\(\\beta_1\\) in lm() will be the same as the one you get for \\(\\rho\\) from cor.test().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html#two-approaches-for-simulating-bivariate-data",
    "href": "corr-and-reg.html#two-approaches-for-simulating-bivariate-data",
    "title": "2  Correlation and Regression",
    "section": "2.4 Two approaches for simulating bivariate data",
    "text": "2.4 Two approaches for simulating bivariate data\n\n2.4.1 Approach 1: Simulating from the covariance matrix\nYou can simulate data from the normal distribution using the function rnorm(). The function rnorm() allows you to specify the mean and standard deviation of a single variable. How do we simulate correlated variables?\nIt should be clear that you can’t just run rnorm() twice and combine the variables, because then you end up with two variables that are unrelated, i.e., with a correlation of zero.\nThe package MASS provides a function mvrnorm() which is the ‘multivariate’ version of rnorm (hence the function name, mv + rnorm, which makes it easy to remember.\n\n\n\n\n\n\nCaution\n\n\n\nThe MASS package comes pre-installed with R. But the only function you’ll probably ever want to use from MASS is mvrnorm(), so rather than load in the package using library(\"MASS\"), it is preferable to use MASS::mvrnorm(), especially as MASS and the dplyr package from tidyverse don’t play well together, due to both packages having the function select(). So if you load in MASS after you load in tidyverse, you’ll end up getting the MASS version of select() instead of the dplyr version. It will do your head in trying to figure out what is wrong with your code, so always use MASS::mvrnorm() without loading library(\"MASS\").\n\n\nHave a look at the documentation for the mvrnorm() function (type ?MASS::mvrnorm in the console).\nThere are three arguments to take note of:\n\n\n\n\n\n\n\narg\ndescription\n\n\n\n\nn\nthe number of samples required\n\n\nmu\na vector giving the means of the variables\n\n\nSigma\na positive-definite symmetric matrix specifying the covariance matrix of the variables.\n\n\n\nThe Sigma argument to MASS::mvrnorm() plays an analogous role to the sd argument in rnorm(); it specifies the spread for the two variables.\n\n\n\n\n\n\nExercise: Simulate bivariate data using MASS::mvrnorm().\n\n\n\nUse the covariance matrix cvmx that you created for Petal.Width and Petal.Length to simulate data for 15 flowers with mean Petal.Width of 1.2 and mean Petal.Length of 3.8.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(1451)\n\nsimdata &lt;- MASS::mvrnorm(n = 15,\n                         mu = c(Petal.Width = 1.2, Petal.Length = 3.8),\n                         Sigma = cvmx)\n\n# print it out\nsimdata\n\n      Petal.Width Petal.Length\n [1,]  -0.7143876    -1.019734\n [2,]   0.7856323     1.953391\n [3,]   0.4665259     1.594703\n [4,]   1.1849520     4.854760\n [5,]   1.5465936     4.380756\n [6,]   2.4345540     7.424954\n [7,]   0.8330843     3.838922\n [8,]   0.3377274     1.985729\n [9,]   0.2660191     1.062691\n[10,]   1.4604112     3.376631\n[11,]   2.8518896     7.657955\n[12,]   2.0207311     5.640116\n[13,]   1.7926629     5.396833\n[14,]   0.9789879     3.820881\n[15,]   1.5327700     4.137704\n\n\n\n\n\n\n\n\n\n2.4.2 Approach 2: Simulating from the regression model\nAnother way you can simulated data is based on a regression model. This way is pretty straightforward because we just follow the \\(Y_i = \\beta_0 + \\beta_1 X_i + e_i\\) formula.\n\nfirst, simulate the \\(X_i\\) values\ncalculate the predicted value from the model\nadd random error to the prediction using rnorm() to yield \\(Y_i\\)\n\nLet’s use this approach to simulate data from the iris dataset. First, let’s get the coefficients and the estimate of \\(\\sigma\\).\n\niris_coef &lt;- coef(mod)\niris_sig &lt;- sigma(mod) \n\niris_coef\n\n(Intercept) Petal.Width \n   1.083558    2.229940 \n\niris_sig\n\n[1] 0.4782058\n\n\nWe want to start by creating a data frame containing the \\(X_i\\) values. But for this we need to know \\(\\mu_x\\) and \\(\\sigma_x\\).\n\niris_mu_x &lt;- mean(iris$Petal.Width)\niris_sd_x &lt;- sd(iris$Petal.Width)\n\nWe’ll create the \\(X_i\\) values in a vector first. We have to decide on the number of paired observations we want; let’s say 150, the same number as in the original dataset. Let’s set the seed before we do any random number generation.\n\nset.seed(1451)\n\niris_x &lt;- rnorm(150, mean = iris_mu_x, sd = iris_sd_x)\n\nNow let’s use the tibble() function (from tidyverse) to create the \\(X_i\\) values in a data frame, followed by the predicted value, y_hat, followed by the errors, e_i\n\nsim_iris &lt;- tibble(x_i = iris_x,\n                   y_hat = iris_coef[1] + iris_coef[2] * x_i,\n                   e_i = rnorm(150, mean = 0, sd = iris_sig),\n                   y_i = y_hat + e_i)\n\nLet’s have a look.\n\nggplot(sim_iris,\n       aes(x = x_i, y = y_i)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis looks fairly similar to the original data, but note that we haven’t taken into account the species of flower, so the points are spread evenly across the x dimension, unlike in Figure Figure 2.1 above. So our model is good, but not really complete.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "multiple-reg.html",
    "href": "multiple-reg.html",
    "title": "3  Multiple regression",
    "section": "",
    "text": "3.1 An example: How to get a good grade in statistics\nLet’s look at some (made up, but realistic) data to see how we can use multiple regression to answer various study questions. In this hypothetical study, you have a dataset for 100 statistics students, which includes their final course grade (grade), the number of lectures each student attended (lecture, an integer ranging from 0-10), how many times each student clicked to download online materials (nclicks) and each student’s grade point average prior to taking the course, GPA, which ranges from 0 (fail) to 4 (highest possible grade).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiple-reg.html#an-example-how-to-get-a-good-grade-in-statistics",
    "href": "multiple-reg.html#an-example-how-to-get-a-good-grade-in-statistics",
    "title": "3  Multiple regression",
    "section": "",
    "text": "3.1.1 Data import and visualization\nLet’s load in the data grades.csv and have a look.\n\nlibrary(\"corrr\") # correlation matrices\nlibrary(\"tidyverse\")\n\ngrades &lt;- read_csv(\"data/grades.csv\", col_types = \"ddii\")\n\ngrades\n\n# A tibble: 100 × 4\n   grade   GPA lecture nclicks\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;\n 1  4     2.52      10     108\n 2  3.02  2.73       7      93\n 3  1.47  1.55       4      71\n 4  1.21  2.55      10     101\n 5  1.90  2.46       9      84\n 6  3.38  2.25       6      93\n 7  4     3.45       8     135\n 8  3.47  2.96       6     126\n 9  2.59  3.22       7     109\n10  1.87  2.64       7      74\n# ℹ 90 more rows\n\n\nFirst let’s look at all the pairwise correlations.\n\ngrades |&gt;\n  correlate() |&gt;\n  shave() |&gt;\n  fashion()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n     term grade  GPA lecture nclicks\n1   grade                           \n2     GPA   .44                     \n3 lecture   .15  .30                \n4 nclicks   .52  .61     .21        \n\n\n\npairs(grades)\n\n\n\n\n\n\n\nFigure 3.1: All pairwise relationships in the grades dataset.\n\n\n\n\n\n\n\n3.1.2 Estimation and interpretation\nTo estimate the regression coefficients (the \\(\\beta\\)s), we will use the lm() function. For a GLM with \\(m\\) predictors:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_m X_{mi} + e_i\n\\]\nThe call to base R’s lm() is\nlm(Y ~ X1 + X2 + ... + Xm, data)\nThe Y variable is your response variable, and the X variables are the predictor variables. Note that you don’t need to explicitly specify the intercept or residual terms; those are included by default.\nFor the current data, let’s predict grade from lecture and nclicks.\n\nmy_model &lt;- lm(grade ~ lecture + nclicks, grades)\n\nsummary(my_model)\n\n\nCall:\nlm(formula = grade ~ lecture + nclicks, data = grades)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.77648 -0.45633  0.04778  0.49755  1.54089 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.100047   0.609603  -1.805   0.0743 .  \nlecture      0.024888   0.045623   0.546   0.5867    \nnclicks      0.033941   0.005903   5.750 1.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7729 on 97 degrees of freedom\nMultiple R-squared:  0.272, Adjusted R-squared:  0.257 \nF-statistic: 18.12 on 2 and 97 DF,  p-value: 2.061e-07\n\n\nWe’ll often write the parameter symbol with a little hat on top to make clear that we are dealing with estimates from the sample rather than the (unknown) true population values. From above:\n\n\\(\\hat{\\beta}_0\\) = -1.1\n\\(\\hat{\\beta}_1\\) = 0.02\n\\(\\hat{\\beta}_2\\) = 0.03\n\nThis tells us that a person’s predicted grade is related to their lecture attendance and download rate by the following formula:\ngrade = -1.1 + 0.02 \\(\\times\\) lecture + 0.03 \\(\\times\\) nclicks\nBecause \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) are both positive, we know that higher values of lecture and nclicks are associated with higher grades.\nSo if you were asked, what grade would you predict for a student who attends 3 lectures and downloaded 70 times, you could easily figure that out by substituting the appropriate values.\ngrade = -1.1 + 0.02 \\(\\times\\) 3 + 0.03 \\(\\times\\) 70\nwhich equals\ngrade = -1.1 + 0.06 + 2.1\nand reduces to\ngrade = 1.06\n\n\n3.1.3 Predictions from the linear model using predict()\nIf we want to predict response values for new predictor values, we can use the predict() function in base R.\npredict() takes two main arguments. The first argument is a fitted model object (i.e., my_model from above) and the second is a data frame (or tibble) containing new values for the predictors.\n\n\n\n\n\n\nCaution\n\n\n\nYou need to include all of the predictor variables in the new table. You’ll get an error message if your tibble is missing any predictors. You also need to make sure that the variable names in the new table exactly match the variable names in the model.\n\n\nLet’s create a tibble with new values and try it out.\n\n## a 'tribble' is a way to make a tibble by rows,\n## rather than by columns. This is sometimes useful\nnew_data &lt;- tribble(~lecture, ~nclicks,\n                    3, 70,\n                    10, 130,\n                    0, 20,\n                    5, 100)\n\n\n\n\n\n\n\nNote\n\n\n\nThe tribble() function provides a way to build a tibble row by row, whereas with tibble() the table is built column by column.\nThe first row of the tribble() contains the column names, each preceded by a tilde (~).\nThis is sometimes easier to read than doing it row by row, although the result is the same. Consider that we could have made the above table using\n\nnew_data &lt;- tibble(lecture = c(3, 10, 0, 5),\n                   nclicks = c(70, 130, 20, 100))\n\n\n\nNow that we’ve created our table new_data, we just pass it to predict() and it will return a vector with the predictions for \\(Y\\) (grade).\n\npredict(my_model, new_data)\n\n        1         2         3         4 \n 1.350520  3.561225 -0.421218  2.418540 \n\n\nThat’s great, but maybe we want to line it up with the predictor values. We can do this by just adding it as a new column to new_data.\n\nnew_data |&gt;\n  mutate(predicted_grade = predict(my_model, new_data))\n\n# A tibble: 4 × 3\n  lecture nclicks predicted_grade\n    &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1       3      70           1.35 \n2      10     130           3.56 \n3       0      20          -0.421\n4       5     100           2.42 \n\n\nWant to see more options for predict()? Check the help at ?predict.lm.\n\n\n3.1.4 Visualizing partial effects\nAs noted above the parameter estimates for each regression coefficient tell us about the partial effect of that variable; it’s effect holding all of the others constant. Is there a way to visualize this partial effect? Yes, you can do this using the predict() function, by making a table with varying values for the focal predictor, while filling in all of the other predictors with their mean values.\nFor example, let’s visualize the partial effect of lecture on grade holding nclicks constant at its mean value.\n\nnclicks_mean &lt;- grades |&gt; pull(nclicks) |&gt; mean()\n\n## new data for prediction\nnew_lecture &lt;- tibble(lecture = 0:10,\n                      nclicks = nclicks_mean)\n\n## add the predicted value to new_lecture\nnew_lecture2 &lt;- new_lecture |&gt;\n  mutate(grade = predict(my_model, new_lecture))\n\nnew_lecture2\n\n# A tibble: 11 × 3\n   lecture nclicks grade\n     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1       0    99.0  2.26\n 2       1    99.0  2.28\n 3       2    99.0  2.31\n 4       3    99.0  2.33\n 5       4    99.0  2.36\n 6       5    99.0  2.38\n 7       6    99.0  2.41\n 8       7    99.0  2.43\n 9       8    99.0  2.46\n10       9    99.0  2.48\n11      10    99.0  2.51\n\n\nNow let’s plot.\n\nggplot(grades, aes(lecture, grade)) + \n  geom_point() +\n  geom_line(data = new_lecture2)\n\n\n\n\n\n\n\nFigure 3.2: Partial effect of ‘lecture’ on grade, with nclicks at its mean value.\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPartial effect plots only make sense when there are no interactions in the model between the focal predictor and any other predictor.\nThe reason is that when there are interactions, the partial effect of focal predictor \\(X_i\\) will differ across the values of the other variables it interacts with.\n\n\nNow can you visualize the partial effect of nclicks on grade?\nSee the solution at the bottom of the page.\n\n\n3.1.5 Standardizing coefficients\nOne kind of question that we often use multiple regression to address is, Which predictors matter most in predicting Y?\nNow, you can’t just read off the \\(\\hat{\\beta}\\) values and choose the one with the largest absolute value, because the predictors are all on different scales. To answer this question, you need to center and scale the predictors.\nRemember \\(z\\) scores?\n\\[\nz = \\frac{X - \\bar{X}}{S_x}\n\\]\nA \\(z\\) score represents the distance of a score \\(X\\) from the sample mean (\\(\\bar{X}\\)) in standard deviation units (\\(S_x\\)). So a \\(z\\) score of 1 means that the score is one standard deviation about the mean; a \\(z\\)-score of -2.5 means 2.5 standard deviations below the mean. \\(Z\\)-scores give us a way of comparing things that come from different populations by calibrating them to the standard normal distribution (a distribution with a mean of 0 and a standard deviation of 1).\nSo we re-scale our predictors by converting them to \\(z\\)-scores. This is easy enough to do.\n\ngrades2 &lt;- grades |&gt;\n  mutate(lecture_z = (lecture - mean(lecture)) / sd(lecture),\n         nclicks_z = (nclicks - mean(nclicks)) / sd(nclicks))\n\ngrades2\n\n# A tibble: 100 × 6\n   grade   GPA lecture nclicks lecture_z nclicks_z\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  4     2.52      10     108     1.72      0.670\n 2  3.02  2.73       7      93     0        -0.444\n 3  1.47  1.55       4      71    -1.72     -2.08 \n 4  1.21  2.55      10     101     1.72      0.150\n 5  1.90  2.46       9      84     1.15     -1.11 \n 6  3.38  2.25       6      93    -0.574    -0.444\n 7  4     3.45       8     135     0.574     2.68 \n 8  3.47  2.96       6     126    -0.574     2.01 \n 9  2.59  3.22       7     109     0         0.745\n10  1.87  2.64       7      74     0        -1.86 \n# ℹ 90 more rows\n\n\nNow let’s re-fit the model using the centered and scaled predictors.\n\nmy_model_scaled &lt;- lm(grade ~ lecture_z + nclicks_z, grades2)\n\nsummary(my_model_scaled)\n\n\nCall:\nlm(formula = grade ~ lecture_z + nclicks_z, data = grades2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.77648 -0.45633  0.04778  0.49755  1.54089 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.43370    0.07729  31.489  &lt; 2e-16 ***\nlecture_z    0.04332    0.07942   0.546    0.587    \nnclicks_z    0.45665    0.07942   5.750 1.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7729 on 97 degrees of freedom\nMultiple R-squared:  0.272, Adjusted R-squared:  0.257 \nF-statistic: 18.12 on 2 and 97 DF,  p-value: 2.061e-07\n\n\nThis tells us that nclicks_z has a relatively larger influence; for each standard deviation increase in this variable, grade increases by about 0.46.\nAnother common approach to standardization involves standardizing the response variable as well as the predictors, i.e., \\(z\\)-scoring the \\(Y\\) values as well as the \\(X\\) values. The relative rank order of the regression coefficients will be the same under this approach. The main difference would be that the coefficients will be expressed in standard deviation (\\(SD\\)) units of the response variable, rather than in raw units.\n\n\n\n\n\n\nMulticollinearity\n\n\n\nIn discussions about multiple regression you may hear concerns expressed about “multicollinearity”, which is a fancy way of referring to the existence of intercorrelations among the predictor variables. This is only a potential problem insofar as it potentially affects the interpretation of the effects of individual predictor variables. When predictor variables are correlated, \\(\\beta\\) values can change depending upon which predictors are included or excluded from the model, sometimes even changing signs. The key things to keep in mind about this are:\n\ncorrelated predictors are probably unavoidable in observational studies;\nregression does not assume that your predictor variables are independent from one another (in other words, finding correlations amongst your predictors is not itself a reason to question your model);\nwhen strong correlations are present, use caution in interpreting individual regression coefficients;\nthere is no known “remedy” for it, nor is it clear that any such remedy is desireable, and many so-called remedies do more harm than good.\n\nFor more information and guidance, see Vanhove (2021).\n\n\n\n\n3.1.6 Model comparison\nAnother common kind of question multiple regression is also used to address is of the form: Does some predictor or set of predictors of interest significantly impact my response variable over and above the effects of some control variables?\nFor example, we saw above that the model including lecture and nclicks was statistically significant, \\(F(2,\n97) =\n18.118\\), \\(p &lt; \\\\.001\\).\nThe null hypothesis for a regression model with \\(m\\) predictors is\n\\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_m = 0;\\]\nin other words, that all of the coefficients (except the intercept) are zero. If the null hypothesis is true, then the null model\n\\[Y_i = \\beta_0\\]\ngives just as good of a prediction as the model including all of the predictors and their coefficients. In other words, your best prediction for \\(Y\\) is just its mean (\\(\\mu_y\\)); the \\(X\\) variables are irrelevant. We rejected this null hypothesis, which implies that we can do better by including our two predictors, lecture and nclicks.\nBut you might ask: maybe its the case that better students get better grades, and the relationship between lecture, nclicks, and grade is just mediated by student quality. After all, better students are more likely to go to lecture and download the materials. So we can ask, are attendance and downloads associated with better grades above and beyond student ability, as measured by GPA?\nThe way we can test this hypothesis is by using model comparison. The logic is as follows. First, estimate a model containing any control predictors but excluding the focal predictors of interest. Second, estimate a model containing the control predictors as well as the focal predictors. Finally, compare the two models, to see if there is any statistically significant gain by including the predictors.\nHere is how you do this:\n\nm1 &lt;- lm(grade ~ GPA, grades) # control model\nm2 &lt;- lm(grade ~ GPA + lecture + nclicks, grades) # bigger model\n\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: grade ~ GPA\nModel 2: grade ~ GPA + lecture + nclicks\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     98 64.030                                \n2     96 56.114  2    7.9165 6.7718 0.001774 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe null hypothesis is that we are just as good predicting grade from GPA as we are predicting it from GPA plus lecture and nclicks. We will reject the null if adding these two variables leads to a substantial enough reduction in the residual sums of squares (RSS); i.e., if they explain away enough residual variance.\nWe see that this is the case: \\(F(2, 96 ) = 6.772\\), \\(p = 0.002\\). So the analysis supports the idea that lecture attendance and downloading the online materials is associated with better grades above and beyond student ability, as measured by GPA.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiple-reg.html#dealing-with-categorical-predictors",
    "href": "multiple-reg.html#dealing-with-categorical-predictors",
    "title": "3  Multiple regression",
    "section": "3.2 Dealing with categorical predictors",
    "text": "3.2 Dealing with categorical predictors\nA regression formula characterizes the response variable as the sum of weighted predictors. What if one of the predictors is categorical (e.g., representing groups such as “rural” or “urban”) rather than numeric? Many variables are nominal: a categorical variable containing names, for which there is no inherent ordering among the levels of the variable. Pet ownership (cat, dog, ferret) is a nominal variable; preferences aside, owning a cat is not greater than owning a dog, and owning a dog is not greater than owning a ferret.\n\n\n\n\n\n\nRepresenting nominal data using numeric predictors\n\n\n\nRepresenting a nominal variable with \\(k\\) levels in a regression model requires \\(k-1\\) numeric predictors; for instance, if you have four levels, you need three predictors. Most numerical coding schemes require that you choose one of these \\(k\\) levels as a baseline level. Each of your \\(k-1\\) variables contrasts one of the other levels level with the baseline.\nExample: You have a variable, pet_type with three levels (cat, dog, ferret).\nYou choose cat as the baseline, and create two numeric predictor variables:\n\ndog_v_cat to encode the contrast between dog and cat, and\nferret_v_cat to encode the contrast between ferret and cat.\n\n\n\nNominal variables are typically represented in a data frame as type character or factor.\nThe difference between a character and a factor variable is that factors contain information about the levels and their order, while character vectors lack this information.\nWhen you specify a model using the R formula syntax, R will check the data types of the predictors on the right hand side of the formula. For example, if your model regresses income on pet_type (e.g., income ~ pet_type), R checks the data type of pet_type.\nFor any variable of type character or factor, R will implicitly create a numeric predictor (or a set of predictors) to represent that variable in the model. There are different schemes available for creating numeric representations of nominal variables. The default in R is to use dummy (or ‘treatment’) coding (see below). Unfortunately, this default is unsuitable for many types of study designs in psychology, so I am going to recommend that you learn how to code your own predictor variables “by hand,” and that you make a habit of doing so.\n\n\n\n\n\n\nDon’t represent levels of a categorical variable with numbers!\n\n\n\nIn the above example, we had a variable pet_type with levels cat, dog, and ferret. Sometimes people represent the levels of a nominal variable with numbers, like so:\n\n1 for cat,\n2 for dog,\n3 for ferret.\n\nThis is a bad idea.\nFirst, the labeling is arbitrary and opaque and anyone attempting to use your data would not know which number goes with which category (and you could also forget!).\nEven worse, if you were to put this variable in as a predictor in a regression model, R would have no way of knowing your intention to use 1, 2, and 3 as arbitrary labels for groups, and would instead assume that pet_type is a measurement for which dogs are 1 unit greater than cats, and ferrets are 2 units greater than cats and 1 unit greater than dogs, which is nonsense!\nIt is far too easy to make this mistake, and difficult to catch if authors do not share their data and code. In 2016, a paper on religious affiliation and altruism in children that was published in Current Biology had to be retracted for just this kind of mistake.\nSo, don’t represent the levels of a nominal variable with numbers, except of course when you deliberately create predictor variables encoding the \\(k-1\\) contrasts needed to properly represent a nominal variable in a regression model.\nIf you get data where someone has done this, you can convert the problematic variable into a factor by using the factor() function.\n\n\n\n3.2.1 Dummy (a.k.a. “treatment”) coding\nFor a nominal variable with only two levels, choose one level as baseline, and create a new variable that is 0 whenever the level is baseline and 1 when it is the other level. The choice of baseline is arbitrary, and will affect only whether the coefficient is positive or negative, but not its magnitude, its standard error nor the associated p-value.\nTo illustrate, let’s gin up some fake data with a single two level categorical predictor.\n\nfake_data &lt;- tibble(Y = rnorm(10),\n                    group = rep(c(\"A\", \"B\"), each = 5))\n\nfake_data\n\n# A tibble: 10 × 2\n         Y group\n     &lt;dbl&gt; &lt;chr&gt;\n 1  0.145  A    \n 2 -0.0212 A    \n 3 -0.462  A    \n 4  0.140  A    \n 5  0.715  A    \n 6  1.50   B    \n 7 -0.791  B    \n 8  1.20   B    \n 9 -0.436  B    \n10 -0.694  B    \n\n\nNow let’s add a new variable, group_d, which is the dummy coded group variable. We will use the dplyr::if_else() function to define the new column.\n\nfake_data2 &lt;- fake_data |&gt;\n  mutate(group_d = if_else(group == \"B\", 1, 0))\n\nfake_data2\n\n# A tibble: 10 × 3\n         Y group group_d\n     &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1  0.145  A           0\n 2 -0.0212 A           0\n 3 -0.462  A           0\n 4  0.140  A           0\n 5  0.715  A           0\n 6  1.50   B           1\n 7 -0.791  B           1\n 8  1.20   B           1\n 9 -0.436  B           1\n10 -0.694  B           1\n\n\nNow we just run it as a regular regression model.\n\nsummary(lm(Y ~ group_d, fake_data2))\n\n\nCall:\nlm(formula = Y ~ group_d, data = fake_data2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.94700 -0.58576 -0.04399  0.46942  1.34136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.10336    0.37349   0.277    0.789\ngroup_d      0.05266    0.52819   0.100    0.923\n\nResidual standard error: 0.8351 on 8 degrees of freedom\nMultiple R-squared:  0.001241,  Adjusted R-squared:  -0.1236 \nF-statistic: 0.009941 on 1 and 8 DF,  p-value: 0.923\n\n\nLet’s reverse the coding. We get the same result, just the sign is different.\n\nfake_data3 &lt;- fake_data |&gt;\n  mutate(group_d = if_else(group == \"A\", 1, 0))\n\nsummary(lm(Y ~ group_d, fake_data3))\n\n\nCall:\nlm(formula = Y ~ group_d, data = fake_data3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.94700 -0.58576 -0.04399  0.46942  1.34136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.15603    0.37349   0.418    0.687\ngroup_d     -0.05266    0.52819  -0.100    0.923\n\nResidual standard error: 0.8351 on 8 degrees of freedom\nMultiple R-squared:  0.001241,  Adjusted R-squared:  -0.1236 \nF-statistic: 0.009941 on 1 and 8 DF,  p-value: 0.923\n\n\nThe interpretation of the intercept is the estimated mean for the group coded as zero. You can see by plugging in zero for X in the prediction formula below. Thus, \\(\\beta_1\\) can be interpreted as the difference between the mean for the baseline group and the group coded as 1.\n\\[\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i \\]\nNote that if we just put the character variable group as a predictor in the model, R will automatically create a dummy variable (or variables) for us as needed.\n\nlm(Y ~ group, fake_data) |&gt;\n  summary()\n\n\nCall:\nlm(formula = Y ~ group, data = fake_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.94700 -0.58576 -0.04399  0.46942  1.34136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.10336    0.37349   0.277    0.789\ngroupB       0.05266    0.52819   0.100    0.923\n\nResidual standard error: 0.8351 on 8 degrees of freedom\nMultiple R-squared:  0.001241,  Adjusted R-squared:  -0.1236 \nF-statistic: 0.009941 on 1 and 8 DF,  p-value: 0.923\n\n\nThe lm() function examines group and figures out the unique levels of the variable, which in this case are A and B. It then chooses as baseline the level that comes first alphabetically, and encodes the contrast between the other level (B) and the baseline level (A). (In the case where group has been defined as a factor, the baseline level is the first element of levels(fake_data$group)).\nThis new variable that it created shows up with the name groupB in the output.\n\n\n3.2.2 Dummy coding when \\(k &gt; 2\\)\nWhen a nominal predictor variable has more than two levels (\\(k &gt; 2\\)), one numeric predictor is no longer sufficient; we need \\(k-1\\) predictors. If the nominal predictor has four levels, we’ll need to define three predictors. Let’s simulate some data to work with, season_wt, which represents a person’s bodyweight (in kg) over the four seasons of the year.\n\nseason_wt &lt;- tibble(season = rep(c(\"winter\", \"spring\", \"summer\", \"fall\"),\n                                 each = 5),\n                    bodyweight_kg = c(rnorm(5, 105, 3),\n                                      rnorm(5, 103, 3),\n                                      rnorm(5, 101, 3),\n                                      rnorm(5, 102.5, 3)))\n\nseason_wt\n\n# A tibble: 20 × 2\n   season bodyweight_kg\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 winter         102. \n 2 winter         105. \n 3 winter         107. \n 4 winter         102. \n 5 winter         109. \n 6 spring         102. \n 7 spring          96.9\n 8 spring          99.9\n 9 spring         103. \n10 spring         105. \n11 summer         102. \n12 summer         102. \n13 summer         106. \n14 summer         107. \n15 summer         105. \n16 fall            99.5\n17 fall           103. \n18 fall           102. \n19 fall           105. \n20 fall           101. \n\n\nNow let’s add three predictors to code the variable season. Try it out and see if you can figure out how it works.\n\n## baseline value is 'winter'\nseason_wt2 &lt;- season_wt |&gt;\n  mutate(spring_v_winter = if_else(season == \"spring\", 1, 0),\n         summer_v_winter = if_else(season == \"summer\", 1, 0),\n         fall_v_winter = if_else(season == \"fall\", 1, 0))\n\nseason_wt2\n\n# A tibble: 20 × 5\n   season bodyweight_kg spring_v_winter summer_v_winter fall_v_winter\n   &lt;chr&gt;          &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1 winter         102.                0               0             0\n 2 winter         105.                0               0             0\n 3 winter         107.                0               0             0\n 4 winter         102.                0               0             0\n 5 winter         109.                0               0             0\n 6 spring         102.                1               0             0\n 7 spring          96.9               1               0             0\n 8 spring          99.9               1               0             0\n 9 spring         103.                1               0             0\n10 spring         105.                1               0             0\n11 summer         102.                0               1             0\n12 summer         102.                0               1             0\n13 summer         106.                0               1             0\n14 summer         107.                0               1             0\n15 summer         105.                0               1             0\n16 fall            99.5               0               0             1\n17 fall           103.                0               0             1\n18 fall           102.                0               0             1\n19 fall           105.                0               0             1\n20 fall           101.                0               0             1\n\n\n\n\n\n\n\n\nReminder: Always look at your data\n\n\n\nWhenever you write code that potentially changes your data, you should double check that the code works as intended by looking at your data. This is especially the case when you are hand-coding nominal variables for use in regression, because sometimes the code will be wrong, but won’t throw an error.\nConsider the code chunk above, where we defined three contrasts to represent the nominal variable season, with winter as our baseline.\nWhat would happen if you accidently misspelled one of the levels (summre for summer) and didn’t notice?\n\nseason_wt3 &lt;- season_wt |&gt;\n  mutate(spring_v_winter = if_else(season == \"spring\", 1, 0),\n         summer_v_winter = if_else(season == \"summre\", 1, 0),\n         fall_v_winter = if_else(season == \"fall\", 1, 0))\n\nWhile the above code chunk runs, we get confusing output when we run the regression; namely, the coefficent for summer_v_winter is NA (not available).\n\nlm(bodyweight_kg ~ spring_v_winter + summer_v_winter + fall_v_winter,\n   season_wt3)\n\n\nCall:\nlm(formula = bodyweight_kg ~ spring_v_winter + summer_v_winter + \n    fall_v_winter, data = season_wt3)\n\nCoefficients:\n    (Intercept)  spring_v_winter  summer_v_winter    fall_v_winter  \n        104.633           -3.471               NA           -2.435  \n\n\nWhat happened? Let’s look at the data to find out. We will use distinct to find the distinct combinations of our original variable season with the three variables we created (see ?dplyr::distinct for details).\n\nseason_wt3 |&gt;\n  distinct(season, spring_v_winter, summer_v_winter, fall_v_winter)\n\n# A tibble: 4 × 4\n  season spring_v_winter summer_v_winter fall_v_winter\n  &lt;chr&gt;            &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1 winter               0               0             0\n2 spring               1               0             0\n3 summer               0               0             0\n4 fall                 0               0             1\n\n\nBecause of our misspelling, the predictor summer_v_winter is not 1 when season == \"summer\"; instead, it is always zero. The if_else() above literally says ‘set summer_v_winter to 1 if season == \"summre\", otherwise 0’. Of course, season is never equal to summre, because summre is a typo. We could have caught this easily by running the above check with distinct(). Get in the habit of doing this when you create your own numeric predictors.\n\n\n\n\n\n\n\n\nA closer look at R’s defaults\n\n\n\nIf you’ve ever used point-and-click statistical software like SPSS, you probably never had to learn about coding categorical predictors. Normally, the software recognizes when a predictor is categorical and, behind the scenes, it takes care of recoding it into a numerical predictor. R is no different: if you supply a predictor of type character or factor to a linear modeling function, it will create numerical dummy-coded predictors for you, as shown in the code below.\n\nlm(bodyweight_kg ~ season, season_wt) |&gt;\n  summary()\n\n\nCall:\nlm(formula = bodyweight_kg ~ season, data = season_wt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2607 -2.6274  0.3037  1.8606  3.9984 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   102.197      1.186  86.198   &lt;2e-16 ***\nseasonspring   -1.036      1.677  -0.618    0.545    \nseasonsummer    2.224      1.677   1.326    0.203    \nseasonwinter    2.646      1.677   1.578    0.134    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.651 on 16 degrees of freedom\nMultiple R-squared:  0.2936,    Adjusted R-squared:  0.1612 \nF-statistic: 2.217 on 3 and 16 DF,  p-value: 0.1257\n\n\nHere, R implicitly creates three dummy variables to code the four levels of season, called seasonspring, seasonsummer and seasonwinter. The unmentioned season, fall, has been chosen as baseline because it comes earliest in the alphabet. These three predictors have the following values:\n\nseasonspring: 1 if spring, 0 otherwise;\nseasonsummer: 1 if summer, 0 otherwise;\nseasonwinter: 1 if winter, 0 otherwise.\n\nThis seems like a handy thing to have R do for us, but dangers lurk in relying on the default. We’ll learn more about these dangers in the next chapter when we talk about interactions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiple-reg.html#equivalence-between-multiple-regression-and-one-way-anova",
    "href": "multiple-reg.html#equivalence-between-multiple-regression-and-one-way-anova",
    "title": "3  Multiple regression",
    "section": "3.3 Equivalence between multiple regression and one-way ANOVA",
    "text": "3.3 Equivalence between multiple regression and one-way ANOVA\nIf we wanted to see whether our bodyweight varies over season, we could do a one way ANOVA on season_wt2 like so.\n\n## make season into a factor with baseline level 'winter'\nseason_wt3 &lt;- season_wt2 |&gt;\n  mutate(season = factor(season, levels = c(\"winter\", \"spring\",\n                                            \"summer\", \"fall\")))\n\nmy_anova &lt;- aov(bodyweight_kg ~ season, season_wt3)\nsummary(my_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nseason       3  46.74  15.580   2.217  0.126\nResiduals   16 112.45   7.028               \n\n\nOK, now can we replicate that result using the regression model below?\n\\[Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\]\n\nsummary(lm(bodyweight_kg ~ spring_v_winter +\n             summer_v_winter + fall_v_winter,\n           season_wt2))\n\n\nCall:\nlm(formula = bodyweight_kg ~ spring_v_winter + summer_v_winter + \n    fall_v_winter, data = season_wt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2607 -2.6274  0.3037  1.8606  3.9984 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      104.844      1.186  88.430   &lt;2e-16 ***\nspring_v_winter   -3.683      1.677  -2.196   0.0432 *  \nsummer_v_winter   -0.422      1.677  -0.252   0.8045    \nfall_v_winter     -2.646      1.677  -1.578   0.1341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.651 on 16 degrees of freedom\nMultiple R-squared:  0.2936,    Adjusted R-squared:  0.1612 \nF-statistic: 2.217 on 3 and 16 DF,  p-value: 0.1257\n\n\nNote that the \\(F\\) values and \\(p\\) values are identical for the two methods!\n\n\n\n\n\n\nSolution to partial effect plot\n\n\n\n\n\nFirst create a tibble with new predictors. We might also want to know the range of values that nclicks varies over.\n\nlecture_mean &lt;- grades |&gt; pull(lecture) |&gt; mean()\nmin_nclicks &lt;- grades |&gt; pull(nclicks) |&gt; min()\nmax_nclicks &lt;- grades |&gt; pull(nclicks) |&gt; max()\n\n## new data for prediction\nnew_nclicks &lt;- tibble(lecture = lecture_mean,\n                      nclicks = min_nclicks:max_nclicks)\n\n## add the predicted value to new_lecture\nnew_nclicks2 &lt;- new_nclicks |&gt;\n  mutate(grade = predict(my_model, new_nclicks))\n\nnew_nclicks2\n\n# A tibble: 66 × 3\n   lecture nclicks grade\n     &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n 1       7      71  1.48\n 2       7      72  1.52\n 3       7      73  1.55\n 4       7      74  1.59\n 5       7      75  1.62\n 6       7      76  1.65\n 7       7      77  1.69\n 8       7      78  1.72\n 9       7      79  1.76\n10       7      80  1.79\n# ℹ 56 more rows\n\n\nNow plot.\n\nggplot(grades, aes(nclicks, grade)) +\n  geom_point() +\n  geom_line(data = new_nclicks2)\n\n\n\n\n\n\n\nFigure 3.3: Partial effect plot of nclicks on grade.\n\n\n\n\n\n\n\n\n\n\n\n\nVanhove, J. (2021). Collinearity isn’t a disease that needs curing. Meta-Psychology, 5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "interactions.html",
    "href": "interactions.html",
    "title": "4  Interactions",
    "section": "",
    "text": "4.1 Continuous-by-Categorical Interactions\nOne common example of this is when you are interested in whether a linear relationship between a continous predictor and a continuous response is different for two groups.\nLet’s consider a simple fictional example. Say you are interested in the effects of sonic distraction on cognitive performance. Each participant in your study is randomly assigned to receive a particular amount of sonic distraction while they perform a simple reaction time task (respond as quickly as possible to a flashing light). You have a technique that allows you to automatically generate different levels of background noise (e.g., frequency and amplitude of city sounds, such as sirens, jackhammers, people yelling, glass breaking, etc.). Each participant performs the task for a randomly chosen level of distraction (0 to 100). Your hypothesis is that urban living makes people’s task performance more immune to sonic distraction. You want to compare the relationship between distraction and performance for city dwellers to the relationship for people from quieter rural environments.\nYou have three variables:\nLet’s start by simulating some data for the urban group. Let’s assume that with zero distraction (silence), the average RT is about 450 milliseconds, and that with each unit increase on the distraction scale, RT increases about 2 ms. This gives us the following linear model:\n\\[Y_i = 450 + 2 X_i + e_i\\]\nwhere \\(X_i\\) is the amount of sonic distraction.\nLet’s simulate data for 100 participants as below with \\(\\sigma = 80\\), setting the seed before we begin.\nlibrary(\"tidyverse\")\nset.seed(1031)\n\nn_subj &lt;- 100L  # simulate data for 100 subjects\nb0_urban &lt;- 450 # y-intercept\nb1_urban &lt;- 2   # slope\n\n# decomposition table\nurban &lt;- tibble(\n  subj_id = 1:n_subj,\n  group = \"urban\",\n  b0 = 450,\n  b1 = 2,\n  dist_level = sample(0:n_subj, n_subj, replace = TRUE),\n  err = rnorm(n_subj, mean = 0, sd = 80),\n  simple_rt = b0 + b1 * dist_level + err)\n\nurban\n\n# A tibble: 100 × 7\n   subj_id group    b0    b1 dist_level     err simple_rt\n     &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1       1 urban   450     2         59  -36.1       532.\n 2       2 urban   450     2         45  128.        668.\n 3       3 urban   450     2         55   23.5       584.\n 4       4 urban   450     2          8    1.04      467.\n 5       5 urban   450     2         47   48.7       593.\n 6       6 urban   450     2         96   88.2       730.\n 7       7 urban   450     2         62  110.        684.\n 8       8 urban   450     2          8  -91.6       374.\n 9       9 urban   450     2         15 -109.        371.\n10      10 urban   450     2         70   20.7       611.\n# ℹ 90 more rows\nLet’s plot the data we created, along with the line of best fit.\nggplot(urban, aes(dist_level, simple_rt)) + \n  geom_point(alpha = .2) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\nFigure 4.1: Effect of sonic distraction on simple RT, urban group.\nNow let’s simulate data for the rural group. We assume that these participants should perhaps have a little higher intercept, maybe because they are less familiar with technology. Most importantly, we assume that they would have a steeper slope because they are more affected by the noise. Something like:\n\\[Y_i = 500 + 3 X_i + e_i\\]\nb0_rural &lt;- 500\nb1_rural &lt;- 3\n\nrural &lt;- tibble(\n  subj_id = 1:n_subj + n_subj,\n  group = \"rural\",\n  b0 = b0_rural,\n  b1 = b1_rural,\n  dist_level = sample(0:n_subj, n_subj, replace = TRUE),\n  err = rnorm(n_subj, mean = 0, sd = 80),\n  simple_rt = b0 + b1 * dist_level + err)\nNow let’s plot the data from the two groups side by side.\nall_data &lt;- bind_rows(urban, rural)\n\nggplot(all_data |&gt; mutate(group = fct_relevel(group, \"urban\")), \n       aes(dist_level, simple_rt, colour = group)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~ group) + \n  theme(legend.position = \"none\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 4.2: Effect of sonic distraction on simple RT for urban and rural participants.\nHere we see very clearly the difference in slope that we built into our data. How do we test whether the two slopes are significantly different? To do this, we can’t have two separate regressions. We need to bring the two regression lines into the same model. How do we do this?\nNote that we can represent one of the regression lines in terms of ‘offset’ values from the other. We (arbitrarily) choose one group as our ‘baseline’ group, and represent the y-intercept and slope of the other group as offsets from this baseline. So if we choose the urban group as our baseline, we can express the y-intercept and slope for the rural group in terms of two offsets, \\(\\beta_2\\) and \\(\\beta_3\\), for the y-intercept and slope, respectively.\nOur urban group had parameters \\(\\beta_{0\\_urban} = 450\\) and \\(\\beta_{1\\_urban} = 2\\), whereas the rural group had \\(\\beta_{0\\_rural} = 500\\) and \\(\\beta_{1\\_rural} = 3\\). It directly follows that:\nSo our two regression models are now:\n\\[Y_{i\\_urban} = \\beta_{0\\_urban} + \\beta_{1\\_urban} X_i + e_i\\]\nand\n\\[Y_{i\\_rural} = (\\beta_{0\\_urban} + \\beta_2) + (\\beta_{1\\_urban} + \\beta_3) X_i + e_i.\\]\nOK, it seems like we’re closer to getting these into a single regression model. Here’s the final trick. We define an additional dummy predictor variable that takes on the value 0 for the urban group (which we chose as our ‘baseline’ group) and 1 for the other group. The box below contains our final model.\nThe term \\(\\beta_3 X_{1i} X_{2i}\\), which has the two predictors multiplied together, is called an interaction term. Let’s now show that the above GLM gives us the two regression lines that we want.\nTo derive the regression equation for the urban group, we plug in 0 for \\(X_{2i}\\). This gives us\n\\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 0 + \\beta_3 X_{1i} 0 + e_i\\]\nwhich, dropping terms that are zero, is just\n\\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + e_i,\\]\nwhich is the regression equation for our baseline (urban) group. Compare this to \\(Y_{i\\_urban}\\) above.\nPlugging in 1 for \\(X_{2i}\\) should give us the equation for our rural group. We get\n\\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 1 + \\beta_3 X_{1i} 1 + e_i\\]\nwhich, after reducing and applying a little algebra, can also be expressed as\n\\[Y_{i} = \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) X_{1i} + e_i.\\]\nCompare this to \\(Y_{i\\_rural}\\) above. The dummy-coding trick works!\nHow do we estimate the regression coefficients in R. Let’s say we wanted to test the hypothesis that the slopes for the two lines are different. Note that this just amounts to testing the null hypothesis that \\(\\beta_3 = 0\\), because \\(\\beta_3\\) is our slope offset. If this parameter is zero, that means there is a single slope for both groups (although they can have different intercepts). In other words, that means the two slopes are parallel. If it is non-zero, that means that the two groups have different slopes; that is, the two slopes are not parallel.\nWe have already created the dataset all_data combining the simulated data for our two groups. The way we express our model using R formula syntax is Y ~ X1 + X2 + X1:X2 where X1:X2 tells R to create a predictor that is the product of predictors X1 and X2. There is a shortcut Y ~ X1 * X2 which tells R to calculate all possible main effects and interactions. First we’ll add a dummy predictor to our model, storing the result in all_data2.\nall_data2 &lt;- all_data |&gt;\n  mutate(grp = if_else(group == \"rural\", 1, 0))\nsonic_mod &lt;- lm(simple_rt ~ dist_level + grp + dist_level:grp,\n                all_data2)\n\nsummary(sonic_mod)\n\n\nCall:\nlm(formula = simple_rt ~ dist_level + grp + dist_level:grp, data = all_data2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-261.130  -50.749    3.617   62.304  191.211 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    460.1098    15.5053  29.674  &lt; 2e-16 ***\ndist_level       1.9123     0.2620   7.299 7.07e-12 ***\ngrp              4.8250    21.7184   0.222    0.824    \ndist_level:grp   1.5865     0.3809   4.166 4.65e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 81.14 on 196 degrees of freedom\nMultiple R-squared:  0.5625,    Adjusted R-squared:  0.5558 \nF-statistic: 83.99 on 3 and 196 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "interactions.html#cont-by-cat",
    "href": "interactions.html#cont-by-cat",
    "title": "4  Interactions",
    "section": "",
    "text": "A continuous response variable, mean_RT, with higher levels reflecting slower RTs;\nA continuous predictor variable, level of sonic distraction (dist_level), with higher levels indicating more distraction;\nA factor with two levels, group (urban vs. rural).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny-intercept: \\(\\beta_{0\\_rural} = \\beta_{0\\_urban} + \\beta_2\\)\nslope: \\(\\beta_{1\\_rural} = \\beta_{1\\_urban} + \\beta_3\\)\n\n\n\n\\(\\beta_2 = 50\\), because \\(\\beta_{0\\_rural} - \\beta_{0\\_urban} = 500 - 450 = 50\\), and\n\\(\\beta_3 = 1\\), because \\(\\beta_{1\\_rural} - \\beta_{1\\_urban} = 3 - 2 = 1\\).\n\n\n\n\n\n\n\n\n\n\n\n\nRegression model with a continuous-by-categorical interaction\n\n\n\n\\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i} + e_{i}\\]\nwhere\n\n\\(X_{1i}\\) is the continous predictor, and\n\\(X_{2i}\\) is a dummy-coded variable taking on 0 for the baseline, 1 for the alternative group.\n\nInterpretation of parameters:\n\n\\(\\beta_0\\): y-intercept for the baseline group;\n\\(\\beta_1\\): slope for the baseline group;\n\\(\\beta_2\\): offset to y-intercept for the alternative group;\n\\(\\beta_3\\): offset to slope for the alternative group.\n\nEstimation in R:\nlm(Y ~ X1 + X2 + X1:X2) or, as a shortcut:\nlm(Y ~ X1 * X2) where * means “all possible main effects and interactions of X1 and X2”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel lines in the sample versus in the population\n\n\n\nI just said that two non-parallel lines mean there is an interaction between the categorical and continuous predictors, and that parallel lines mean no interaction. It is important to be clear that I am talking about whether or not the lines are parallel in the population. Whether or not they are parallel in the sample depends not only on their status in the population, but also on biases introduced by measurement and by sampling. Lines that are parallel in the population are nonetheless extremely likely to give rise to lines in the sample with slopes that appear different, especially if your sample is small.\nGenerally, you will be interested in whether the slopes are the same or different in the population, not in the sample. For this reason, you can’t just look at a graph of sample data and reason, “the lines are not parallel and so there must be an interaction”, or vice versa, “the lines look parallel, so there is no interaction.” You must run an inferential statistical test.\nWhen the interaction term is statistically significant at some \\(\\alpha\\) level (e.g., 0.05), you reject the null hypothesis that the interaction coefficient is zero (e.g., \\(H_0: \\beta_3 = 0\\)), which implies the lines are not parallel in the population.\nHowever, a non-significant interaction does not necessarily imply that the lines are parallel in the population. They might be, but it’s also possible that they are not, and your study just lacked sufficient power to detect the difference.\nThe best you can do to get evidence for the null hypothesis is to run what is called an equivalence test, where you seek to reject a null hypothesis that the population effect is larger than some smallest effect size of interest; see Lakens et al. (2018) for a tutorial.\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\nGiven the regression model\n\\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i} + e_{i}\\]\n(where \\(X_{1i}\\) is the continuous predictor and \\(X_{2i}\\) is the categorical predictor) and the output from lm() above, identify \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\), and \\(\\hat{\\beta}_3\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\hat{\\beta_0}\\) = 460.110\n\\(\\hat{\\beta_1}\\) = 1.912\n\\(\\hat{\\beta_2}\\) = 4.825\n\\(\\hat{\\beta_3}\\) = 1.587\n\n\n\n\nBased on these parameter estimates, what is the regression line for the (baseline) urban group?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe regression line for the urban group is just\n\\(Y_i = \\beta_0 + \\beta_1 X_{1i}\\) which is\n\\(Y_i =\\) 460.110 \\(+\\) 1.912 \\(X_{1i}\\)\n\n\n\nWhat is the regression line for the rural group?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(Y_i = \\beta_0 + \\beta_2 + \\left(\\beta_1 + \\beta_3\\right) X_{1i}\\) or\n\\(Y_i =\\) 464.935 \\(+\\) 3.499 \\(X_{1i}\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "interactions.html#categorical-by-categorical-interactions",
    "href": "interactions.html#categorical-by-categorical-interactions",
    "title": "4  Interactions",
    "section": "4.2 Categorical-by-Categorical Interactions",
    "text": "4.2 Categorical-by-Categorical Interactions\nFactorial designs are very common in psychology, and are most often analyzed using ANOVA-based techniques, which can obscure the fact that ANOVA, like regression, also assumes an underlying linear model.\nA factorial design is one in which the predictors (IVs) are all categorical: each is a factor having a fixed number of levels. In a full-factorial design, the factors are fully crossed with each other such that each possible combination of factors is represented. We call each unique combination a cell of the design. You will often hear designs referred to as “a two-by-two design” (2x2), which means that there are two factors, each of which has two levels. A “three-by-three” (3x3) design is one where there are two factors, each with three levels; a “two-by-two-by-two” 2x2x2 design is one in which there are three factors, each with two levels; and so on.\nTypically, factorial designs are given a tabular representation, showing all the combinations of factor levels. Below is a tabular representation of a 2x2 design.\n\n\n\n\nTable 4.1\n\n\n\n\n \n  \n     \n    \\(B_1\\) \n    \\(B_2\\) \n  \n \n\n  \n    \\(A_1\\) \n    \\(AB_{11}\\) \n    \\(AB_{12}\\) \n  \n  \n    \\(A_2\\) \n    \\(AB_{21}\\) \n    \\(AB_{22}\\) \n  \n\n\n\n\n\n\n\n\nA 3x2 design might be shown as follows.\n\n\n\n\nTable 4.2\n\n\n\n\n \n  \n     \n    \\(B_1\\) \n    \\(B_2\\) \n  \n \n\n  \n    \\(A_1\\) \n    \\(AB_{11}\\) \n    \\(AB_{12}\\) \n  \n  \n    \\(A_2\\) \n    \\(AB_{21}\\) \n    \\(AB_{22}\\) \n  \n  \n    \\(A_3\\) \n    \\(AB_{31}\\) \n    \\(AB_{32}\\) \n  \n\n\n\n\n\n\n\n\nAnd finally, here’s a 2x2x2 design.\n\n\n\n\n\n\nTable 4.3: \\(C_1\\)\n\n\n\n\n \n  \n     \n    \\(B_1\\) \n    \\(B_2\\) \n  \n \n\n  \n    \\(A_1\\) \n    \\(ABC_{111}\\) \n    \\(ABC_{121}\\) \n  \n  \n    \\(A_2\\) \n    \\(ABC_{211}\\) \n    \\(ABC_{221}\\) \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.4: \\(C_2\\)\n\n\n\n\n \n  \n     \n    \\(B_1\\) \n    \\(B_2\\) \n  \n \n\n  \n    \\(A_1\\) \n    \\(ABC_{112}\\) \n    \\(ABC_{122}\\) \n  \n  \n    \\(A_2\\) \n    \\(ABC_{212}\\) \n    \\(ABC_{222}\\) \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t confuse factors and levels!\n\n\n\nIf you hear about a study that has three treatment groups (treatment A, treatment B, and control), that is not a “three-factor (three-way) design”. That is a one-factor (one-way) design with a single three-level factor (treatment condition).\nThere is no such thing as a factor that has only one level.\n\n\nYou can find out how many cells a design has by multiplying the number of levels of each factor. So, a 2x3x4 design would have 24 cells in the design.\n\n\n\n\n\n\nFactorial plots\n\n\n\nA common way to visualize data from a factorial design is through a factorial plot. For a factorial plot, you compute the mean for each cell, and then (arbitrarily) choose one of the two categorical factors to form the x-axis of your plot. You distinguish the levels of the other categorical factor by having a separate line for each level, and possibly different shapes and colors as well.\nHere is some made up data looking at people’s performance on a cognitive task while listening to music at a low or high volume.\n\ncogdata &lt;- tibble::tribble(~ genre, ~ volume, ~ mean_performance,\n                           \"metal\", \"low\", 8.2,\n                           \"metal\", \"high\", 5.2,\n                           \"classical\", \"low\", 8.0,\n                           \"classical\", \"high\", 3.3)\n\nCan you recreate the following factorial plot using ggplot2?\n\n\n\n\n\n\n\n\nFigure 4.3: The effect of music genre and volume on cognitive performance.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(\"ggplot2\")\n\nggplot(cogdata, aes(volume, mean_performance,\n                    color = genre)) +\n  geom_point(aes(shape = genre), size = 3) +\n  geom_line(aes(group = genre)) +\n  coord_cartesian(ylim = c(0, 10))\n\n\n\n\n\n\n\n4.2.1 Effects of cognitive therapy and drug therapy on mood\nLet’s consider a simple factorial design and think about the types of patterns our data can show. After we get the concepts down from this concrete example, we’ll map them onto the more abstract statistical terminology.\nImagine you’ve running a study looking at effects of two different types of therapy for depressed patients, cognitive therapy and drug therapy. Half of the participants are randomly assigned to receive Cognitive Behavioral Therapy (CBT) and the other half get some other kind of control activity. Also, you further divide your patients through random assignment into a drug therapy group, whose members receive anti-depressants, and an control group, whose members receive a placebo. After treatment (or control/placebo), you measure their mood on a scale, with higher numbers corresponding to a more positive mood.\nLet’s imagine that the means we obtain below are the population means, free of measurement or sampling error. We’ll take a moment to consider three different possible outcomes and what they imply about how these therapies might work independently or interactively to affect mood.\n\n\n\n\n\n\nCaution\n\n\n\nThe reminder about populations and samples for categorical-by-continuous interactions also applies here. Except when simulating data, you will almost never know the true means of any population that you are studying. Below, we’re talking about the hypothetical situation where you actually know the population means and can draw conclusions without any statistical tests. Any real sample means you look at will include sampling and measurement error, and any inferences you’d make would depend on the outcome of statistical tests, rather than the observed pattern of means.\n\n\n\nScenario A\nBelow is a table of cell means and marginal means. The cell means are the mean values of the dependent variable (mood) at each cell of the design. The marginal means (in the margins of the table) provide the means for each row and column.\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Scenario A\n\n\n\n\n\n\n\n\n\n\nTable 4.5: Scenario A\n\n\n\n\n \n  \n     \n    No CBT \n    CBT \n     \n  \n \n\n  \n    Placebo \n    40 \n    60 \n    50 \n  \n  \n    Drug \n    60 \n    80 \n    70 \n  \n  \n     \n    50 \n    70 \n     \n  \n\n\n\n\n\n\n\n\n\n\nIf this was our outcome, what would you conclude? Is cognitive therapy having an effect on mood? How about drug therapy. The answer to both of these questions is yes: The mean mood for people who got CBT (70; mean of column 2) is 20 points higher than the mean mood for people who didn’t (50; mean of column 1).\nLikewise, people who got anti-depressants showed enhanced mood (70; mean of row 2) relative to people who got the placebo (50; mean of row 1).\nNow we can also ask the following question: did the effect of cognitive therapy depend on whether or not the patient was simultaneously receiving drug therapy? The answer to this, is no. To see why, note that for the Placebo group (Row 1), cognitive therapy increased mood by 20 points (from 40 to 60). But this was the same for the Drug group: there was an increase of 20 points from 60 to 80. So, no evidence that the effect of one factor on mood depends on the other.\n\n\nScenario B\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: Scenario B\n\n\n\n\n\n\n\n\n\n\nTable 4.6: Scenario B\n\n\n\n\n \n  \n     \n    No CBT \n    CBT \n     \n  \n \n\n  \n    Placebo \n    40 \n    60 \n    50 \n  \n  \n    Drug \n    40 \n    60 \n    50 \n  \n  \n     \n    40 \n    60 \n     \n  \n\n\n\n\n\n\n\n\n\n\nIn this scenario, we also see that CBT improved mood (again, by 20 points), but there was no effect of Drug Therapy (equal marginal means of 50 for row 1 and row 2). We can also see here that the effect of CBT also didn’t depend upon Drug therapy; there is an increase of 20 points in each row.\n\n\nScenario C\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Scenario C\n\n\n\n\n\n\n\n\n\n\nTable 4.7: Scenario C\n\n\n\n\n \n  \n     \n    No CBT \n    CBT \n     \n  \n \n\n  \n    Placebo \n    40 \n    60 \n    50 \n  \n  \n    Drug \n    50 \n    90 \n    70 \n  \n  \n     \n    45 \n    75 \n     \n  \n\n\n\n\n\n\n\n\n\n\nFollowing the logic in previous sections, we see that overall, people who got cognitive therapy showed elevated mood relative to control (75 vs 45), and that people who got drug therapy also showed elevated mood relative to placebo (70 vs 50). But there is something else going on here: it seems that the effect of cognitive therapy on mood was more pronounced for patients who were also receiving drug therapy. For patients on antidepressants, there was a 40 point increase in mood relative to control (from 50 to 90; row 2 of the table). For patients who got the placebo, there was only a 20 point increase in mood, from 40 to 60 (row 1 of the table). So in this hypothetical scenario, the effect of cognitive therapy depends on whether or not there is also ongoing drug therapy.\n\n\n\n4.2.2 Effects in a factorial design\nIf you understand the basic patterns of effects described in the previous section, you are then ready to map these concepts onto statistical language.\n\n4.2.2.1 Main effect\nMain effect: The effect of a factor on the DV ignoring the other factors in the design.\nThe test of a main effect is a test of the equivalence of marginal means. So in Scenario A above, when you compared the row means for drug therapy, you were assessing the main effect of this factor on mood. The null hypothesis would be that the two marginal means are equal:\n\\[\\bar{Y}_{1..} = \\bar{Y}_{2..}\\]\nwhere \\(Y_{i..}\\) is the mean of row \\(i\\), ignoring the column factor.\nIf you have a factor with \\(k\\) levels where \\(k &gt; 2\\), the null hypothesis for the main effect is\n\\[\\bar{Y}_{1..} = \\bar{Y}_{2..} = \\ldots = \\bar{Y}_{k..},\\]\ni.e., that all of the row (or column) means are equal.\n\n\n4.2.2.2 Simple effect\nA Simple effect is the effect of one factor at a specific level of another factor (i.e., holding that factor constant at a particular value).\nSo for instance, in Scenario C, we talked about the effect of CBT for participants in the anti-depressant group. In that case, the simple effect of CBT for participants receiving anti-depressants was 40 units.\nWe could also talk about the simple effect of drug therapy for patients who received cognitive therapy. In Scenario C, this was an increase in mood from 60 to 90 (column 2).\n\n\n4.2.2.3 Interaction\nWe say that an interaction is present when the effect of one variable differs across the levels of another variable.\nA more mathematical definition is that an interaction is present when the simple effects of one factor differ across the levels of another factor. We saw this in Scenario C, with a 40 point boost of CBT for the anti-depressant group, compared to a boost of 20 for the placebo group. Perhaps the elevated mood caused by the anti-depressants made patients more susceptable to CBT.\nThe main point here is that we say there is a simple interaction between A and B when the simple effects of A differ across the levels of B. You could also check whether the simple effects of B differ across A. It is not possible for one of these statements to be true without the other also being true, so it doesn’t matter which way you look at the simple effects.\n\n\n\n4.2.3 Higher-order designs\nTwo-factor (also known as “two-way”) designs are very common in psychology and neuroscience, but sometimes you will also see designs with more than two factors, such as a 2x2x2 design.\nTo figure out the number of effects we have of different kinds, we use the formula below, which gives us the number of possible combinations for \\(n\\) elements take \\(k\\) at a time:\n\\[\\frac{n!}{k!(n - k)!}\\]\nRather than actually computing this by hand, we can just use the choose(n, k) function in R.\nFor any design with \\(n\\) factors, you will have:\n\n\\(n\\) main effects;\n\\(\\frac{n!}{2!(n - 2)!}\\) two-way interactions;\n\\(\\frac{n!}{3!(n - 3)!}\\) three-way interactions;\n\\(\\frac{n!}{4!(n - 4)!}\\) four-way interactions… and so forth.\n\nSo if we have a three-way design, e.g., a 2x2x2 with factors \\(A\\), \\(B\\), and \\(C\\), we would have 3 main effects: \\(A\\), \\(B\\), and \\(C\\). We would have choose(3, 2) = three two way interactions: \\(AB\\), \\(AC\\), and \\(BC\\), and choose(3, 3) = one three-way interaction: \\(ABC\\).\nThree-way interactions are hard to interpret, but what they imply is that the simple interaction between any two given factors varies across the level of the third factor. For example, it would imply that the \\(AB\\) interaction at \\(C_1\\) would be different from the \\(AB\\) interaction at \\(C_2\\).\nIf you have a four way design, you have four main effects, choose(4, 2) =6 two-way interactions, choose(4, 3) =4 three-way interactions, and one four-way interaction. It is next to impossible to interpret results from a four-way design, so keep your designs simple!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "interactions.html#the-glm-for-a-factorial-design",
    "href": "interactions.html#the-glm-for-a-factorial-design",
    "title": "4  Interactions",
    "section": "4.3 The GLM for a factorial design",
    "text": "4.3 The GLM for a factorial design\nNow let’s look at the math behind these models. The typical way you’ll see the GLM for an ANOVA written for a 2x2 factorial design uses “ANOVA” notation, like so:\n\\[Y_{ijk} = \\mu + A_i + B_j + AB_{ij} + S(AB)_{ijk}.\\]\nIn the above formula,\n\n\\(Y_{ijk}\\) is the score for observation \\(k\\) at level \\(i\\) of \\(A\\) and level \\(j\\) of \\(B\\);\n\\(\\mu\\) is the grand mean;\n\\(A_i\\) is the main effect of factor \\(A\\) at level \\(i\\) of \\(A\\);\n\\(B_j\\) is the main effect of factor \\(B\\) at level \\(j\\) of \\(B\\);\n\\(AB_{ij}\\) is the \\(AB\\) interaction at level \\(i\\) of \\(A\\) and level \\(j\\) of \\(B\\);\n\\(S(AB)_{ijk}\\) is the residual.\n\nAn important mathematical fact is that the individual main and interaction effects sum to zero, often written as:\n\n\\(\\Sigma_i A_i = 0\\);\n\\(\\Sigma_j B_j = 0\\);\n\\(\\Sigma_{ij} AB_{ij} = 0\\).\n\nThe best way to understand these effects is to see them in a decomposition table. Study the decomposition table belo wfor 12 simulated observations from a 2x2 design with factors \\(A\\) and \\(B\\). The indexes \\(i\\), \\(j\\), and \\(k\\) are provided just to help you keep track of what observation you are dealing with. Remember that \\(i\\) indexes the level of factor \\(A\\), \\(j\\) indexes the level of factor \\(B\\), and \\(k\\) indexes the observation number within the cell \\(AB_{ij}\\).\n\n\n# A tibble: 12 × 9\n       Y     i     j     k    mu   A_i   B_j AB_ij   err\n   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n 1    11     1     1     1    10     4    -2    -1     0\n 2    14     1     1     2    10     4    -2    -1     3\n 3     8     1     1     3    10     4    -2    -1    -3\n 4    17     1     2     1    10     4     2     1     0\n 5    15     1     2     2    10     4     2     1    -2\n 6    19     1     2     3    10     4     2     1     2\n 7     8     2     1     1    10    -4    -2     1     3\n 8     4     2     1     2    10    -4    -2     1    -1\n 9     3     2     1     3    10    -4    -2     1    -2\n10    10     2     2     1    10    -4     2    -1     3\n11     7     2     2     2    10    -4     2    -1     0\n12     4     2     2     3    10    -4     2    -1    -3\n\n\n\n4.3.1 Estimation equations\nThese are the equations you would used to estimate effects in an ANOVA.\n\n\\(\\hat{\\mu} = Y_{...}\\)\n\\(\\hat{A}_i = Y_{i..} - \\hat{\\mu}\\)\n\\(\\hat{B}_j = Y_{.j.} - \\hat{\\mu}\\)\n\\(\\widehat{AB}_{ij} = Y_{ij.} - \\hat{\\mu} - \\hat{A}_i - \\hat{B}_j\\)\n\nNote that the \\(Y\\) variable with the dots in the subscripts are means of \\(Y\\), taken while ignoring anything appearing as a dot. So \\(Y_{...}\\) is mean of \\(Y\\), \\(Y_{i..}\\) is the mean of \\(Y\\) at level \\(i\\) of \\(A\\), \\(Y_{.j.}\\) is the mean of \\(Y\\) at level \\(j\\) of \\(B\\), and \\(Y_{ij.}\\) is the mean of \\(Y\\) at level \\(i\\) of \\(A\\) and level \\(j\\) of \\(B\\), i.e., the cell mean \\(ij\\).\n\n\n4.3.2 Factorial App\n\nLaunch this web application and experiment with factorial designs until you understand the key concepts of main effects and interactions in a factorial design.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "interactions.html#code-your-own-categorical-predictors-in-factorial-designs",
    "href": "interactions.html#code-your-own-categorical-predictors-in-factorial-designs",
    "title": "4  Interactions",
    "section": "4.4 Code your own categorical predictors in factorial designs",
    "text": "4.4 Code your own categorical predictors in factorial designs\nMany studies in psychology—especially experimental psychology—involve categorical independent variables. Analyzing data from these studies requires care in specifying the predictors, because the defaults in R are not ideal for experimental situations. The main problem is that the default coding of categorical predictors gives you simple effects rather than main effects in the output, when what you usually want are the latter. People are sometimes unaware of this and misinterpret their output. It also happens that researchers report results from a regression with categorical predictors but do not explicitly report how they coded them, making their findings potentially difficult to interpret and irreproducible. In the interest of reproducibility, transparency, and accurate interpretation, it is a good idea to learn how to code categorical predictors “by hand” and to get into the habit of reporting them in your reports.\nBecause the R defaults aren’t good for factorial designs, I’m going to suggest that you should always code your own categorical variables when including them as predictors in linear models. Don’t include them as factor variables.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "interactions.html#coding-schemes-for-categorical-variables",
    "href": "interactions.html#coding-schemes-for-categorical-variables",
    "title": "4  Interactions",
    "section": "4.5 Coding schemes for categorical variables",
    "text": "4.5 Coding schemes for categorical variables\nMany experimentalists who are trying to make the leap from ANOVA to linear mixed-effects models (LMEMs) in R struggle with the coding of categorical predictors. It is unexpectedly complicated, and the defaults provided in R turn out to be wholly inappropriate for factorial experiments. Indeed, using those defaults with factorial experiments can lead researchers to draw erroneous conclusions from their data.\nTo keep things simple, we’ll start with situations where design factors have no more than two levels before moving on to designs with more than three levels.\n\n4.5.1 Simple versus main effects\nIt is important that you understand the difference between a simple effect and a main effect, and between a simple interaction and a main interaction in a three-way design.\nIn an \\(A{\\times}B\\) design, the simple effect of \\(A\\) is the effect of \\(A\\) controlling for \\(B\\), while the main effect of \\(A\\) is the effect of \\(A\\) ignoring \\(B\\). Another way of looking at this is to consider the cell means (\\(\\bar{Y}_{11}\\), \\(\\bar{Y}_{12}\\), \\(\\bar{Y}_{21}\\), and \\(\\bar{Y}_{22}\\)) and marginal means (\\(\\bar{Y}_{1.}\\), \\(\\bar{Y}_{2.}\\), \\(\\bar{Y}_{.1}\\), and \\(\\bar{Y}_{.2}\\)) in a factorial design. (The dot subscript tells you to “ignore” the dimension containing the dot; e.g., \\(\\bar{Y}_{.1}\\) tells you to take the mean of the first column ignoring the row variable.) To test the main effect of A is to test the null hypothesis that \\(\\bar{Y}_{1.}=\\bar{Y}_{2.}\\). To test a simple effect of \\(A\\)—the effect of \\(A\\) at a particular level of \\(B\\)—would be, for instance, to test the null hypothesis that \\(\\bar{Y}_{11}=\\bar{Y}_{21}\\).\n\n\n\n\nTable 4.8\n\n\n\n\n \n  \n     \n    \\(B_1\\) \n    \\(B_2\\) \n     \n     \n  \n \n\n  \n    \\(A_1\\) \n    \\(\\bar{Y}_{11}\\) \n    \\(\\bar{Y}_{12}\\) \n     \n    \\(\\bar{Y}_{1.}\\) \n  \n  \n    \\(A_2\\) \n    \\(\\bar{Y}_{21}\\) \n    \\(\\bar{Y}_{22}\\) \n     \n    \\(\\bar{Y}_{2.}\\) \n  \n  \n     \n     \n     \n     \n     \n  \n  \n     \n    \\(\\bar{Y}_{.1}\\) \n    \\(\\bar{Y}_{.2}\\) \n     \n     \n  \n\n\n\n\n\n\n\n\nThe distinction between simple interactions and main interactions has the same logic: the simple interaction of (AB) in an (ABC) design is the interaction of (AB) at a particular level of (C); the main interaction of (AB) is the interaction ignoring C. The latter is what we are usually talking about when we talk about lower-order interactions in a three-way design. It is also what we are given in the output from standard ANOVA procedures, e.g., the aov() function in R, SPSS, SAS, etc.\n\n\n4.5.2 The key coding schemes\nGenerally, the choice of a coding scheme impacts the interpretation of:\n\nthe intercept term; and\nthe interpretation of the tests for all but the highest-order effects and interactions in a factorial design.\n\nIt also can influence the interpretation/estimation of random effects in a mixed-effects model (see this blog post for further discussion). If you have a design with only a single two-level factor, and are using a maximal random-effects structure, the choice of coding scheme doesn’t really matter.\nThere are many possible coding schemes (see ?contr.treatment for more information). The most relevant ones are treatment, sum, and deviation. Sum and deviation coding can be seen as special cases of effect coding; by effect coding, people generally mean codes that sum to zero.\nFor a two-level factor, you would use the following codes:\n\n\n\n\nTable 4.9: codes for a two-level factor\n\n\n\n\n \n  \n    Scheme \n    \\(A_1\\) \n    \\(A_2\\) \n  \n \n\n  \n    Treatment (dummy) \n    \\(0\\) \n    \\(1\\) \n  \n  \n    Sum \n    \\(-1\\) \n    \\(1\\) \n  \n  \n    Deviation \n    \\(-\\frac{1}{2}\\) \n    \\(\\frac{1}{2}\\) \n  \n\n\n\n\n\n\n\n\nThe default in R is to use treatment coding for any variable defined as a =factor= in the model (see ?factor and ?contrasts for information). To see why this is not ideal for factorial designs, consider a 2x2x2 factorial design with factors \\(A\\), \\(B\\) and \\(C\\). We will just consider a fully between-subjects design with only one observation per subject as this allows us to use the simplest possible error structure. We would fit such a model using lm():\n\nlm(Y ~ A * B * C)\n\nThe figure below spells out the notation for the various cell and marginal means for a 2x2x2 design.\n\n\n\n\n\n\nTable 4.10: \\(C_1\\)\n\n\n\n\n \n  \n     \n    \\(B_1\\) \n    \\(B_2\\) \n     \n     \n  \n \n\n  \n    \\(A_1\\) \n    \\(\\bar{Y}_{111}\\) \n    \\(\\bar{Y}_{121}\\) \n     \n    \\(\\bar{Y}_{1.1}\\) \n  \n  \n    \\(A_2\\) \n    \\(\\bar{Y}_{211}\\) \n    \\(\\bar{Y}_{221}\\) \n     \n    \\(\\bar{Y}_{2.1}\\) \n  \n  \n     \n     \n     \n     \n     \n  \n  \n     \n    \\(\\bar{Y}_{.11}\\) \n    \\(\\bar{Y}_{.21}\\) \n     \n     \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4.11: \\(C_2\\)\n\n\n\n\n \n  \n     \n    \\(B_1\\) \n    \\(B_2\\) \n     \n     \n  \n \n\n  \n    \\(A_1\\) \n    \\(\\bar{Y}_{112}\\) \n    \\(\\bar{Y}_{122}\\) \n     \n    \\(\\bar{Y}_{1.2}\\) \n  \n  \n    \\(A_2\\) \n    \\(\\bar{Y}_{212}\\) \n    \\(\\bar{Y}_{222}\\) \n     \n    \\(\\bar{Y}_{2.2}\\) \n  \n  \n     \n     \n     \n     \n     \n  \n  \n     \n    \\(\\bar{Y}_{.12}\\) \n    \\(\\bar{Y}_{.22}\\) \n     \n     \n  \n\n\n\n\n\n\n\n\n\n\nThe table below provides the interpretation for various effects in the model under the three different coding schemes. Note that \\(Y\\) is the dependent variable, and the dots in the subscript mean to “ignore” the corresponding dimension. Thus, ({Y}{.1.}) is the mean of B_1 (ignoring factors (A) and (C)) and ({Y}{…}) is the “grand mean” (ignoring all factors).\n\n\n\n\n\n\nterm\ntreatment\nsum\ndeviation\n\n\n\n\n\\(\\mu\\)\n\\(\\bar{Y}_{111}\\)\n\\(\\bar{Y}_{...}\\)\n\\(\\bar{Y}_{...}\\)\n\n\n\\(A\\)\n\\(\\bar{Y}_{211} - \\bar{Y}_{111}\\)\n\\(\\frac{(\\bar{Y}_{2..} - \\bar{Y}_{1..})}{2}\\)\n\\(\\bar{Y}_{2..} - \\bar{Y}_{1..}\\)\n\n\n\\(B\\)\n\\(\\bar{Y}_{121} - \\bar{Y}_{111}\\)\n\\(\\frac{(\\bar{Y}_{.2.} - \\bar{Y}_{.1.})}{2}\\)\n\\(\\bar{Y}_{.2.} - \\bar{Y}_{.1.}\\)\n\n\n\\(C\\)\n\\(\\bar{Y}_{112} - \\bar{Y}_{111}\\)\n\\(\\frac{(\\bar{Y}_{..2} - \\bar{Y}_{..1})}{2}\\)\n\\(\\bar{Y}_{..2} - \\bar{Y}_{..1}\\)\n\n\n\\(AB\\)\n\\((\\bar{Y}_{221} - \\bar{Y}_{121}) - (\\bar{Y}_{211} - \\bar{Y}_{111})\\)\n\\(\\frac{(\\bar{Y}_{22.} - \\bar{Y}_{12.}) - (\\bar{Y}_{21.} - \\bar{Y}_{11.})}{4}\\)\n\\((\\bar{Y}_{22.} - \\bar{Y}_{12.}) - (\\bar{Y}_{21.} - \\bar{Y}_{11.})\\)\n\n\n\\(AC\\)\n\\((\\bar{Y}_{212} - \\bar{Y}_{211}) - (\\bar{Y}_{112} - \\bar{Y}_{111})\\)\n\\(\\frac{(\\bar{Y}_{2.2} - \\bar{Y}_{1.2}) - (\\bar{Y}_{2.1} - \\bar{Y}_{1.1})}{4}\\)\n\\((\\bar{Y}_{2.2} - \\bar{Y}_{1.2}) - (\\bar{Y}_{2.1} - \\bar{Y}_{1.1})\\)\n\n\n\\(BC\\)\n\\((\\bar{Y}_{122} - \\bar{Y}_{112}) - (\\bar{Y}_{121} - \\bar{Y}_{111})\\)\n\\(\\frac{(\\bar{Y}_{.22} - \\bar{Y}_{.12}) - (\\bar{Y}_{.21} - \\bar{Y}_{.11})}{4}\\)\n\\((\\bar{Y}_{.22} - \\bar{Y}_{.12}) - (\\bar{Y}_{.21} - \\bar{Y}_{.11})\\)\n\n\n\n\n\n\n\n\nFor the three way (A B C) interaction:\n\n\n\n\n\n\nscheme\ninterpretation\n\n\n\n\ntreatment\n\\(\\displaystyle\\left[\\displaystyle\\left(\\bar{Y}_{221} - \\bar{Y}_{121}\\right) - \\displaystyle\\left(\\bar{Y}_{211} - \\bar{Y}_{111}\\right)\\right] - \\displaystyle\\left[\\displaystyle\\left(\\bar{Y}_{222} - \\bar{Y}_{122}\\right) - \\displaystyle\\left(\\bar{Y}_{212} - \\bar{Y}_{112}\\right)\\right]\\)\n\n\nsum\n\\(\\frac{\\displaystyle\\left[\\displaystyle\\left(\\bar{Y}_{221} - \\bar{Y}_{121}\\right) - \\displaystyle\\left(\\bar{Y}_{211} - \\bar{Y}_{111}\\right)\\right] - \\displaystyle\\left[\\displaystyle\\left(\\bar{Y}_{222} - \\bar{Y}_{122}\\right) - \\displaystyle\\left(\\bar{Y}_{212} - \\bar{Y}_{112}\\right)\\right]}{8}\\)\n\n\ndeviation\n\\(\\displaystyle\\left[\\displaystyle\\left(\\bar{Y}_{221} - \\bar{Y}_{121}\\right) - \\displaystyle\\left(\\bar{Y}_{211} - \\bar{Y}_{111}\\right)\\right] - \\displaystyle\\left[\\displaystyle\\left(\\bar{Y}_{222} - \\bar{Y}_{122}\\right) - \\displaystyle\\left(\\bar{Y}_{212} - \\bar{Y}_{112}\\right)\\right]\\)\n\n\n\n\n\n\n\n\nNote that the inferential tests of \\(A \\times B \\times C\\) will all have the same outcome, despite the parameter estimate for sum coding being one-eighth of that for the other schemes. For all lower-order effects, sum and deviation coding will give different parameter estimates but identical inferential outcomes. Both of these schemes provide identical tests of the canonical main effects and main interactions for a three-way ANOVA. In contrast, treatment (dummy) coding will provide inferential tests of simple effects and simple interactions. So, if what you are interested in getting are the “canonical” tests from ANOVA, use sum or deviation coding.\n\n\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1, 259–269. https://journals.sagepub.com/doi/abs/10.1177/2515245918770963",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "mixed-effects.html",
    "href": "mixed-effects.html",
    "title": "5  Mixed-effects models",
    "section": "",
    "text": "5.1 The sleepstudy dataset\nIn this chapter, we’ll be working with some real data from a study looking at the effects of sleep deprivation on psychomotor performance (Belenky et al., 2003). Data from this study is included as the built-in dataset sleepstudy in the lme4 package for R (Bates et al., 2015).\nLet’s start by looking at the documentation for the sleepstudy dataset. After loading the lme4 package, you can access the documentation by typing ?sleepstudy in the console.\nThese data meet our definition of multilevel data due to repeated measurements on the same dependent variable (mean RT) for the same participants over ten days. Multilevel data of this type is extremely common in psychology. Unfortunately, most statistics textbooks commonly used in psychology courses don’t sufficiently discuss multilevel data, beyond paired t-tests and repeated-measures ANOVA. The sleepstudy dataset is interesting because it is multilevel but has a continuous predictor, and thus does not fit will with t-test or ANOVA, because both of these approaches are for categorical predictors. There are ways you could make the data fit into one of these frameworks, but not without losing information or possibly violating assumptions.\nIt is a shame that psychology students don’t really learn much about analyzing multilevel data. Think about studies you have read recently in psychology or neuroscience. How many of them take a single measurement on the DV from each participant? Very few, if any. Nearly all take multiple measurements, for one or more of the following reasons: (1) the researchers are measuring the same participants across levels of a factor in a within-subject design; (2) they are interested in assessing change over time; or (3) they are measuring responses to multiple stimuli. Multilevel data is so common that multilevel analysis should be taught as the default approach in psychology. Learning about multilevel analysis can be challenging, but you already know much of what you need by having learned about correlation and regression. You will see that it is just an extension of simple regression.\nLet’s take a closer look at the sleepstudy data. The dataset contains eighteen participants from the three-hour sleep condition. Each day, over 10 days, participants performed a ten-minute “psychomotor vigilance test” where they had to monitor a display and press a button as quickly as possible each time a stimulus appeared. The dependent measure in the dataset is the participant’s average response time (RT) on the task for that day.\nA good way to start every analysis is to plot the data. Figure 5.1 shows data for a single subject.\nlibrary(\"lme4\")\nlibrary(\"tidyverse\")\n\njust_308 &lt;- sleepstudy |&gt;\n  filter(Subject == \"308\")\n\nggplot(just_308, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9)\n\n\n\n\n\n\n\nFigure 5.1: Data from a single subject in Belenky et al. (2003)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effects models</span>"
    ]
  },
  {
    "objectID": "mixed-effects.html#the-sleepstudy-dataset",
    "href": "mixed-effects.html#the-sleepstudy-dataset",
    "title": "5  Mixed-effects models",
    "section": "",
    "text": "sleepstudy                package:lme4                 R Documentation\n\nReaction times in a sleep deprivation study\n\nDescription:\n\n     The average reaction time per day (in milliseconds) for subjects\n     in a sleep deprivation study.\n\n     Days 0-1 were adaptation and training (T1/T2), day 2 was baseline\n     (B); sleep deprivation started after day 2.\n\nFormat:\n\n     A data frame with 180 observations on the following 3 variables.\n\n     ‘Reaction’ Average reaction time (ms)\n\n     ‘Days’ Number of days of sleep deprivation\n\n     ‘Subject’ Subject number on which the observation was made.\n\nDetails:\n\n     These data are from the study described in Belenky et al.  (2003),\n     for the most sleep-deprived group (3 hours time-in-bed) and for\n     the first 10 days of the study, up to the recovery period.  The\n     original study analyzed speed (1/(reaction time)) and treated day\n     as a categorical rather than a continuous predictor.\n\nReferences:\n\n     Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L.\n     Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and\n     Thomas J. Balkin (2003) Patterns of performance degradation and\n     restoration during sleep restriction and subsequent recovery: a\n     sleep dose-response study. _Journal of Sleep Research_ *12*, 1-12.\n\n\n\n\n\n\n\n\n\n\n\nNote 5.1: Exercise 4.1\n\n\n\n\n\nUse ggplot to recreate Figure 5.2 below, which shows data for all 18 subjects.\n\n\n\n\n\n\n\n\nFigure 5.2: Data from Belenky et al. (2003)\n\n\n\n\n\nIt looks like RT is increasing with each additional day of sleep deprivation, starting from day 2 and increasing until day 10.\n\n\n\n\n\n\nHint\n\n\n\n\n\nJust above, you were given the code to make a plot for a single participant. Adapt this code to show all of the participants by getting rid of the filter() statement and adding a ggplot2 function that starts with facet_.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSame as above, except you just add one line: facet_wrap(~Subject)\n\nggplot(sleepstudy, aes(x = Days, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:9) +\n  facet_wrap(~Subject)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effects models</span>"
    ]
  },
  {
    "objectID": "mixed-effects.html#how-to-model-the-relationship",
    "href": "mixed-effects.html#how-to-model-the-relationship",
    "title": "5  Mixed-effects models",
    "section": "5.2 How to model the relationship?",
    "text": "5.2 How to model the relationship?\nWe wish to understand the effect of sleep deprivation on respnose time, so we will try to model this relationship. To model the data appropriately, first we need to know more about the study design. This is how Belenky et al. (2003) describe it (p. 2):\n\nThe first 3 days (T1, T2 and B) were adaptation and training (T1 and T2) and baseline (B) and subjects were required to be in bed from 23:00 to 07:00 h [8 h required time in bed (TIB)]. On the third day (B), baseline measures were taken. Beginning on the fourth day and continuing for a total of 7 days (E1–E7) subjects were in one of four sleep conditions [9 h required TIB (22:00–07:00 h), 7 h required TIB (24:00–07:00 h), 5 h required TIB (02:00–07:00 h), or 3 h required TIB (04:00–07:00 h)], effectively one sleep augmentation condition, and three sleep restriction conditions.\n\nThere were seven nights of sleep restriction, with the first night of restriction occurring after the third day. The first two days, coded as 0, 1, were adaptation and training. The day coded as 2, where the baseline measurement was taken, should be the place where we start our analysis. If we include the days 0 and 1 in our analysis, this might bias our results, since any changes in performance during the first two days have to do with training, not sleep restriction. Exercise 4.2 shows how to do this. The resulting data is stored in the data frame sleep2, which we will use in the analysis.\n\n\n\n\n\n\nExercise 4.2\n\n\n\n\n\nRemove from the dataset observations where Days is coded 0 or 1, and then make a new variable days_deprived from the Days variable so that the sequence starts at day 2, with day 2 being re-coded as day 0, day 3 as day 1, day 4 as day 2, etc. This new variable now tracks the number of days of sleep deprivation. Store the new table as sleep2.\n\n\n\n\n\n\nHint\n\n\n\n\n\nYou will need to get rid of observations where Days is less than 2, and then create a new variable, days_deprived, which is a transformation of the variable Days.\nYou can do this by using two of the following six one-table dplyr verbs on the data: select(), mutate(), group_by(), filter(), arrange(), summarise().\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsleep2 &lt;- sleepstudy |&gt;\n  filter(Days &gt;= 2L) |&gt;\n  mutate(days_deprived = Days - 2L)\n\nIt is always a good idea to double check that the code works as intended. First, look at it:\n\nhead(sleep2)\n\n  Reaction Days Subject days_deprived\n1 250.8006    2     308             0\n2 321.4398    3     308             1\n3 356.8519    4     308             2\n4 414.6901    5     308             3\n5 382.2038    6     308             4\n6 290.1486    7     308             5\n\n\nAnd check that Days and days_deprived match up.\n\nsleep2 |&gt;\n  count(days_deprived, Days)\n\n  days_deprived Days  n\n1             0    2 18\n2             1    3 18\n3             2    4 18\n4             3    5 18\n5             4    6 18\n6             5    7 18\n7             6    8 18\n8             7    9 18\n\n\nLooks good. Note that the variable n in generated by count() and tells you how many rows there are for each unique combination of Days and days_deprived. In this case, there were 18, one row for each participant.\n\n\n\n\n\n\nNow let’s re-plot the data looking at just these eight data points from Day 0 to Day 7. We’ve just copied the code from above, substituting sleep2 for sleepstudy and using days_deprived for our x variable.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\nFigure 5.3: Data from Belenky et al. (2003), showing reaction time at baseline (0) and after each day of sleep deprivation.\n\n\n\n\n\nTake a moment to think about how me might model the relationship between days_deprived and Reaction. Does reaction time increase or decrease with increasing sleep deprivation? Is the relationship roughly stable or does it change with time?\nWith only one exception (subject 335) it looks like reaction time increases with each additional day of sleep deprivation. It looks like we could fit a line to each participant’s data. Recall the general equation for a line is of the form y = y-intercept + slope \\(\\times\\) x. In regression, the we usually express a linear relationship with the formula\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\]\nwhere \\(\\beta_0\\) is the y-intercept and \\(\\beta_1\\) is the slope. We estimate these two :regression coefficients from the data.\nThe lines will all differ in intercept (mean RT at day zero, before the sleep deprivation began) and slope (the change in RT with each additional day of sleep deprivation). But should we fit the same line to everyone? Or a totally different line for each subject? Or something somewhere in between?\nLet’s start by considering three different approaches we might take. Following McElreath, we will distinguish these approaches by calling them complete pooling, no pooling, and partial pooling.\n\n5.2.1 Complete pooling: One size fits all\nThe complete pooling approach is a “one-size-fits-all” model: it estimates a single intercept and slope for the entire dataset, ignoring the fact that different subjects might vary in their intercepts or slopes. If that sounds like a bad approach, it is; but you know this because you’ve already visualized the data and noted that the pattern for each participant would seem to require different y-intercept and slope values.\nFitting one line is called the “complete pooling” approach because we pool together data from all subjects to get single estimates for an overall intercept and slope. The GLM for this approach is simply\n\\[Y_{sd} = \\beta_0 + \\beta_1 X_{sd} + e_{sd}\\]\n\\[e_{sd} \\sim N\\left(0, \\sigma^2\\right)\\]\nwhere \\(Y_{sd}\\) is the mean RT for subject \\(s\\) on day \\(d\\), \\(X_{sd}\\) is the value of days_deprived associated with that case (0-7), and \\(e_{sd}\\) is the error.\nWe would fit such a model in R using the lm() function, e.g.:\n\ncp_model &lt;- lm(Reaction ~ days_deprived, sleep2)\n\nsummary(cp_model)\n\n\nCall:\nlm(formula = Reaction ~ days_deprived, data = sleep2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.284  -26.732    2.143   27.734  140.453 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    267.967      7.737  34.633  &lt; 2e-16 ***\ndays_deprived   11.435      1.850   6.183 6.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 50.85 on 142 degrees of freedom\nMultiple R-squared:  0.2121,    Adjusted R-squared:  0.2066 \nF-statistic: 38.23 on 1 and 142 DF,  p-value: 6.316e-09\n\n\nAccording to this model, the predicted mean response time on Day 0 is about 268 milliseconds, with an increase of about 11 milliseconds per day of deprivation, on average. We can’t trust the standard errors for our regression coefficients, however, because we are assuming that all of the observations are independent (technically, that the residuals are). However, we can be pretty sure this is a bad assumption.\nLet’s add the model predictions to the graph that we created above. We can use geom_abline() to do so, specifying the intercept and slope for the line using the regression coefficients from the model fit, coef(cp_model), which returns a two-element vector with intercept and slope, respectively.\n\ncoef(cp_model)\n\n  (Intercept) days_deprived \n    267.96742      11.43543 \n\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_abline(intercept = coef(cp_model)[1],\n              slope = coef(cp_model)[2],\n              color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\nFigure 5.4: Data plotted against predictions from the complete pooling model.\n\n\n\n\n\nThe model fits the data badly. We need a different approach.\n\n\n5.2.2 No pooling\nPooling all the information to get just one intercept and one slope estimate is inappropriate. Another approach would be to fit separate lines for each participant. This means that the estimates for each participant will be completely uninformed by the estimates for the other participants. In other words, we separately estimate 18 individual intercept/slope pairs.\nThis model could be implemented in two ways: (1) by running separate regressions for each participant or (2) by running fixed-effects regression. We’ll do the latter, so that everything is in one big model. We know how to do this already: we add in dummy codes for the Subject factor. We have 18 levels of this factor, so we’d need 17 dummy codes. Fortunately, R saves us from the trouble of creating the 17 variables we would need by hand. All we need to do is include Subject as a predictor in the model, and interact this categorical predictor with days_deprived to allow intercepts and slopes to vary.\n\n\n\n\n\n\nCaution\n\n\n\nThe variable Subject in the sleep2 dataset is nominal. We just use numbers as labels to preserve anonymity, without intending to imply that Subject 310 is one point better than Subject 309 and two points better than 308. Make sure that you define it as a factor so that it is not included as a continuous variable!\nWe can test whether something is a factor in various ways. One is to use summary() on the table.\n\nsleep2 |&gt; summary()\n\n    Reaction          Days         Subject   days_deprived \n Min.   :203.0   Min.   :2.00   308    : 8   Min.   :0.00  \n 1st Qu.:265.2   1st Qu.:3.75   309    : 8   1st Qu.:1.75  \n Median :303.2   Median :5.50   310    : 8   Median :3.50  \n Mean   :308.0   Mean   :5.50   330    : 8   Mean   :3.50  \n 3rd Qu.:347.7   3rd Qu.:7.25   331    : 8   3rd Qu.:5.25  \n Max.   :466.4   Max.   :9.00   332    : 8   Max.   :7.00  \n                                (Other):96                 \n\n\nHere you can see that it is not treated as a number because rather than giving you distributional information (means, etc.) it tells you how many observations there are at each level.\nYou can also test it directly:\n\nsleep2 |&gt; pull(Subject) |&gt; is.factor()\n\n[1] TRUE\n\n\nIf something is not a factor, you can make it one be re-defining it using the factor() function.\n\n\n\nnp_model &lt;- lm(Reaction ~ days_deprived + Subject +\n                 days_deprived:Subject,\n               data = sleep2)\n\nsummary(np_model)\n\n\nCall:\nlm(formula = Reaction ~ days_deprived + Subject + days_deprived:Subject, \n    data = sleep2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-106.521   -8.541    1.143    8.889  128.545 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              288.2175    16.4772  17.492  &lt; 2e-16 ***\ndays_deprived             21.6905     3.9388   5.507 2.49e-07 ***\nSubject309               -87.9262    23.3023  -3.773 0.000264 ***\nSubject310               -62.2856    23.3023  -2.673 0.008685 ** \nSubject330               -14.9533    23.3023  -0.642 0.522422    \nSubject331                 9.9658    23.3023   0.428 0.669740    \nSubject332                27.8157    23.3023   1.194 0.235215    \nSubject333                -2.7581    23.3023  -0.118 0.906000    \nSubject334               -50.2051    23.3023  -2.155 0.033422 *  \nSubject335               -25.3429    23.3023  -1.088 0.279207    \nSubject337                24.6143    23.3023   1.056 0.293187    \nSubject349               -59.2183    23.3023  -2.541 0.012464 *  \nSubject350               -40.2023    23.3023  -1.725 0.087343 .  \nSubject351               -24.2467    23.3023  -1.041 0.300419    \nSubject352                43.0655    23.3023   1.848 0.067321 .  \nSubject369               -21.5040    23.3023  -0.923 0.358154    \nSubject370               -53.3072    23.3023  -2.288 0.024107 *  \nSubject371               -30.4896    23.3023  -1.308 0.193504    \nSubject372                 2.4772    23.3023   0.106 0.915535    \ndays_deprived:Subject309 -17.3334     5.5703  -3.112 0.002380 ** \ndays_deprived:Subject310 -17.7915     5.5703  -3.194 0.001839 ** \ndays_deprived:Subject330 -13.6849     5.5703  -2.457 0.015613 *  \ndays_deprived:Subject331 -16.8231     5.5703  -3.020 0.003154 ** \ndays_deprived:Subject332 -19.2947     5.5703  -3.464 0.000765 ***\ndays_deprived:Subject333 -10.8151     5.5703  -1.942 0.054796 .  \ndays_deprived:Subject334  -3.5745     5.5703  -0.642 0.522423    \ndays_deprived:Subject335 -25.8995     5.5703  -4.650 9.47e-06 ***\ndays_deprived:Subject337   0.7518     5.5703   0.135 0.892895    \ndays_deprived:Subject349  -5.2644     5.5703  -0.945 0.346731    \ndays_deprived:Subject350   1.6007     5.5703   0.287 0.774382    \ndays_deprived:Subject351 -13.1681     5.5703  -2.364 0.019867 *  \ndays_deprived:Subject352 -14.4019     5.5703  -2.585 0.011057 *  \ndays_deprived:Subject369  -7.8948     5.5703  -1.417 0.159273    \ndays_deprived:Subject370  -1.0495     5.5703  -0.188 0.850912    \ndays_deprived:Subject371  -9.3443     5.5703  -1.678 0.096334 .  \ndays_deprived:Subject372 -10.6041     5.5703  -1.904 0.059613 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.53 on 108 degrees of freedom\nMultiple R-squared:  0.849, Adjusted R-squared:  0.8001 \nF-statistic: 17.35 on 35 and 108 DF,  p-value: &lt; 2.2e-16\n\n\nWhat this model has done is take one subject to be the baseline (specifically, subject 308), and represent each subject in terms of offsets from that baseline. You saw this already when we talked about continuous-by-categorical interactions.\n\n\n\n\n\n\nExercise 4.3\n\n\n\n\n\nAnswer the questions below about values estimated by the model.\n\nWhat is the intercept for subject 308?\nWhat is the slope for subject 308?\nWhat is the intercept for subject 335?\nWhat is the slope for subject 335?\n\n\n\n\n\n\n\n\nAnswers and explanation\n\n\n\n\n\nThe baseline subject is 308; the default in R is to sort the levels of the factor alphabetically and chooses the first one as the baseline. This means that the intercept and slope for 308 are given by (Intercept) and days_deprived respectively, because all of the other 17 dummy variables will be zero for subject 308.\nAll of the regression coefficients for the other subjects are represented as offsets from this baseline subject. If we want to calculate the intercept and slope for a given subject, we just add in the corresponding offsets. So, the answers are\n\nintercept for 308: 288.217\nslope for 308: 21.69\nintercept for 335: (Intercept) + Subject335 = 288.217 + -25.343 = 262.874\nslope for 335: days_deprived + days_deprived:Subject335 = 21.69 + -25.899 = -4.209\n\n\n\n\n\n\n\nIn the “no pooling” model, there is no overall population intercept and slope that is being estimated; in this case, (Intercept) and days_deprived are estimates of the intercept and slope for subject 308, which was (arbitrarily) chosen as the baseline subject. To get population estimates, we could introduce a second stage of analysis where we calculate means of the individual intercepts and slopes. Let’s use the model estimates to calculate the intercepts and slopes for each subject.\n\nall_intercepts &lt;- c(coef(np_model)[\"(Intercept)\"],\n                    coef(np_model)[3:19] + coef(np_model)[\"(Intercept)\"])\n\nall_slopes  &lt;- c(coef(np_model)[\"days_deprived\"],\n                 coef(np_model)[20:36] + coef(np_model)[\"days_deprived\"])\n\nids &lt;- sleep2 |&gt;\n  pull(Subject) |&gt;\n  levels()\n\n# make a tibble with the data extracted above\nnp_coef &lt;- tibble(Subject = factor(ids),\n                  intercept = all_intercepts,\n                  slope = all_slopes)\n\nnp_coef\n\n# A tibble: 18 × 3\n   Subject intercept slope\n   &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n 1 308          288. 21.7 \n 2 309          200.  4.36\n 3 310          226.  3.90\n 4 330          273.  8.01\n 5 331          298.  4.87\n 6 332          316.  2.40\n 7 333          285. 10.9 \n 8 334          238. 18.1 \n 9 335          263. -4.21\n10 337          313. 22.4 \n11 349          229. 16.4 \n12 350          248. 23.3 \n13 351          264.  8.52\n14 352          331.  7.29\n15 369          267. 13.8 \n16 370          235. 20.6 \n17 371          258. 12.3 \n18 372          291. 11.1 \n\n\nLet’s see how well this model fits our data (Figure 5.5).\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_abline(data = np_coef,\n              mapping = aes(intercept = intercept,\n                            slope = slope),\n              color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\nFigure 5.5: Data plotted against fits from the no-pooling approach.\n\n\n\n\n\nThis is much better than the complete pooling model. If we want to test the null hypothesis that the fixed slope is zero, we could do so using a one-sample test.\n\nnp_coef |&gt;\n  pull(slope) |&gt;\n  t.test()\n\n\n    One Sample t-test\n\ndata:  pull(np_coef, slope)\nt = 6.1971, df = 17, p-value = 9.749e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n  7.542244 15.328613\nsample estimates:\nmean of x \n 11.43543 \n\n\nThis tells us that the mean slope of 11.435 is significantly different from zero, t(17) = 6.197, \\(p &lt; .001\\).\n\n\n5.2.3 Partial pooling using mixed-effects models\nNeither the complete nor the no-pooling approach is satisfactory. It would be desirable to improve our estimates for individual participants by taking advantage of what we know about the other participants. This will help us better distinguish signal from error for each participant and improve generalization to the population. As the web app below will show, this becomes particularly important when we have unbalanced or missing data.\nIn the no-pooling model, we treated Subject as a :fixed factor. Each pair of intercept and slope estimates is determined by that subject’s data alone. However, we are not interested in these 18 subjects in and of themselves; rather, we are interested in them as examples drawn from a larger population of potential subjects. This subjects-as-fixed-effects approach is suboptimal if your goal is to generalize to new participants in the population of interest.\nPartial pooling happens when you treat a factor as a random instead of fixed in your analysis. A :random factor is a factor whose levels are considered to represent a proper subset of all the levels in the population. Usually, you treat a factor as random when the levels you have in the data are the result of sampling, and you want to generalize beyond those levels. In this case, we have eighteen unique subjects and thus, eighteen levels of the Subject factor, and would like to say something general about effects of sleep deprivation on the population of potential subjects.\n\nA way to include random factors in your analysis is to use a :linear mixed-effects model. When you do this, estimates at each level of the factor (i.e., for each subject) become informed by information at other levels (i.e., for other subjects). Rather than estimating the intercept and slope for each participant without considering the estimates for other subjects, the model estimates values for the population, and pulls the estimates for individual subjects toward those values, a statistical phenomenon known as “shrinkage”.\nThe multilevel model is below. It is important that you understand the math and what it means. It looks complicated at first, but there’s really nothing below that you haven’t seen before. We’ll explain everything step by step.\nLevel 1:\n\\[\\begin{equation}\nY_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + e_{sd}\n\\end{equation}\\]\nLevel 2:\n\\[\\begin{equation}\n\\beta_{0s} = \\gamma_{0} + S_{0s}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\beta_{1s} = \\gamma_{1} + S_{1s}\n\\end{equation}\\]\nVariance Components:\n\\[\\begin{equation}\n\\langle S_{0s}, S_{1s} \\rangle \\sim N\\left(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}\\right)\n\\end{equation}\\]\n\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\\begin{array}{cc}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n         \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\\\\n         \\end{array}\\right)\n\\end{equation}\\]\n\\[\\begin{equation}\ne_{sd} \\sim N\\left(0, \\sigma^2\\right)\n\\end{equation}\\]\nIn case you get lost, here’s a table with an explanation for all of the variables in the set of equations above.\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\n\\(Y_{sd}\\)\nobserved\nValue of Reaction for subject \\(s\\) on day \\(d\\)\n\n\n\\(X_{sd}\\)\nobserved\nValue of days_deprived (0-7) for subject \\(s\\) on day \\(d\\)\n\n\n\\(\\beta_{0s}\\)\nderived\nlevel 1 intercept parameter\n\n\n\\(\\beta_{1s}\\)\nderived\nlevel 1 slope parameter\n\n\n\\(e_{sd}\\)\nderived\nError for subject \\(s\\), day \\(d\\)\n\n\n\\(\\gamma_0\\)\nfixed\nGrand intercept (“gamma”)\n\n\n\\(\\gamma_1\\)\nfixed\nGrand slope (“gamma”)\n\n\n\\(S_{0s}\\)\nderived\nRandom intercept (offset) for subject \\(s\\)\n\n\n\\(S_{1s}\\)\nderived\nRandom slope (offset) for subject \\(s\\)\n\n\n\\(\\mathbf{\\Sigma}\\)\nrandom\nVariance-covariance matrix\n\n\n\\({\\tau_{00}}^2\\)\nrandom\nVariance of random intercepts\n\n\n\\(\\rho\\)\nrandom\nRandom correlation between intercepts and slopes\n\n\n\\({\\tau_{11}}^2\\)\nrandom\nVariance of random slopes\n\n\n\\(\\sigma^2\\)\nrandom\nError variance\n\n\n\nNote the “Status” column of the table contains values fixed, random, and derived. Although fixed and random are standard terms, derived is not; I have introduced it to help you think about what these different variables mean in the context of the model and to help you distinguish variables that are directly estimated from variables that are not.\nLet’s begin with the Level 1 equation of our model, which represents the general relationship between the predictors and response variable. It captures the functional form of the main relationship between reaction time \\(Y_{sd}\\) and sleep deprivation \\(X_{sd}\\): a straight line with intercept \\(\\beta_{0s}\\) and slope \\(\\beta_{1s}\\). Now \\(\\beta_{0s}\\) and \\(\\beta_{1s}\\) makes it look like the complete pooling model, where we estimated a single intercept and single slope for the entire dataset; however, we’re not actually estimating these directly. Instead, we’re going to think of \\(\\beta_{0s}\\) and \\(\\beta_{1s}\\) as derived: they are wholly defined by variables at Level 2 of the model.\nLevel 2 of the model, defined by two equations, represents relationships at the participant level. Here, we define the intercept \\(\\beta_{0s}\\) in terms of a fixed effect \\(\\gamma_0\\) and a random intercept \\(S_{0s}\\); likewise, we define the slope \\(\\beta_{1s}\\) in terms of a fixed slope \\(\\gamma_1\\) and a random slope \\(S_{1s}\\).\nThe final equations represent the Variance Components of the model. We’ll get into this more in detail below.\nLet’s substitute the Level 2 equations into Level 1 to see the advantages of representing things in the multilevel way.\n\\[\\begin{equation}\nY_{sd} = \\gamma_{0} + S_{0s} + \\left(\\gamma_{1} + S_{1s}\\right) X_{sd} + e_{sd}\n\\end{equation}\\]\nWhile this “combined” formula syntax is easy enough to understand in this particular case, the multilevel form more clearly allows us to see the functional form of the model: a straight line. We could easily change the functional form to, for instance, capture non-linear trends:\n\\[Y_{sd} = \\beta_{0s} + \\beta_{1s} X_{sd} + \\beta_{2s} X_{sd}^2 + e_{sd}\\]\nThis functional form gets obscured in the combined syntax. The multilevel syntax also makes it easy to see which terms go with the intercept and which terms go with the slope. Also, as designs get more compilicated—for example, if we were to assign participants to different experimental conditions, thus introducing a further predictors at Level 2—the combined equations get harder and harder to parse and reason about.\nFixed effect parameters like \\(\\gamma_0\\) and \\(\\gamma_1\\) are estimated from the data, and reflect stable properties of the population. In this example, \\(\\gamma_0\\) is the population intercept and \\(\\gamma_1\\) is the population slope. You can think of these fixed-effects parameters as representing the average intercept and slope in the population. These are “fixed” in the sense that we assume that they reflect the true underlying values in the population; they are not assumed to vary from sample to sample. These fixed effects parameters are often of prime theoretical interest; we want to measure them and their standard errors in a manner that is as unbiased and precise as the data allow. In experimental settings they are often the targets of hypothesis tests.\nRandom effects like \\(S_{0i}\\) and \\(S_{1i}\\) allow intercepts and slopes (respectively) to vary over subjects. These random effects are offsets: deviations from the population ‘grand mean’ values. Some subjects will just be slower responders than others, such that they will have a higher intercept (mean RT) on day 0 than the population’s estimated value of \\(\\hat{\\gamma_0}\\). These slower-than-average subjects will have positive \\(S_{0i}\\) values; faster-than-average subjects will have negative \\(S_{0i}\\) values. Likewise, some subjects will show stronger effects of sleep deprivation (steeper slope) than the estimated population effect, \\(\\hat{\\gamma_1}\\), which implies a positive offset \\(S_{1s}\\), while others may show weaker effects or close to none at all (negative offset).\nEach participant can be represented as a vector pair \\(\\langle S_{0i}, S_{1i} \\rangle\\). If the subjects in our sample comprised the entire population, we would be justified in treating them as fixed and estimating their values, as in the “no-pooling” approach above. This is not the case here. In recognition of the fact they are sampled, we are going to treat subjects as a random factor rather than a fixed factor. Instead of estimating the values for the subjects we happened to pick, we will estimate the covariance matrix that represents the bivariate distribution from which these pairs of values are drawn. By doing this, we allow the subjects in the sample to inform us about characteristics of the population.\n\n\n5.2.4 The variance-covariance matrix\n\\[\\begin{equation}\n\\langle S_{0s}, S_{1s} \\rangle \\sim N\\left(\\langle 0, 0 \\rangle, \\mathbf{\\Sigma}\\right)\n\\end{equation}\\]\n\\[\\begin{equation}\n\\mathbf{\\Sigma} = \\left(\\begin{array}{cc}{\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n         \\rho\\tau_{00}\\tau_{11} & {\\tau_{11}}^2 \\\\\n         \\end{array}\\right)\n\\end{equation}\\]\nEquations in the Variance Components characterize our estimates of variability. The first equation states our assumption that the random intercept / random slope pairs \\(\\langle S_{0s}, S_{1s} \\rangle\\) are drawn from a bivariate normal distribution centered at the origin \\(\\langle 0, 0 \\rangle\\) with variance-covariance matrix \\(\\mathbf{\\Sigma}\\).\nThe variance-covariance matrix is key: it determines the probability of drawing random effect pairs \\(\\langle S_{0s}, S_{1s} \\rangle\\) from the population. You have seen these before, in the chapter on correlation and regression. The covariance matrix is always a square matrix (equal numbers of columns and rows). On the main diagonal (upper left and bottom right cells) it has random effect variances \\({\\tau_{00}}^2\\) and \\({\\tau_{11}}^2\\). \\({\\tau_{00}}^2\\) is the random intercept variance, which captures how much subjects vary in their mean response time on Day 0, before any sleep deprivation. \\({\\tau_{11}}^2\\) is the random slope variance, which captures how much subjects vary in their susceptibility to the effects of sleep deprivation.\nThe cells in the off-diagonal contain covariances, but this information is represented redundantly in the matrix; the lower left element is identical to the upper right element; both capture the covariance between random intercepts and slopes, as expressed by \\(\\rho\\tau_{00}\\tau_{11}\\). In this equation \\(\\rho\\) is the correlation between the intercept and slope. So, all the information in the matrix can be captured by just three parameters: \\(\\tau_{00}\\), \\(\\tau_{11}\\), and \\(\\rho\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effects models</span>"
    ]
  },
  {
    "objectID": "mixed-effects.html#estimating-linear-mixed-effects-models",
    "href": "mixed-effects.html#estimating-linear-mixed-effects-models",
    "title": "5  Mixed-effects models",
    "section": "5.3 Estimating linear mixed-effects models",
    "text": "5.3 Estimating linear mixed-effects models\nTo estimate parameters, we are going to use the lmer() function of the lme4 package (Bates, Mächler, Bolker, & Walker, 2015). The basic syntax of lmer() is\nlmer(formula, data, ...)\nwhere formula expresses the structure of the underlying model in a compact format and data is the data frame where the variables mentioned in the formula can be found.\nThe general format of the model formula for N fixed effects (fix) and K random effects (ran) is\nDV ~ fix1 + fix2 + ... + fixN + (ran1 + ran2 + ... + ranK | random_factor1)\nInteractions between factors A and B can be specified using either A * B (interaction and main effects) or A:B (just the interaction).\nA key difference from standard R model syntax is the presence of a random effect term, which is enclosed in parentheses, e.g., (ran1 + ran2 + ... + ranK | random_factor). Each bracketed expression represents random effects associated with a single random factor. You can have more than one random effects term in a single formula, as we will see when we talk about crossed random factors. You should think of the random effects terms as providing lmer() with instructions on how to construct variance-covariance matrices.\nOn the left side of the bar | you put the effects you want to allow to vary over the levels of the random factor named on the right side. Usually, the right-side variable is one whose values uniquely identify individual subjects (e.g., subject_id).\nConsider the following possible model formulas for the sleep2 data and the variance-covariance matrices they construct.\n\n\n\n\n\n\n\n\n\nmodel\nsyntax\n\n\n\n\n1\nrandom intercepts only\nReaction ~ days_deprived + (1 | Subject)\n\n\n2\nrandom intercepts and slopes\nReaction ~ days_deprived + (1 + days_deprived | Subject)\n\n\n3\nmodel 2 alternate syntax\nReaction ~ days_deprived + (days_deprived | Subject)\n\n\n4\nrandom slopes only\nReaction ~ days_deprived + (0 + days_deprived | Subject)\n\n\n5\nmodel 2 + zero-covariances\nReaction ~ days_deprived + (days_deprived || Subject)\n\n\n\nModel 1:\n\\[\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    {\\tau_{00}}^2 & 0 \\\\\n                0 & 0 \\\\\n  \\end{array}\\right)\n\\end{equation*}\\]\nModels 2 and 3:\n\\[\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n             {\\tau_{00}}^2 & \\rho\\tau_{00}\\tau_{11} \\\\\n    \\rho\\tau_{00}\\tau_{11} &          {\\tau_{11}}^2 \\\\\n  \\end{array}\\right)\n\\end{equation*}\\]\nModel 4:\n\\[\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    0 &             0 \\\\\n    0 & {\\tau_{11}}^2 \\\\\n  \\end{array}\\right)\n\\end{equation*}\\]\nModel 5:\n\\[\\begin{equation*}\n  \\mathbf{\\Sigma} = \\left(\n  \\begin{array}{cc}\n    {\\tau_{00}}^2 &             0 \\\\\n                0 & {\\tau_{11}}^2 \\\\\n  \\end{array}\\right)\n\\end{equation*}\\]\nThe most reasonable model for these data is Model 2, so we’ll stick with that.\nLet’s fit the model, storing the result in object pp_mod.\n\npp_mod &lt;- lmer(Reaction ~ days_deprived + (days_deprived | Subject),\n               data = sleep2)\n\nsummary(pp_mod)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ days_deprived + (days_deprived | Subject)\n   Data: sleep2\n\nREML criterion at convergence: 1404.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0157 -0.3541  0.0069  0.4681  5.0732 \n\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n Residual               651.60   25.526       \nNumber of obs: 144, groups:  Subject, 18\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    267.967      8.266  32.418\ndays_deprived   11.435      1.845   6.197\n\nCorrelation of Fixed Effects:\n            (Intr)\ndays_deprvd -0.062\n\n\nBefore discussing how to interpret the output, let’s first plot the data against our model predictions. We can get model predictions using the predict() function (see ?predict.merMod for information about use with mixed-effects models).\nFirst, create a new data frame with predictor values for Subject and days_deprived.\n\nnewdata &lt;- crossing(\n  Subject = sleep2 |&gt;\n    pull(Subject) |&gt;\n    levels() |&gt;\n    factor(),\n  days_deprived = 0:7)\n\nhead(newdata, 17)\n\n# A tibble: 17 × 2\n   Subject days_deprived\n   &lt;fct&gt;           &lt;int&gt;\n 1 308                 0\n 2 308                 1\n 3 308                 2\n 4 308                 3\n 5 308                 4\n 6 308                 5\n 7 308                 6\n 8 308                 7\n 9 309                 0\n10 309                 1\n11 309                 2\n12 309                 3\n13 309                 4\n14 309                 5\n15 309                 6\n16 309                 7\n17 310                 0\n\n\nThen, run this through predict(). Typically we will add the prediction in as a new variable in the data frame of new data, giving it the same name as our DV (Reaction).\n\nnewdata2 &lt;- newdata |&gt;\n  mutate(Reaction = predict(pp_mod, newdata))\n\nNow we are ready to plot.\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_line(data = newdata2,\n            color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:7) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\nFigure 5.6: Data plotted against predictions from a partial pooling approach.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effects models</span>"
    ]
  },
  {
    "objectID": "mixed-effects.html#interpreting-lmer-output-and-extracting-estimates",
    "href": "mixed-effects.html#interpreting-lmer-output-and-extracting-estimates",
    "title": "5  Mixed-effects models",
    "section": "5.4 Interpreting lmer() output and extracting estimates",
    "text": "5.4 Interpreting lmer() output and extracting estimates\nThe call to lmer() returns a fitted model object of class “lmerMod”. To find out more about the lmerMod class, which is in turn a specialized version of the merMod class, see ?lmerMod-class.\n\n5.4.1 Fixed effects\nThe section of the output called Fixed effects: should look familiar; it is similar to what you would see in the output for a simple linear model fit by lm().\n\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    267.967      8.266  32.418\ndays_deprived   11.435      1.845   6.197\n\n\nThis indicates that the estimated mean reaction time for participants at Day 0 was about 268 milliseconds, with each day of sleep deprivation adding an additional 11 milliseconds to the response time, on average.\nIf we need to get the fixed effects from the model, we can extract them using fixef().\n\nfixef(pp_mod)\n\n  (Intercept) days_deprived \n    267.96742      11.43543 \n\n\nThe standard errors give us estimates of the variability for these parameters due to sampling error. You could use these to calculate the \\(t\\)-values or derive confidence intervals. Extract them using vcov(pp_mod) which gives a variance-covariance matrix (not the one associated with the random effects), pull out the diagonal using diag() and then take the square root using sqrt().\n\nsqrt(diag(vcov(pp_mod)))\n\n  (Intercept) days_deprived \n     8.265896      1.845293 \n\n# OR, equivalently using pipes:\n# vcov(pp_mod) |&gt; diag() |&gt; sqrt()\n\nNote that these \\(t\\) values do not appear with \\(p\\) values, as is customary in simpler modeling frameworks. There are multiple approaches for getting \\(p\\) values from mixed-effects models, with advantages and disadvantages to each; see Luke (2017) for a survey of options. The \\(t\\) values do not appear with degrees of freedom, because the degrees of freedom in a mixed-effects model are not well-defined. Often people will treat them as Wald \\(z\\) values, i.e., as observations from the standard normal distribution. Because the \\(t\\) distribution asymptotes the standard normal distribution as the number of observations goes to infinity, this “t-as-z” practice is legitimate if you have a large enough set of observations.\nTo calculate the Wald \\(z\\) values, just divide the fixed effect estimate by its standard error:\n\ntvals &lt;- fixef(pp_mod) / sqrt(diag(vcov(pp_mod)))\n\ntvals\n\n  (Intercept) days_deprived \n    32.418437      6.197082 \n\n\nYou can get the associated \\(p\\)-values using the following formula:\n\n2 * (1 - pnorm(abs(tvals)))\n\n  (Intercept) days_deprived \n  0.00000e+00   5.75197e-10 \n\n\nThis gives us strong evidence against the null hypothesis \\(H_0: \\gamma_1 = 0\\). Sleep deprivation does appear to increase response time.\nYou can get confidence intervals for the estimates using confint() (this technique uses the parametric bootstrap). confint() is a generic function, so to get help on this function, use ?confint.merMod.\n\nconfint(pp_mod)\n\nComputing profile confidence intervals ...\n\n\n                    2.5 %      97.5 %\n.sig01         19.0979934  46.3366599\n.sig02         -0.4051073   0.8058951\n.sig03          4.0079284  10.2487351\n.sigma         22.4666029  29.3494509\n(Intercept)   251.3443396 284.5904989\ndays_deprived   7.7245247  15.1463328\n\n\n\n\n5.4.2 Random effects\n\n\nRandom effects:\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n Residual               651.60   25.526       \nNumber of obs: 144, groups:  Subject, 18\n\n\nThe random effects part of the summary() output is less familiar. What you find here is a table with information about the variance components: the variance-covariance matrix (or matrices, if you have multiple random factors) and the residual variance.\nLet’s start with the Residual line. This tells us that the residual variance, \\(\\sigma^2\\), was estimated at about 651.6. The value in the next column, 25.526, is just the standard deviation, \\(\\sigma\\), which is the square root of the variance.\nWe extract the residual standard deviation using the sigma() function.\n\nsigma(pp_mod) # residual\n\n[1] 25.5264\n\n\nThe two lines above the Residual line give us information about the variance-covariance matrix for the Subject random factor.\n\n\n Groups   Name          Variance Std.Dev. Corr\n Subject  (Intercept)   958.35   30.957       \n          days_deprived  45.78    6.766   0.18\n\n\nThe values in the Variance column gives us the main diagonal of the matrix, and the Std.Dev. values are just the square roots of these values. The Corr column tells us the correlation between the intercept and slope.\nWe can extract these values from the fitted object pp_mod using the VarCorr() function. This returns a named list, with one element for each random factor. We have Subject as our only random factor, so the list will just be of length 1.\n\n# variance-covariance matrix for random factor Subject\nVarCorr(pp_mod)[[\"Subject\"]] # equivalently: VarCorr(pp_mod)[[1]]\n\n              (Intercept) days_deprived\n(Intercept)      958.3517      37.20460\ndays_deprived     37.2046      45.77766\nattr(,\"stddev\")\n  (Intercept) days_deprived \n    30.957255      6.765919 \nattr(,\"correlation\")\n              (Intercept) days_deprived\n(Intercept)     1.0000000     0.1776263\ndays_deprived   0.1776263     1.0000000\n\n\nThe first few lines are a printout of the variance covariance matrix. You can see the variances in the main diagonal. We can get these with:\n\ndiag(VarCorr(pp_mod)[[\"Subject\"]]) # just the variances\n\n  (Intercept) days_deprived \n    958.35165      45.77766 \n\n\nWe can get the correlation between the intecepts and slopes in two ways. First, by extracting the \"correlation\" attribute and then pulling out the element in row 1 column 2 ([1, 2]):\n\nattr(VarCorr(pp_mod)[[\"Subject\"]], \"correlation\")[1, 2] # the correlation\n\n[1] 0.1776263\n\n\nOr we can directly compute the value from the variance-covariance matrix itself.\n\n# directly compute correlation from variance-covariance matrix\nmx &lt;- VarCorr(pp_mod)[[\"Subject\"]]\n\n## if cov = rho * t00 * t11, then\n## rho = cov / (t00 * t11).\nmx[1, 2] / (sqrt(mx[1, 1]) * sqrt(mx[2, 2]))\n\n[1] 0.1776263\n\n\nWe can pull out the estimated random effects (BLUPS) using ranef(). Like VarCorr() , the result is a named list, with each element corresponding to a single random factor.\n\nranef(pp_mod)[[\"Subject\"]]\n\n    (Intercept) days_deprived\n308  24.4992891     8.6020000\n309 -59.3723102    -8.1277534\n310 -39.4762764    -7.4292365\n330   1.3500428    -2.3845976\n331  18.4576169    -3.7477340\n332  30.5270040    -4.8936899\n333  13.3682027     0.2888639\n334 -18.1583020     3.8436686\n335 -16.9737887   -12.0702333\n337  44.5850842    10.1760837\n349 -26.6839022     2.1946699\n350  -5.9657957     8.1758613\n351  -5.5710355    -2.3718494\n352  46.6347253    -0.5616377\n369   0.9616395     1.7385130\n370 -18.5216778     5.6317534\n371  -7.3431320     0.2729282\n372  17.6826159     0.6623897\n\n\nThere are other extractor functions that are useful. See ?merMod-class for details.\nWe can get fitted values from the model using fitted() and residuals using residuals(). (These functions take into account “the conditional modes of the random effects”, i.e., the BLUPS).\n\nmutate(sleep2,\n       fit = fitted(pp_mod),\n       resid = residuals(pp_mod)) %&gt;%\n  group_by(Subject) %&gt;%\n  slice(c(1,10)) %&gt;%\n  print(n = +Inf)\n\n# A tibble: 18 × 6\n# Groups:   Subject [18]\n   Reaction  Days Subject days_deprived   fit  resid\n      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;           &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     251.     2 308                 0  292. -41.7 \n 2     203.     2 309                 0  209.  -5.62\n 3     234.     2 310                 0  228.   5.83\n 4     284.     2 330                 0  269.  14.5 \n 5     302.     2 331                 0  286.  15.4 \n 6     273.     2 332                 0  298. -25.5 \n 7     277.     2 333                 0  281.  -4.57\n 8     243.     2 334                 0  250.  -6.44\n 9     254.     2 335                 0  251.   3.50\n10     292.     2 337                 0  313. -20.9 \n11     239.     2 349                 0  241.  -2.36\n12     256.     2 350                 0  262.  -5.80\n13     270.     2 351                 0  262.   7.50\n14     327.     2 352                 0  315.  12.3 \n15     257.     2 369                 0  269. -11.7 \n16     239.     2 370                 0  249. -10.5 \n17     278.     2 371                 0  261.  17.3 \n18     298.     2 372                 0  286.  11.9 \n\n\nFinally, we can get predictions for new data using predict(), as we did above. Below we use predict() to imagine what might have happened had we continued our study for three extra days.\n\n## create the table with new predictor values\nndat &lt;- crossing(Subject = sleep2 %&gt;% pull(Subject) %&gt;% levels() %&gt;% factor(),\n                 days_deprived = 8:10) %&gt;%\n  mutate(Reaction = predict(pp_mod, newdata = .))\n\n\nggplot(sleep2, aes(x = days_deprived, y = Reaction)) +\n  geom_line(data = bind_rows(newdata2, ndat),\n            color = 'blue') +\n  geom_point() +\n  scale_x_continuous(breaks = 0:10) +\n  facet_wrap(~Subject) +\n  labs(y = \"Reaction Time\", x = \"Days deprived of sleep (0 = baseline)\")\n\n\n\n\n\n\n\nFigure 5.7: Data against model with extrapolation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effects models</span>"
    ]
  },
  {
    "objectID": "mixed-effects.html#multi-level-app",
    "href": "mixed-effects.html#multi-level-app",
    "title": "5  Mixed-effects models",
    "section": "5.5 Multi-level app",
    "text": "5.5 Multi-level app\nTry out the multi-level web app to sharpen your understanding of the three different approaches to multi-level modeling.\n\n\n\n\nBates, D., Maechler, M., Bolker, B., & Walker, S. (2015). Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67, 1–48.\n\n\nBelenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H. C., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: A sleep dose-response study. Journal of Sleep Research, 12(1), 1–12.\n\n\nLuke, S. G. (2017). Evaluating significance in linear mixed-effects models in r. Behavior Research Methods, 49, 1494–1502.\n\n\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian course with examples in R and STAN. CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mixed-effects models</span>"
    ]
  },
  {
    "objectID": "multivariate.html",
    "href": "multivariate.html",
    "title": "6  Psychometrics and Multivariate Analysis",
    "section": "",
    "text": "(intentionally left blank)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Psychometrics and Multivariate Analysis</span>"
    ]
  },
  {
    "objectID": "path.html",
    "href": "path.html",
    "title": "7  Path analysis",
    "section": "",
    "text": "7.1 SEM software: lavaan\nFor estimating structural equation models in R we will use the lavaan add-on package (Rosseel, 2012) for R. You can install it by typing install.packages(\"lavaan\") in your R console window.\nThe main workhorse function for lavaan is called, simply, lavaan(). This is a powerful function that lets you specify a wide array of models. You can see all the options by looking at the help page (?lavaan::lavaan):\nThe ... in the function syntax suggests there are more options than are listed, and the help page says “See lavOptions for a complete list.” And if you type lavOptions() you will see there are 113 additional options! This can be quite overwhelming for the novice user.\nFortunately, lavaan provides two ‘convenience’ functions that hide away some of this complexity and that automatically choose sensible defaults. These functions are:\nIn what follows, we’ll be using sem() to fit a path model. The function documentation indicates what options of lavaan are set to by default (see ?lavOptions for further information).\nMany of these options are only relevant for models including latent variables, which our path models will not include. Thus, when using path analyses, these are the defaults you should keep in mind.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Path analysis</span>"
    ]
  },
  {
    "objectID": "path.html#sem-software-lavaan",
    "href": "path.html#sem-software-lavaan",
    "title": "7  Path analysis",
    "section": "",
    "text": "lavaan                 package:lavaan                  R Documentation\n\nFit a Latent Variable Model\n\nDescription:\n\n     Fit a latent variable model.\n\nUsage:\n\n     lavaan(model = NULL, data = NULL, ordered = NULL,\n            sampling.weights   = NULL,\n            sample.cov = NULL, sample.mean = NULL, sample.th = NULL,\n            sample.nobs = NULL,\n            group = NULL, cluster = NULL, constraints = \"\", \n            WLS.V = NULL, NACOV = NULL, ov.order = \"model\",\n            slotOptions = NULL, slotParTable = NULL, slotSampleStats = NULL,\n            slotData = NULL, slotModel = NULL, slotCache = NULL,\n            sloth1 = NULL,\n            ...)\n\n\n\n\n\nfunction\ndescription\n\n\n\n\nsem()\nFit a structural equation model\n\n\ncfa()\nFit a confirmatory factor analysis\n\n\n\n\n     The ‘sem’ function is a wrapper for the more general lavaan\n     function, but setting the following default options: ‘int.ov.free\n     = TRUE’, ‘int.lv.free = FALSE’, ‘auto.fix.first = TRUE’ (unless\n     ‘std.lv = TRUE’), ‘auto.fix.single = TRUE’, ‘auto.var = TRUE’,\n     ‘auto.cov.lv.x = TRUE’, ‘auto.efa = TRUE’, ‘auto.th = TRUE’,\n     ‘auto.delta = TRUE’, and ‘auto.cov.y = TRUE’.\n\n\n\n\n\n\n\n\noption setting\neffect\n\n\n\n\nint.ov.free=TRUE\nintercepts of observed variables are not fixed to zero\n\n\nauto.var=TRUE\nresidual variances for observed variables are set free\n\n\nauto.cov.y=TRUE\ncovariances of endogenous variables are modeled and set free",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Path analysis</span>"
    ]
  },
  {
    "objectID": "path.html#example-sem-vs-regression",
    "href": "path.html#example-sem-vs-regression",
    "title": "7  Path analysis",
    "section": "7.2 Example: SEM vs regression",
    "text": "7.2 Example: SEM vs regression\nWe will begin our practical introduction to path analysis with an example that represents a simple extension from ordinary regression modeling—namely, a situation where you have multiple response variables.\nLet’s assume we’re interested in the social determinants of narcisissm and we have given a (subclinical) population a narcissism questionnaire (scale goes from 1-100).\nTo follow along, download the following files. NB: These files contain simulated (i.e., not real) data, so don’t draw any conclusions about narcissism from the analyses.\n\n\n\nfile\ndescription\n\n\n\n\nnarc_1.csv\nnarcisissm data, 1 outcome variable\n\n\nnarc_2.csv\nnarcisissm data, 2 outcome variables\n\n\n\n\n7.2.1 single outcome data\nWe might have the following theory of narcisissm, where parenting style (specifically, the degree of “helicopter” parenting where parents smother their child with attention) and economic privilege jointly determine the level of adult narcissism.\n\n\n\n\n\nHere, the SEM framework wouldn’t buy us anything because we can fit this model using basic regression.\n\nlibrary(\"tidyverse\") # for read_csv\nlibrary(\"lavaan\")    # for SEM; we'll need these functions later\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\nnarc_1 &lt;- read_csv(\"data/narc_1.csv\", col_types = \"ddd\")\n\nmod_reg &lt;- lm(Narc ~ HP + EP, data = narc_1)\n\nsummary(mod_reg)\n\n\nCall:\nlm(formula = Narc ~ HP + EP, data = narc_1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.6504  -4.5028  -0.3351   4.4134  18.0993 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.62670    6.48476   6.573 4.31e-10 ***\nHP           0.18531    0.06704   2.764  0.00625 ** \nEP           0.33032    0.12922   2.556  0.01133 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.149 on 197 degrees of freedom\nMultiple R-squared:  0.07027,   Adjusted R-squared:  0.06083 \nF-statistic: 7.445 on 2 and 197 DF,  p-value: 0.0007642\n\n\n\nsem_formula &lt;- 'Narc ~ HP + EP'\n\nmod_sem &lt;- sem(sem_formula, data = narc_1)\n\nsummary(mod_sem, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         3\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Narc ~                                              \n    HP                0.185    0.067    2.785    0.005\n    EP                0.330    0.128    2.576    0.010\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .Narc             50.345    5.034   10.000    0.000\n\nR-Square:\n                   Estimate\n    Narc              0.070\n\n\nYou can see that the regression coefficients closely match the ones from the regression. We also get an estimate of the residual variance for the DV of 50.345, which is close to the “Residual standard error” in the regression model, because sqrt(50.345) = 7.095, which is close to 7.149.\nThe summary suggests that we have estimated 3 parameters. What are they? Let’s use lavaan’s parTable() function to see.\n\nparTable(mod_sem)\n\n  id  lhs op  rhs user block group free ustart exo label plabel  start    est\n1  1 Narc  ~   HP    1     1     1    1     NA   0         .p1.  0.185  0.185\n2  2 Narc  ~   EP    1     1     1    2     NA   0         .p2.  0.330  0.330\n3  3 Narc ~~ Narc    0     1     1    3     NA   0         .p3. 50.345 50.345\n4  4   HP ~~   HP    0     1     1    0     NA   1         .p4. 56.991 56.991\n5  5   HP ~~   EP    0     1     1    0     NA   1         .p5.  1.423  1.423\n6  6   EP ~~   EP    0     1     1    0     NA   1         .p6. 15.341 15.341\n     se\n1 0.067\n2 0.128\n3 5.034\n4 0.000\n5 0.000\n6 0.000\n\n\nThe ‘free’ column counts the free parameters. You’ll note that Narc ~ HP, Narc ~ EP and Narc ~~ Narc are the (population) direct effects and variance that are being estimated. All the variances/covariances for the exogenous variables are just directly calculated from the sample, and so they have no standard errors (= 0).\nWe can also get confidence intervals using the parameterEstimates() function, which also puts the estimates in a table, making them easier to access for reporting.\n\nparameterEstimates(mod_sem)\n\n   lhs op  rhs    est    se      z pvalue ci.lower ci.upper\n1 Narc  ~   HP  0.185 0.067  2.785  0.005    0.055    0.316\n2 Narc  ~   EP  0.330 0.128  2.576  0.010    0.079    0.582\n3 Narc ~~ Narc 50.345 5.034 10.000  0.000   40.477   60.212\n4   HP ~~   HP 56.991 0.000     NA     NA   56.991   56.991\n5   HP ~~   EP  1.423 0.000     NA     NA    1.423    1.423\n6   EP ~~   EP 15.341 0.000     NA     NA   15.341   15.341\n\n\nWe can also get standardized estimates. The std.all column is the one you want to look at.\n\nparameterEstimates(mod_sem, standardized = TRUE)\n\n   lhs op  rhs    est    se      z pvalue ci.lower ci.upper std.lv std.all\n1 Narc  ~   HP  0.185 0.067  2.785  0.005    0.055    0.316  0.185   0.190\n2 Narc  ~   EP  0.330 0.128  2.576  0.010    0.079    0.582  0.330   0.176\n3 Narc ~~ Narc 50.345 5.034 10.000  0.000   40.477   60.212 50.345   0.930\n4   HP ~~   HP 56.991 0.000     NA     NA   56.991   56.991 56.991   1.000\n5   HP ~~   EP  1.423 0.000     NA     NA    1.423    1.423  1.423   0.048\n6   EP ~~   EP 15.341 0.000     NA     NA   15.341   15.341 15.341   1.000\n  std.nox\n1   0.025\n2   0.045\n3   0.930\n4  56.991\n5   1.423\n6  15.341\n\n\n\n\n\n\n\n\n\n7.2.2 Adding a second DV\nUp to this point using SEM doesn’t really buy us anything over and above ordinary regression. But the situation changes if we have more than one DV. Let’s say that instead of a single measure we had a measure of two types of narcissism: vulnerable narcissism (VN) and grandiose narcissism (GN), which, for the sake of example, let’s say are not mutually exclusive (i.e., a person can have a little bit of each).\nWe could perform two separate regressions, but this has the problem that each regression is performed independently of the other, which only makes sense if we are sure that the two DVs are totally uncorrelated. If they are correlated, then the estimates from this approach will be biased. Here, a SEM approach is useful because it can account for this covariance.\n\n\nnarc_2 &lt;- read_csv(\"data/narc_2.csv\",\n                   col_types = \"dddd\")\n\nsem_formula2 &lt;- '\nVN ~ HP + EP\nGN ~ HP + EP'\n\nmod_sem2 &lt;- sem(sem_formula2, data = narc_2)\n\nsummary(mod_sem2, rsquare = TRUE)\n\nlavaan 0.6-19 ended normally after 9 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                           200\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  VN ~                                                \n    HP                0.131    0.030    4.351    0.000\n    EP               -0.193    0.058   -3.315    0.001\n  GN ~                                                \n    HP                0.145    0.039    3.754    0.000\n    EP               -0.014    0.074   -0.190    0.849\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .VN ~~                                               \n   .GN                0.994    0.941    1.056    0.291\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .VN               10.375    1.037   10.000    0.000\n   .GN               16.975    1.698   10.000    0.000\n\nR-Square:\n                   Estimate\n    VN                0.125\n    GN                0.066\n\n\nLet’s have a look at the model estimates, including standardized coefficients, and add the (raw) coefficients to our path diagram.\n\nparameterEstimates(mod_sem2, standardized = TRUE)\n\n   lhs op rhs    est    se      z pvalue ci.lower ci.upper std.lv std.all\n1   VN  ~  HP  0.131 0.030  4.351  0.000    0.072    0.191  0.131   0.288\n2   VN  ~  EP -0.193 0.058 -3.315  0.001   -0.307   -0.079 -0.193  -0.219\n3   GN  ~  HP  0.145 0.039  3.754  0.000    0.069    0.221  0.145   0.257\n4   GN  ~  EP -0.014 0.074 -0.190  0.849   -0.160    0.132 -0.014  -0.013\n5   VN ~~  VN 10.375 1.037 10.000  0.000    8.341   12.408 10.375   0.875\n6   GN ~~  GN 16.975 1.698 10.000  0.000   13.648   20.302 16.975   0.934\n7   VN ~~  GN  0.994 0.941  1.056  0.291   -0.850    2.838  0.994   0.075\n8   HP ~~  HP 56.991 0.000     NA     NA   56.991   56.991 56.991   1.000\n9   HP ~~  EP  1.423 0.000     NA     NA    1.423    1.423  1.423   0.048\n10  EP ~~  EP 15.341 0.000     NA     NA   15.341   15.341 15.341   1.000\n   std.nox\n1    0.038\n2   -0.056\n3    0.034\n4   -0.003\n5    0.875\n6    0.934\n7    0.075\n8   56.991\n9    1.423\n10  15.341\n\n\n\n\n\n\n\nRosseel, Y. (2012). Lavaan: An r package for structural equation modeling. Journal of Statistical Software, 48(2), 1–36. https://doi.org/10.18637/jss.v048.i02",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Path analysis</span>"
    ]
  },
  {
    "objectID": "mediation.html",
    "href": "mediation.html",
    "title": "8  Mediation models",
    "section": "",
    "text": "8.1 Fitting mediation models\nLet’s take a look at how to fit the model and estimate these effects in lavaan.\nWe can specify the model one of two ways, either as two or three regression equations. Both models will be equivalent, but using two equations (one for each endogenous variable) is the more typical approach.\nlibrary(\"lavaan\")\n\nmod_med_2 &lt;- \"\n# path a\nOI ~ IP\n# paths b and c'\nPR ~ OI + IP\"\n\nmod_fit_2 &lt;- sem(mod_med_2, data = wpdat)\nWe can specify all the paths individually, just to check that the model is the same.\nmod_med_3 &lt;- \"\n# direct effect (path c')\nPR ~ IP\n# path a\nOI ~ IP\n# path b\nPR ~ OI\"\n\nmod_fit_3 &lt;- sem(mod_med_3, data = wpdat)\n\nparameterEstimates(mod_fit_3) |&gt; arrange(op, lhs, rhs)\n\n  lhs op rhs     est    se      z pvalue ci.lower ci.upper\n1  OI  ~  IP  22.978 3.020  7.608  0.000   17.059   28.898\n2  PR  ~  IP   0.162 2.922  0.055  0.956   -5.565    5.888\n3  PR  ~  OI   0.698 0.030 23.458  0.000    0.639    0.756\n4  IP ~~  IP   0.012 0.000     NA     NA    0.012    0.012\n5  OI ~~  OI 109.309 4.888 22.361  0.000   99.728  118.890\n6  PR ~~  PR  96.697 4.324 22.361  0.000   88.221  105.172\nIf we compare to the parameter estimates for the 2 equation syntax, we can see that they are identical.\nparameterEstimates(mod_fit_2) |&gt; arrange(op, lhs, rhs)\n\n  lhs op rhs     est    se      z pvalue ci.lower ci.upper\n1  OI  ~  IP  22.978 3.020  7.608  0.000   17.059   28.898\n2  PR  ~  IP   0.162 2.922  0.055  0.956   -5.565    5.888\n3  PR  ~  OI   0.698 0.030 23.458  0.000    0.639    0.756\n4  IP ~~  IP   0.012 0.000     NA     NA    0.012    0.012\n5  OI ~~  OI 109.309 4.888 22.361  0.000   99.728  118.890\n6  PR ~~  PR  96.697 4.324 22.361  0.000   88.221  105.172\nLet’s look at the model output, including \\(R^2\\) and standardized estimates.\nsummary(mod_fit_2, standardized=TRUE, rsquare=TRUE)\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         5\n\n  Number of observations                          1000\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  OI ~                                                                  \n    IP               22.978    3.020    7.608    0.000   22.978    0.234\n  PR ~                                                                  \n    OI                0.698    0.030   23.458    0.000    0.698    0.606\n    IP                0.162    2.922    0.055    0.956    0.162    0.001\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .OI              109.309    4.888   22.361    0.000  109.309    0.945\n   .PR               96.697    4.324   22.361    0.000   96.697    0.632\n\nR-Square:\n                   Estimate\n    OI                0.055\n    PR                0.368\nOur model is just-identified (\\(df = 0\\)), meaning that the model fit is perfect and we have no additional degrees of freedom for considering measures of model fit. We can also see that our model gives us an \\(R^2\\) of 0.368 for the endogenous variable PR, which means that it accounts for about 36.8% of the variation in productivity. We can also see that the effect of in-person work only accounts for about 5.5% of the variation in organizational knowledge, indicating that there is much more to learn about how such knowledge develops. Finally, we can see from the output that the direct effect \\(c'\\) (PR ~ IP; \\(p = 0\\\\.956\\)) is not significant in this model (it did not ‘survive’ controlling for the mediation). We should be careful not to accept the null hypothesis, however; we would have to perform a more stringent analysis such as an equivalence test before doing so.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Mediation models</span>"
    ]
  },
  {
    "objectID": "mediation.html#calculating-indirect-effects",
    "href": "mediation.html#calculating-indirect-effects",
    "title": "8  Mediation models",
    "section": "8.2 Calculating indirect effects",
    "text": "8.2 Calculating indirect effects\nWhat’s missing from the output above are calculations of the indirect effect, the total effect, and the proportion of the total effect explained by the indirect effect. We could compute these from the output using simple arithmetic, but then we would lack information about their standard errors.\nTo get these values we need to update the model syntax and include calculations by using the x := ... operator, which you can think of as variable x “is calculated as” (whatever is on the right hand side). Using this syntax you can do arbitrary calculations and get statistical information in the output.\nHowever, to perform such calculations we first need to be able to give model parameters predictable names. Up to now, when we specified a regression model in lavaan we’ve followed the standard R model syntax, where the regression coefficients are omitted.\nHowever, we can also use a lavaan-specific syntax where we give the coefficients names. So, y ~ x would be the same as y ~ b1 * x. This will tell lavaan to give the coefficient for x the name b1 rather than just choosing some default name. This allows us to refer to it again later. We’re limited in our names to using alphanumeric characters (e.g., A-Z and 0-9). So we can’t legally call something c' but we can call it cprime.\n\nmod_med_names &lt;- \"\n# path a\nOI ~ a * IP\n# paths b and c'\nPR ~ b * OI + cprime * IP\n\"\n\nmod_fit_names &lt;- sem(mod_med_names, data = wpdat)\nparameterEstimates(mod_fit_names)\n\n  lhs op rhs  label     est    se      z pvalue ci.lower ci.upper\n1  OI  ~  IP      a  22.978 3.020  7.608  0.000   17.059   28.898\n2  PR  ~  OI      b   0.698 0.030 23.458  0.000    0.639    0.756\n3  PR  ~  IP cprime   0.162 2.922  0.055  0.956   -5.565    5.888\n4  OI ~~  OI        109.309 4.888 22.361  0.000   99.728  118.890\n5  PR ~~  PR         96.697 4.324 22.361  0.000   88.221  105.172\n6  IP ~~  IP          0.012 0.000     NA     NA    0.012    0.012\n\n\nNow we can expand the syntax further to compute or mediation statistics.\n\nmod_med_stats &lt;- \"\n# regression equations\nOI ~ a * IP\nPR ~ b * OI + cprime * IP\n\n# compute mediation stats; indir = indirect effect\nindir := a * b\ntotal := indir + cprime\nprop := indir / total\n\"\n\nmod_fit_stats &lt;- sem(mod_med_stats, data = wpdat)\nparameterEstimates(mod_fit_stats)\n\n    lhs op          rhs  label     est    se      z pvalue ci.lower ci.upper\n1    OI  ~           IP      a  22.978 3.020  7.608  0.000   17.059   28.898\n2    PR  ~           OI      b   0.698 0.030 23.458  0.000    0.639    0.756\n3    PR  ~           IP cprime   0.162 2.922  0.055  0.956   -5.565    5.888\n4    OI ~~           OI        109.309 4.888 22.361  0.000   99.728  118.890\n5    PR ~~           PR         96.697 4.324 22.361  0.000   88.221  105.172\n6    IP ~~           IP          0.012 0.000     NA     NA    0.012    0.012\n7 indir :=          a*b  indir  16.032 2.215  7.237  0.000   11.690   20.374\n8 total := indir+cprime  total  16.193 3.537  4.578  0.000    9.261   23.126\n9  prop :=  indir/total   prop   0.990 0.179  5.539  0.000    0.640    1.340\n\n\nNow we get statistics for the computed values in the output. Note that the \\(z\\) statistic in the output is our test statistics. For the indirect effect, this is known as the Sobel statistic. This test assumes normality, but assumption is unlikely to hold when we are looking at the distribution of an estimator calculated as a product. So, a better method to use for getting standard errors is bootstrapping, which we can specify in the call to sem() using the argument se=\"boot\". Let’s re-fit the model. This will take longer due to the bootstrapping.\n\nmod_fit_boot &lt;- sem(mod_med_stats, data = wpdat, se = \"boot\")\n\nWarning: lavaan-&gt;lav_model_nvcov_bootstrap():  \n   19 bootstrap runs failed or did not converge.\n\n\nWe got a warning about non-convergence when re-fitting the model to the bootstrapped data. This can happen sometimes. The default number of bootstrap samples is 1,000, so we don’t have to worry too much about losing data for 19 of them. But if there had been more than 19 instances, we might want to increase the number of bootstrap samples (see ?lavOptions for information).\n\nparameterEstimates(mod_fit_boot)\n\n    lhs op          rhs  label     est    se      z pvalue ci.lower ci.upper\n1    OI  ~           IP      a  22.978 2.978  7.716  0.000   17.240   28.979\n2    PR  ~           OI      b   0.698 0.031 22.374  0.000    0.634    0.760\n3    PR  ~           IP cprime   0.162 2.822  0.057  0.954   -5.386    5.500\n4    OI ~~           OI        109.309 4.827 22.646  0.000  100.536  118.833\n5    PR ~~           PR         96.697 4.243 22.788  0.000   88.222  104.994\n6    IP ~~           IP          0.012 0.000     NA     NA    0.012    0.012\n7 indir :=          a*b  indir  16.032 2.153  7.445  0.000   11.881   20.296\n8 total := indir+cprime  total  16.193 3.427  4.725  0.000    9.554   22.642\n9  prop :=  indir/total   prop   0.990 0.217  4.571  0.000    0.736    1.499\n\n\nFrom the output we can see that our estimated mediation effect is 16.032, which is statistically significant, \\(z = 7\\\\.445\\), \\(p &lt; \\\\.001\\). We can also see that the mediation effect accounts for about 99.0% of the total effect.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Mediation models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bates, D., Maechler, M., Bolker, B., & Walker, S. (2015). Fitting\nlinear mixed-effects models using lme4. Journal of Statistical\nSoftware, 67, 1–48.\n\n\nBelenky, G., Wesensten, N. J., Thorne, D. R., Thomas, M. L., Sing, H.\nC., Redmond, D. P., Russo, M. B., & Balkin, T. J. (2003). Patterns\nof performance degradation and restoration during sleep restriction and\nsubsequent recovery: A sleep dose-response study. Journal of\nSleep Research, 12(1), 1–12.\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence\ntesting for psychological research: A tutorial. Advances in Methods\nand Practices in Psychological Science, 1, 259–269. https://journals.sagepub.com/doi/abs/10.1177/2515245918770963\n\n\nLuke, S. G. (2017). Evaluating significance in linear mixed-effects\nmodels in r. Behavior Research Methods,\n49, 1494–1502.\n\n\nMcElreath, R. (2020). Statistical Rethinking: A\nBayesian course with examples in R and STAN. CRC Press.\n\n\nRosseel, Y. (2012). Lavaan: An r package for structural equation\nmodeling. Journal of Statistical Software, 48(2),\n1–36. https://doi.org/10.18637/jss.v048.i02\n\n\nStevens, S. S. (1946). On the theory of scales of measurement.\nScience, 103(2684), 677–680. https://doi.org/10.1126/science.103.2684.677\n\n\nVanhove, J. (2021). Collinearity isn’t a disease that needs curing.\nMeta-Psychology, 5.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "criterion\nThe :response variable, usually used in the regression context to refer to the variable whose value we are trying to predict. Conventionally, this variable appears on the left side of the regression equation. For example, in the simple regression equation\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\]\nthe variable \\(Y_i\\) is the response variable.\nSee also :predictor variable.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#dependent-variable",
    "href": "glossary.html#dependent-variable",
    "title": "Glossary",
    "section": "dependent variable",
    "text": "dependent variable\nThe variable in an analysis corresponding to an outcome that we are interested in, often abbreviated as “DV”. The value of the DV is assumed to be related in some way to the value of an :independent variable or set of independent variables.\nConventionally, researchers use the more general term :response variable or :criterion, with “dependent variable” usually referring to the response variable in an experiment.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#ide",
    "href": "glossary.html#ide",
    "title": "Glossary",
    "section": "integrated development environment (IDE)",
    "text": "integrated development environment (IDE)\nAn integrated development environment or IDE is software that you use to develop computer programs. In the context of data analysis we often refer to the plain text files containing the programming instructions as a “script”. The IDE includes things like a script editor and features like syntax highlighting that make it easier to develop scripts.\nIt is always important to distinguish the software that provides the programming environment (e.g., RStudio Desktop, created by the for-profit company Posit) from the software that performs the computations (e.g., R, created for free by numerous volunteers). The scripts you develop are (ideally) independent of the IDE.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#independent-variable",
    "href": "glossary.html#independent-variable",
    "title": "Glossary",
    "section": "independent variable",
    "text": "independent variable\nA variable whose value is assumed to affect the value of a :dependent variable, abbreviated as “IV”. In a randomized experiment, the value of the IV is manipulated by the experimenter. Similar to a :predictor variable, but usually used in the context of randomized experiments.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#deviation-score",
    "href": "glossary.html#deviation-score",
    "title": "Glossary",
    "section": "deviation score",
    "text": "deviation score\nA transformed score, where the mean is subtracted from the original score. If \\(X\\) is a variable, then the deviation score is \\(X - \\bar{X}\\), where \\(\\bar{X}\\) is the :mean of \\(X\\).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#fixed-factor",
    "href": "glossary.html#fixed-factor",
    "title": "Glossary",
    "section": "fixed factor",
    "text": "fixed factor\nTODO\nSee also :random factor",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#interval-scale",
    "href": "glossary.html#interval-scale",
    "title": "Glossary",
    "section": "interval scale of measurement",
    "text": "interval scale of measurement\nOne of the four basic :scales of measurement. Values measured on an interval scale are numerical, quantitative values where differences between measurements represent meaningful ‘distances’ on the scale. Unlike the :ratio scale, interval scales lack a true zero corresponding to the absence of the quantity being measured. Instead, the zero point is determined by convention. A common example is temperature, where zero does not reflect an absence of temperature but is defined more arbitrarily (e.g., zero degrees Centigrade is different from zero degrees Farenheit).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#linear-mixed-effects-model",
    "href": "glossary.html#linear-mixed-effects-model",
    "title": "Glossary",
    "section": "linear mixed-effects model",
    "text": "linear mixed-effects model\nTODO",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#mean",
    "href": "glossary.html#mean",
    "title": "Glossary",
    "section": "mean",
    "text": "mean\nA measure of the central tendency of a variable, calculated by summing up the values and then dividing by \\(N\\), the number of values. The mean of a set of observations is often notated by putting a horizontal bar over the symbol for the variable; e.g., for variable \\(X\\), the mean would be notated as \\(\\bar{X}\\). In contrast, population means are often denoted using the greek letter \\(\\mu\\) (“mu”) with the variable name as a subscript, e.g., \\(\\mu_x\\).\nThe mean is calculated using the formula:\n\\[\\bar{X} = \\frac{\\Sigma X}{N}\\]\nwhere \\(N\\) is the number of observations, and \\(\\Sigma\\) is an instruction to sum together all of the \\(X\\) values.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#multilevel",
    "href": "glossary.html#multilevel",
    "title": "Glossary",
    "section": "multilevel",
    "text": "multilevel\nWe say that a dataset is multilevel when there are multiple measurements on the dependent variable for a given sampling unit. Usually, the sampling unit is a participant in a research study.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#multivariate",
    "href": "glossary.html#multivariate",
    "title": "Glossary",
    "section": "multivariate",
    "text": "multivariate\nWe say that a dataset is multivariate when there are multiple dependent variables rather than a single dependent variable.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#nominal-scale",
    "href": "glossary.html#nominal-scale",
    "title": "Glossary",
    "section": "nominal scale of measurement",
    "text": "nominal scale of measurement\nOne of the four basic :scales of measurement. Values measured on a nominal scale consist of discrete, unordered categories. “Scottish political party affiliation” is an example of a nominal variable, which (in 2025) might be given the following levels: Conservative, Labour, SNP, Lib Dem, Green, Reform, Alba, Other. Note that the levels correspond to discrete categories rather than numeric values, and that there is no intrinsic ordering among them. An easy way to remember this is that “nominal” comes from the Latin “nomen” which means name. Nominal data contrasts with :ordinal data, where in the later case there is an intrinsic ordering to the categories.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#Nutshell",
    "href": "glossary.html#Nutshell",
    "title": "Glossary",
    "section": "Nutshell",
    "text": "Nutshell\nNutshell is a web tool developed by Nicky Case to make expandable, embeddable explanations within a web page, like this one. A Nutshell definition appears with a dashed underline and two dots to the upper left. When you click on the link, the definition of the term will appear embedded within the page. Click the link again (or the little X at the bottom) to remove the definition.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#ordinal-scale",
    "href": "glossary.html#ordinal-scale",
    "title": "Glossary",
    "section": "ordinal scale",
    "text": "ordinal scale\nOne of the four basic :scales of measurement. Ordinal data is like :nominal data, except there is an intrinsic rank ordering among the categories, but the distance between the ranks may not be (psychologically) equal, unlike with :interval data. An example would be a Likert agreement scale with five categories: Strongly Agree, Somewhat Agree, Neither Agree Nor Disagree, Somewhat Disagree, Strongly Disagree.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#predictor-variable",
    "href": "glossary.html#predictor-variable",
    "title": "Glossary",
    "section": "predictor variable",
    "text": "predictor variable\nA variable in a regression whose value is used to predict the value of the :response variable, often in tandem with other predictor variables. For example, in the simple regression equation\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\]\nthe variable \\(X_i\\) is a predictor variable.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#p-value",
    "href": "glossary.html#p-value",
    "title": "Glossary",
    "section": "\\(p\\)-value",
    "text": "\\(p\\)-value\nA probability associated with a test statistic; specifically, the probability, assuming the null hypothesis is true, of obtaining a test statistic at least as extreme as, or more extreme than, the one observed.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#power",
    "href": "glossary.html#power",
    "title": "Glossary",
    "section": "power",
    "text": "power\nThe probability of rejecting the null hypothesis when it is false, for a specific analysis, effect size, sample size, and significance level.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#random-factor",
    "href": "glossary.html#random-factor",
    "title": "Glossary",
    "section": "random factor",
    "text": "random factor\nTODO\nSee also :fixed factor.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#ratio-scale-of-measurement-ratio-scale",
    "href": "glossary.html#ratio-scale-of-measurement-ratio-scale",
    "title": "Glossary",
    "section": "ratio scale of measurement (#ratio-scale)",
    "text": "ratio scale of measurement (#ratio-scale)\nOne of the four basic :scales of measurement. Ratio data, like :interval data is quantitative in nature and with a consistent distance between units, but unlike interval data, it has a true zero representing the absence of the quantity being measure. Response time, which is a measure of duration, is an example of ratio scale data, where ‘zero’ is a theoretically possible value, meaning that a response was made instantaneously (although such a fast response time is unattainable in practice).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#regression-coefficient",
    "href": "glossary.html#regression-coefficient",
    "title": "Glossary",
    "section": "regression coefficient",
    "text": "regression coefficient\nA parameter in a regression equation, whose true (i.e., population) value is usually estimated from the sample. Each predictor in a model is weighted by an associated regression coefficient. For example, in the simple regression equation\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are both regression coefficients.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#response-variable",
    "href": "glossary.html#response-variable",
    "title": "Glossary",
    "section": "response variable",
    "text": "response variable\nThe outcome variable in a regression. Used interchangeably with :criterion variable.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#scales-of-measurement",
    "href": "glossary.html#scales-of-measurement",
    "title": "Glossary",
    "section": "scales of measurement",
    "text": "scales of measurement\nA typology introduced by Stevens (1946) of four types of measurement scales found in psychology: :nominal, :interval, :ratio, and :ordinal.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#standard-deviation",
    "href": "glossary.html#standard-deviation",
    "title": "Glossary",
    "section": "standard deviation",
    "text": "standard deviation\nA measure of variability, defined as the mean of the :deviation scores.\n\n\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677–680. https://doi.org/10.1126/science.103.2684.677",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Notation",
    "section": "",
    "text": "blah!!",
    "crumbs": [
      "Notation"
    ]
  },
  {
    "objectID": "cfa.html",
    "href": "cfa.html",
    "title": "9  Confirmatory factor analysis",
    "section": "",
    "text": "This walkthrough will take a look at a famous dataset in SEM, namely the Holzinger & Swineford (1939) study of the cognitive ability of children in two schools. We will perform a Confirmatory Factor Analysis (CFA) on these data in R using lavaan. In developing these materials, I relied on two excellent sources: the CFA tutorial on the lavaan website as well as this video by Sasha Epskamp.\nLet’s set up the environment and have a look.\n\nlibrary(\"lavaan\")\nlibrary(\"tidyverse\") # for some minimal data wrangling\n\n\n?HolzingerSwineford1939\n\nHolzingerSwineford1939         package:lavaan          R Documentation\n\nHolzinger and Swineford Dataset (9 Variables)\n\nDescription:\n\n     The classic Holzinger and Swineford (1939) dataset consists of\n     mental ability test scores of seventh- and eighth-grade children\n     from two different schools (Pasteur and Grant-White). In the\n     original dataset (available in the ‘MBESS’ package), there are\n     scores for 26 tests. However, a smaller subset with 9 variables is\n     more widely used in the literature (for example in Joreskog's 1969\n     paper, which also uses the 145 subjects from the Grant-White\n     school only).\n\nUsage:\n\n     data(HolzingerSwineford1939)\n     \nFormat:\n\n     A data frame with 301 observations of 15 variables.\n\n     ‘id’ Identifier\n\n     ‘sex’ Gender\n\n     ‘ageyr’ Age, year part\n\n     ‘agemo’ Age, month part\n\n     ‘school’ School (Pasteur or Grant-White)\n\n     ‘grade’ Grade\n\n     ‘x1’ Visual perception\n\n     ‘x2’ Cubes\n\n     ‘x3’ Lozenges\n\n     ‘x4’ Paragraph comprehension\n\n     ‘x5’ Sentence completion\n\n     ‘x6’ Word meaning\n\n     ‘x7’ Speeded addition\n\n     ‘x8’ Speeded counting of dots\n\n     ‘x9’ Speeded discrimination straight and curved capitals\n\nSource:\n\n     This dataset was originally retrieved from\n     http://web.missouri.edu/~kolenikovs/stata/hs-cfa.dta (link no\n     longer active) and converted to an R dataset.\n\nReferences:\n\n     Holzinger, K., and Swineford, F. (1939). A study in factor\n     analysis: The stability of a bifactor solution. Supplementary\n     Educational Monograph, no. 48.  Chicago: University of Chicago\n     Press.\n\n     Joreskog, K. G. (1969). A general approach to confirmatory maximum\n     likelihood factor analysis. _Psychometrika_, 34, 183-202.\nThe basic idea behind the dataset is that you have three indicators for each of three latent factors representing different cognitive abilities, as shown in Figure 9.1 below.\n\n\n\n\n\n\nFigure 9.1: Confirmatory factor model for the Holzinger & Swineford data. Source: https://lavaan.ugent.be/tutorial/cfa.html\n\n\n\nThe data from the study is ‘built-in’ to the lavaan package, which means that after you’ve loaded lavaan using library(\"lavaan\"), the dataset will be accessible as the named variable HolzingerSwineford1939.\n\nHolzingerSwineford1939 |&gt;\n  as_tibble() # convert from data.frame to tibble for printing\n\n# A tibble: 301 × 15\n      id   sex ageyr agemo school  grade    x1    x2    x3    x4    x5    x6\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1    13     1 Pasteur     7  3.33  7.75 0.375  2.33  5.75 1.29 \n 2     2     2    13     7 Pasteur     7  5.33  5.25 2.12   1.67  3    1.29 \n 3     3     2    13     1 Pasteur     7  4.5   5.25 1.88   1     1.75 0.429\n 4     4     1    13     2 Pasteur     7  5.33  7.75 3      2.67  4.5  2.43 \n 5     5     2    12     2 Pasteur     7  4.83  4.75 0.875  2.67  4    2.57 \n 6     6     2    14     1 Pasteur     7  5.33  5    2.25   1     3    0.857\n 7     7     1    12     1 Pasteur     7  2.83  6    1      3.33  6    2.86 \n 8     8     2    12     2 Pasteur     7  5.67  6.25 1.88   3.67  4.25 1.29 \n 9     9     2    13     0 Pasteur     7  4.5   5.75 1.5    2.67  5.75 2.71 \n10    11     2    12     5 Pasteur     7  3.5   5.25 0.75   2.67  5    2.57 \n# ℹ 291 more rows\n# ℹ 3 more variables: x7 &lt;dbl&gt;, x8 &lt;dbl&gt;, x9 &lt;dbl&gt;\n\n\nWhen performing CFA in lavaan, we can either write everything out in full lavaan syntax and fit the model using lavaan(), or we can use the convenience function cfa() which has appropriate defaults. We will do the latter.\nThe first step is to define the syntax of the model. To perform CFA, we need to use a new syntactic operator in lavaan, =~, which is used to define your measurement model. When you see =~ you should read this as “… is measured by …”. This operator will appear in a formula such as lv =~ i1 + i2 + ... which means “latent variable lv is measured by indicators i1, i2, etc.\nSo, the model syntax to reproduce the diagram in Figure 9.1 would be as follows (with the syntax enclosed between single quotes, as is usual in lavaan).\n\nmod_hs &lt;- '\nvisual  =~ x1 + x2 + x3\ntextual =~ x4 + x5 + x6\nspeed   =~ x7 + x8 + x9\n'\n\nThat is really it, if we are using the function cfa(). Everything else in the model—the variances of the model indicators, the covariances among the latent factors—will be included in the model by default.\nWhat’s left is just to run cfa().\n\nfit_hs &lt;- cfa(mod_hs, data = HolzingerSwineford1939)\n\nfit_hs  ## just printing\n\nlavaan 0.6-19 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                85.306\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\n\nHere we can see that we estimated 21 parameters and thus has 24 degrees of freedom. It also reports back that the default estimation algorithm of Maximum Likelihood (ML) was used. The model has a \\(\\chi^2(24)=85.306\\), with \\(p&lt;.001\\), so the null hypothesis of perfect fit is rejected (as it often is with moderately large datasets).\nLet’s get more information using summary(), including measures of fit.\n\nsummary(fit_hs, fit.measures = TRUE)\n\nlavaan 0.6-19 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                85.306\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.931\n  Tucker-Lewis Index (TLI)                       0.896\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3737.745\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7517.490\n  Bayesian (BIC)                              7595.339\n  Sample-size adjusted Bayesian (SABIC)       7528.739\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.092\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.114\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.840\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.065\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.554    0.100    5.554    0.000\n    x3                0.729    0.109    6.685    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5                1.113    0.065   17.014    0.000\n    x6                0.926    0.055   16.703    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.180    0.165    7.152    0.000\n    x9                1.082    0.151    7.155    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual ~~                                           \n    textual           0.408    0.074    5.552    0.000\n    speed             0.262    0.056    4.660    0.000\n  textual ~~                                          \n    speed             0.173    0.049    3.518    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.549    0.114    4.833    0.000\n   .x2                1.134    0.102   11.146    0.000\n   .x3                0.844    0.091    9.317    0.000\n   .x4                0.371    0.048    7.779    0.000\n   .x5                0.446    0.058    7.642    0.000\n   .x6                0.356    0.043    8.277    0.000\n   .x7                0.799    0.081    9.823    0.000\n   .x8                0.488    0.074    6.573    0.000\n   .x9                0.566    0.071    8.003    0.000\n    visual            0.809    0.145    5.564    0.000\n    textual           0.979    0.112    8.737    0.000\n    speed             0.384    0.086    4.451    0.000\n\n\nThe fit indices don’t look spectacular, so perhaps we can look into ways to improve the model. The modificationIndices() function shows the \\(\\chi^2\\) value associated with various possible modifications to the original fit.\n\nmodificationIndices(fit_hs) |&gt;\n  arrange(desc(mi))\n\n       lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n1   visual =~  x9 36.411  0.577   0.519    0.515    0.515\n2       x7 ~~  x8 34.145  0.536   0.536    0.859    0.859\n3   visual =~  x7 18.631 -0.422  -0.380   -0.349   -0.349\n4       x8 ~~  x9 14.946 -0.423  -0.423   -0.805   -0.805\n5  textual =~  x3  9.151 -0.272  -0.269   -0.238   -0.238\n6       x2 ~~  x7  8.918 -0.183  -0.183   -0.192   -0.192\n7  textual =~  x1  8.903  0.350   0.347    0.297    0.297\n8       x2 ~~  x3  8.532  0.218   0.218    0.223    0.223\n9       x3 ~~  x5  7.858 -0.130  -0.130   -0.212   -0.212\n10  visual =~  x5  7.441 -0.210  -0.189   -0.147   -0.147\n11      x1 ~~  x9  7.335  0.138   0.138    0.247    0.247\n12      x4 ~~  x6  6.220 -0.235  -0.235   -0.646   -0.646\n13      x4 ~~  x7  5.920  0.098   0.098    0.180    0.180\n14      x1 ~~  x7  5.420 -0.129  -0.129   -0.195   -0.195\n15      x7 ~~  x9  5.183 -0.187  -0.187   -0.278   -0.278\n16 textual =~  x9  4.796  0.138   0.137    0.136    0.136\n17  visual =~  x8  4.295 -0.210  -0.189   -0.187   -0.187\n18      x3 ~~  x9  4.126  0.102   0.102    0.147    0.147\n19      x4 ~~  x8  3.805 -0.069  -0.069   -0.162   -0.162\n20      x1 ~~  x2  3.606 -0.184  -0.184   -0.233   -0.233\n21      x1 ~~  x4  3.554  0.078   0.078    0.173    0.173\n22 textual =~  x8  3.359 -0.121  -0.120   -0.118   -0.118\n23  visual =~  x6  2.843  0.111   0.100    0.092    0.092\n24      x4 ~~  x5  2.534  0.186   0.186    0.457    0.457\n25      x2 ~~  x9  1.895  0.075   0.075    0.094    0.094\n26      x3 ~~  x6  1.855  0.055   0.055    0.100    0.100\n27   speed =~  x2  1.580 -0.198  -0.123   -0.105   -0.105\n28      x5 ~~  x7  1.233 -0.049  -0.049   -0.083   -0.083\n29  visual =~  x4  1.211  0.077   0.069    0.059    0.059\n30      x5 ~~  x9  0.999  0.040   0.040    0.079    0.079\n31      x1 ~~  x3  0.935 -0.139  -0.139   -0.203   -0.203\n32      x5 ~~  x6  0.916  0.101   0.101    0.253    0.253\n33      x2 ~~  x6  0.785  0.039   0.039    0.062    0.062\n34   speed =~  x3  0.716  0.136   0.084    0.075    0.075\n35      x3 ~~  x7  0.638 -0.044  -0.044   -0.054   -0.054\n36      x1 ~~  x8  0.634 -0.041  -0.041   -0.079   -0.079\n37      x2 ~~  x4  0.534 -0.034  -0.034   -0.052   -0.052\n38      x1 ~~  x5  0.522 -0.033  -0.033   -0.067   -0.067\n39      x5 ~~  x8  0.347  0.023   0.023    0.049    0.049\n40      x6 ~~  x8  0.275  0.018   0.018    0.043    0.043\n41   speed =~  x6  0.273  0.044   0.027    0.025    0.025\n42      x6 ~~  x7  0.259 -0.020  -0.020   -0.037   -0.037\n43   speed =~  x5  0.201 -0.044  -0.027   -0.021   -0.021\n44      x4 ~~  x9  0.196 -0.016  -0.016   -0.035   -0.035\n45      x3 ~~  x4  0.142 -0.016  -0.016   -0.028   -0.028\n46 textual =~  x7  0.098 -0.021  -0.021   -0.019   -0.019\n47      x6 ~~  x9  0.097 -0.011  -0.011   -0.024   -0.024\n48      x3 ~~  x8  0.059 -0.012  -0.012   -0.019   -0.019\n49      x2 ~~  x8  0.054 -0.012  -0.012   -0.017   -0.017\n50      x1 ~~  x6  0.048  0.009   0.009    0.020    0.020\n51      x2 ~~  x5  0.023 -0.008  -0.008   -0.011   -0.011\n52 textual =~  x2  0.017 -0.011  -0.011   -0.010   -0.010\n53   speed =~  x1  0.014  0.024   0.015    0.013    0.013\n54   speed =~  x4  0.003 -0.005  -0.003   -0.003   -0.003\n\n\nThese are some ways you could improve model fit, but here you want to be careful. Note that some cross-loadings are being suggested (e.g., visual =~ x9) but you might want to avoid these unless you have some theoretical reason for including them.\nMaybe what we would want to do instead would be to improve the model by allowing a few covariances between indicator variables. So let’s restrict the set of possibilities to these.\n\nmodificationIndices(fit_hs) |&gt;\n  filter(op == \"~~\") |&gt;   # the syntactic symbol for covariances\n  arrange(desc(mi)) |&gt;    # order them in terms of the modification index\n  slice(1:15)\n\n   lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n1   x7 ~~  x8 34.145  0.536   0.536    0.859    0.859\n2   x8 ~~  x9 14.946 -0.423  -0.423   -0.805   -0.805\n3   x2 ~~  x7  8.918 -0.183  -0.183   -0.192   -0.192\n4   x2 ~~  x3  8.532  0.218   0.218    0.223    0.223\n5   x3 ~~  x5  7.858 -0.130  -0.130   -0.212   -0.212\n6   x1 ~~  x9  7.335  0.138   0.138    0.247    0.247\n7   x4 ~~  x6  6.220 -0.235  -0.235   -0.646   -0.646\n8   x4 ~~  x7  5.920  0.098   0.098    0.180    0.180\n9   x1 ~~  x7  5.420 -0.129  -0.129   -0.195   -0.195\n10  x7 ~~  x9  5.183 -0.187  -0.187   -0.278   -0.278\n11  x3 ~~  x9  4.126  0.102   0.102    0.147    0.147\n12  x4 ~~  x8  3.805 -0.069  -0.069   -0.162   -0.162\n13  x1 ~~  x2  3.606 -0.184  -0.184   -0.233   -0.233\n14  x1 ~~  x4  3.554  0.078   0.078    0.173    0.173\n15  x4 ~~  x5  2.534  0.186   0.186    0.457    0.457\n\n\nThese are the top candidates for improving our model. First of these is to allow x7 to covary with x8. Let’s do that and check the improvement to the fit.\n\nmod_hs2 &lt;- '\nvisual  =~ x1 + x2 + x3\ntextual =~ x4 + x5 + x6\nspeed   =~ x7 + x8 + x9\n\n## included to improve model fit\nx7 ~~ x8\n'\n\nfit_hs2 &lt;- cfa(mod_hs2, data = HolzingerSwineford1939)\n\nsummary(fit_hs2, fit.measures=TRUE)\n\nlavaan 0.6-19 ended normally after 43 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                53.272\n  Degrees of freedom                                23\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.966\n  Tucker-Lewis Index (TLI)                       0.946\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3721.728\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7487.457\n  Bayesian (BIC)                              7569.013\n  Sample-size adjusted Bayesian (SABIC)       7499.242\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.066\n  90 Percent confidence interval - lower         0.043\n  90 Percent confidence interval - upper         0.090\n  P-value H_0: RMSEA &lt;= 0.050                    0.118\n  P-value H_0: RMSEA &gt;= 0.080                    0.175\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.047\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  visual =~                                           \n    x1                1.000                           \n    x2                0.576    0.098    5.898    0.000\n    x3                0.752    0.103    7.289    0.000\n  textual =~                                          \n    x4                1.000                           \n    x5                1.115    0.066   17.015    0.000\n    x6                0.926    0.056   16.682    0.000\n  speed =~                                            \n    x7                1.000                           \n    x8                1.244    0.194    6.414    0.000\n    x9                2.515    0.641    3.924    0.000\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x7 ~~                                               \n   .x8                0.353    0.067    5.239    0.000\n  visual ~~                                           \n    textual           0.400    0.073    5.511    0.000\n    speed             0.184    0.054    3.423    0.001\n  textual ~~                                          \n    speed             0.102    0.036    2.854    0.004\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .x1                0.576    0.101    5.678    0.000\n   .x2                1.122    0.100   11.171    0.000\n   .x3                0.832    0.087    9.552    0.000\n   .x4                0.372    0.048    7.791    0.000\n   .x5                0.444    0.058    7.600    0.000\n   .x6                0.357    0.043    8.287    0.000\n   .x7                1.036    0.090   11.501    0.000\n   .x8                0.795    0.080    9.988    0.000\n   .x9                0.088    0.188    0.466    0.641\n    visual            0.783    0.135    5.810    0.000\n    textual           0.978    0.112    8.729    0.000\n    speed             0.147    0.056    2.615    0.009\n\n\nYou will probably want to compare models to see if the inclusion is justified.\n\nanova(fit_hs2, fit_hs)\n\n\nChi-Squared Difference Test\n\n        Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit_hs2 23 7487.5 7569.0 53.272                                          \nfit_hs  24 7517.5 7595.3 85.305     32.033 0.32109       1  1.516e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYou could (in principal) continue this process until you get a satisfactory model fit. But let’s put this aside and have a look at another useful thing you can do.\nOne thing you might want to know is: what are the correlations between these various latent factors? The output gives use covariances, which are hard to interpret, because the variances of the latent factors are not standardized. Rather than using unit loading identification (ULI), we can switch to unit variance identification (UVI); in other words, rather than having the first loading scaled to one, we allow the loading to vary but fix the variance of our latent factors to 1. We can do this by adding std.lv=TRUE option to our call to cfa().\n\nfit_cfa2 &lt;- cfa(mod_hs2, data = HolzingerSwineford1939,\n                std.lv = TRUE)\n\n\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n .x7 ~~                                               \n   .x8                0.353    0.067    5.239    0.000\n  visual ~~                                           \n    textual           0.457    0.064    7.142    0.000\n    speed             0.544    0.078    6.965    0.000\n  textual ~~                                          \n    speed             0.270    0.065    4.141    0.000\n\n\nNow that we have this information, we can interpret the covariance between latent factors as correlations. We obtain small to medium positive correlations between all three factors: so, a correlation of 0.457 between visual and textual; of 0.544 between visual and speed; and of 0.27 between textual and speed.\n\n\n\n\nHolzinger, K. J., & Swineford, F. (1939). A study in factor analysis: The stability of a bi-factor solution. Supplementary Educational Monographs, xi + 91.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Confirmatory factor analysis</span>"
    ]
  }
]