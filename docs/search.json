[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Models in Psychology and Neuroscience: A Survey",
    "section": "",
    "text": "Preface\nThis course offers a survey of common approaches to statistical modeling found in psychology and neuroscience, with three key goals in mind: flexibility, generalizability, and reproducibility. We focus on analytical techniques that are flexible, in the sense that they can be adapted to different types of study designs with different types of data. We seek to structure analyses to support claims that are generalizable to the larger population of people (and possibly stimuli) from which our sample was drawn. Finally, we aim to analyse data in ways that are reproducible by writing plain text scripts in R. In this way, the full set of procedures have been documented in an unambiguous way to enable others (including our future selves!) to easily reproduce our analysis from the raw data to the results and perhaps even up to generating the research report.\nThis textbook accompanies a one-semester survey course whose aim is to introduce students to the most common analytical approaches in the field. But psychology and neuroscience are broad research areas with many different traditions and approaches, reflecting the complexity of the subject matter. Fortunately, all of these approaches are built on the foundation of linear regression. My hope is to provide a solid foundation in regression so that you know enough to go further on your own with any technique you are interested in. Be under no illusion that we give adequate treatment to techniques introduced in this textbook beyond multiple regression. For each approach, I provide guidance for learning more at the end of the corresponding chapter.\nThe textbook focuses on the practical implementation of concepts introduced in lectures of my course. The textbook itself does not provide much conceptual discussion. Nevertheless, it may still be useful for people who already understand these concepts but are looking to learn their implementation in R statistical programming.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Statistical Models in Psychology and Neuroscience: A Survey",
    "section": "About this book",
    "text": "About this book\nThe material in this course forms the basis for a one-semester course for third-year undergradautes taught by Dale Barr at the University of Glasgow School of Psychology and Neuroscience. It is part of the PsyTeachR series of course materials developed by University of Glasgow Psychology staff.\nThis textbook is meant to be interactive. Each chapter contains embedded exercises as well as web applications to help students better understand the content. The interactive content will only work if you access this material through a web browser. Printing out the material is not recommended.\n\nThe main tool that we will be using is the statistical programming environment R. To follow along with the code in this textbook you will need R version 4.2.0 or later. It is also recommended that you install the add-on packages tidyverse, lme4, psych, corrr, and lavaan.\nFor anyone starting out with R, you will need to choose an :Integrated Development Environment. For beginners, the RStudio Desktop is a good choice.\n\nHow to use this book\nThis book has ‘dark’ and ‘light’ modes that you can toggle between to suit your reading preferences. Look for the toggle switch next to where the book title is displayed in your browser.\nWhen discussing statistical modeling, some use of technical terminology is unavoidable. This book contains a glossary where you can find definitions of common terms. I have used the Nutshell web tool which allows you to “expand” definitions to appear within the body of the web page, to avoid flipping back and forth between the main text and the glossary. Whereever you see an underlined term that has two dots to the top left of it, like this term—:Nutshell—you can click on the term to expand the definition (try it!).\n\n\nHow to cite this book\nBarr, Dale J. (2024). Statistical models in psychology and neuroscience: A survey. Version 0.9.0. Retrieved from https://psyteachr.github.io/stat-models-v2.\n\n\n\nFree to re-use and remix!\nYou are free to re-use and modify the material in this textbook for your own purposes with the stipulation that you cite the original work. Please note additional terms of the Creative Commons CC-BY-SA 4.0 license governing re-use of this material.\n\n\nHow this book was made\nThis book was authored using the Quarto publishing system. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro-placeholder.html",
    "href": "intro-placeholder.html",
    "title": "1  Introduction",
    "section": "",
    "text": "(under construction)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html",
    "href": "corr-and-reg.html",
    "title": "2  Correlation and Regression",
    "section": "",
    "text": "2.1 Correlation\nA correlation coefficient quantifies the strength and direction of an association between two variables. It is usually represented by the symbol \\(r\\) or \\(\\rho\\) (Greek letter “rho”). The correlation coefficient ranges between -1 and 1, with 0 corresponding to no relationship, positive values reflecting a positive relationship (as one variable increases, so does the other), and negative values reflecting a negative relationship (as one variable increases, the other decreases).\nThe web app below shows 100 random points from a bivariate normal distribution for variables \\(X\\) and \\(Y\\). You can use the sliders to change the parameters of that distribution: the correlation (Greek symbol \\(\\rho\\), “rho”), the standard deviation for \\(X\\) (\\(\\sigma_X\\), “sigma X”), the standard deviation for \\(Y\\) (\\(\\sigma_Y\\)), the mean of \\(X\\) (\\(\\mu_X\\), “mu X”) and the mean of \\(Y\\) (\\(\\mu_Y\\), “mu Y”). These five parameters are all you need to characterize a bivariate normal distribution.\nPlay around with the sliders until you have a conceptual understanding of the various parameters. Use the “new sample” button to get a new set of 100 randomly-generated pairs, and use “reset” to reset the parameter values to their defaults.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html#correlation",
    "href": "corr-and-reg.html#correlation",
    "title": "2  Correlation and Regression",
    "section": "",
    "text": "2.1.1 Bivariate application\n\n\nThere are different correlation coefficients that make different assumptions or that allow you to work with different types of data. To start, let’s consider the one most commonly used: the Pearson product-moment correlation coefficient, which can be applied to :interval or :ratio scale data. When you hear someone talk about a “correlation” without further qualification, they are most likely talking about a Pearson correlation coefficient. If we have the variables \\(X\\) and \\(Y\\) then we might use \\(r_{XY}\\) or \\(\\rho_{XY}\\) as a symbol for the correlation.\nThere are multiple equivalent formulas for calculating a Pearson correlation coefficient. The most important one for us is\n\\[r_{XY} = \\frac{cov_{XY}}{S_X S_Y}\\]\nbecause it can offer some conceptual understanding. The quantity \\(cov_{XY}\\) is the covariance between two variables, \\(X\\) and \\(Y\\), which is defined as\n\\[cov_{XY} = \\frac{\\Sigma (X - \\bar{X})(Y - \\bar{Y})}{N}.\\]\nIn other words, it is the sum of the products of the deviation scores for each \\((X, Y)\\) pair of observations divided by the number of pairs.\nNote that the covariance of a variable with itself is\n\\[cov_{XX} = \\frac{\\Sigma (X - \\bar{X})(X - \\bar{X})}{N} = \\frac{\\Sigma (X - \\bar{X})^2}{N}\\]\nwhich is the formula for the variance of a variable. Taking the square root gives us the formula for the standard deviation:\n\\[SD_X = \\sqrt{\\frac{\\Sigma (X - \\bar{X})^2}{N}}.\\]\nThe above formulas are for calculating these statistics for the sample. When we want to estimate the corresponding parameters for the population we have sampled from, these formulas will have \\(N-1\\) instead of \\(N\\) in their denominators. The R functions cov(), var(), and sd() are used to compute these values on vectors or matrices.\n\n\n\n\n\n\nExercise: Calculate a correlation coefficient for variables in a data frame.\n\n\n\nThe data frame iris in R has measurements of different parts of 50 flowers from three different species of iris. See help(\"iris\") for more information about this dataset.\n\nhead(iris, 6)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nCalculate the correlation between the petal length and petal width of these flowers in R using the cov() and sd() functions in R.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\\[r_{XY} = \\frac{cov_{XY}}{S_X S_Y}\\]\ncov(iris$Petal.Length, iris$Petal.Width) # covariance\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncov(iris$Petal.Length, iris$Petal.Width) /\n  (sd(iris$Petal.Length) * sd(iris$Petal.Width))\n\n[1] 0.9628654\n\n\nLet’s compare this result with the result of the function cor().\n\ncor(iris$Petal.Length, iris$Petal.Width)\n\n[1] 0.9628654\n\n\n\n\n\n\n\n\n\n2.1.2 Covariance matrix\nCovariance matrices become very important once we start talking about multilevel modelling and multivariate analyses.\nA covariance matrix (also known as the variance-covariance matrix) is a mathematical structure that describes the spread of a multivariate distribution. It is multidimensional analogue of the standard deviation.\nTo fully describe a univariate (single variable) normal distribution, you need to know only two parameters: the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)). Mathematically, this is often represented as\n\\[X_i \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\\]\nwhich is read as, “each case \\(i\\) of the \\(X\\) variable is drawn from a normal distribution with mean of \\(\\mu\\) and variance \\(\\sigma^2\\).\nTo fully describe a bivariate normal distribution, you need five parameters: the means of each of the two variables, their standard deviations, and their correlation. The mathematical way to represent the idea that pairs of observations for case \\(i\\), (\\(X_i\\), \\(Y_i\\)) are drawn from a normal distribution is:\n\\[\\begin{pmatrix} X_i \\\\ Y_i\\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix}\\mu_X \\\\ \\mu_Y\\end{pmatrix}, \\mathbf{\\Sigma}\\right)\\]\nwhere \\(\\mathbf{\\Sigma}\\) is a covariance matrix, defined as\n\\[\\mathbf{\\Sigma} = \\begin{pmatrix}cov_{xx} & cov_{xy} \\\\ cov_{yx} & cov_{yy}\\end{pmatrix} = \\begin{pmatrix}\\rho_{xx}\\sigma_x\\sigma_x & \\rho_{xy}\\sigma_x\\sigma_y \\\\ \\rho_{yx}\\sigma_y\\sigma_x & \\rho_{yy}\\sigma_y\\sigma_y\\end{pmatrix}.\\]\nYou can see that the entries are just a rearrangement of the correlation formula\n\\[\\rho_{xy} = \\frac{cov_{xy}}{\\sigma_x \\sigma_y}.\\]\nwhere we rearrange to solve for \\(cov_{xy}\\)\n\\[cov_{xy} = \\rho_{xy}\\sigma_x \\sigma_y.\\]\nBecause the correlation of a variable with itself (\\(\\rho_{xx}\\), \\(\\rho_{yy}\\)) is always 1, we will often see \\(\\mathbf{\\Sigma}\\) written equivalently as\n\\[\\begin{pmatrix}{\\sigma_x}^2 & \\rho_{xy}\\sigma_x\\sigma_y \\\\ \\rho_{yx}\\sigma_y\\sigma_x & {\\sigma_y}^2\\end{pmatrix}.\\]\nNote two things about the above matrix: first, the variances appear in the main diagonal, and the covariance appear off the main diagonal. Second, a correlation is not directional, and so \\(\\rho_{xy} = \\rho_{yx}\\), which means that the two off-diagonal elements have the same value.\n\n\n\n\n\n\nWarning: \\(\\Sigma\\) doing double duty\n\n\n\nYou may have noticed that the Greek capital letter \\(\\Sigma\\) is used both to represent the instruction to add things together, e.g., \\(\\Sigma X\\) = sum(X), and now it’s being used as a symbol to represent a covariance matrix \\(\\mathbf{\\Sigma}\\). In the latter case, \\(\\Sigma\\) appears in boldface (compare \\(\\mathbf{\\Sigma}\\) to \\(\\Sigma\\)). Usually the context will make things clear.\n\n\n\n\n\n\n\n\nTell me more about matrices\n\n\n\n\n\nIn mathematics, matrices are just generalizations of the concept of a vector: a vector can be thought of as having one dimension, whereas a matrix can have any number of dimensions.\nSo the matrix\n\\[\n\\begin{pmatrix}\n1 & 4 & 7 \\\\\n2 & 5 & 8 \\\\\n3 & 6 & 9 \\\\\n\\end{pmatrix}\n\\]\nis a 3 (row) by 3 (column) matrix containing the column vectors \\(\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ \\end{pmatrix}\\), \\(\\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\\\ \\end{pmatrix}\\), and \\(\\begin{pmatrix} 7 \\\\ 8 \\\\ 9 \\\\ \\end{pmatrix}\\). Conventionally, we refer to matrices in \\(i\\) by \\(j\\) format, with \\(i\\) being the number of rows and \\(j\\) being the number of columns. So a 3x2 matrix has 3 rows and 2 columns, like so.\n\\[\n\\begin{pmatrix}\na & d \\\\\nb & e \\\\\nc & f \\\\\n\\end{pmatrix}\n\\]\nA square matrix is a matrix where the number of rows is equal to the number of columns.\n\n\n\n\n\n\n\n\n\nExercise: Generate a covariance matrix\n\n\n\nGenerate a covariance matrix corresponding to the relation between Petal.Width and Petal.Length for the iris dataset. Name your covariance matrix cvmx.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nr_wl &lt;- cor(iris$Petal.Width, iris$Petal.Length)\nsd_w &lt;- sd(iris$Petal.Width)\nsd_l &lt;- sd(iris$Petal.Length)\n\ncov_wl &lt;- r_wl * sd_w * sd_l\n\n## bind together rows of the matrix using rbind()\ncvmx &lt;- rbind(c(sd_w^2, cov_wl),\n              c(cov_wl, sd_l^2))\n\ncvmx\n\n          [,1]     [,2]\n[1,] 0.5810063 1.295609\n[2,] 1.2956094 3.116278",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html#regression",
    "href": "corr-and-reg.html#regression",
    "title": "2  Correlation and Regression",
    "section": "2.2 Regression",
    "text": "2.2 Regression\nWhen we have two variables, \\(X\\) and \\(Y\\), and our goal is to predict values of \\(Y\\) from \\(X\\), then we usually want to fit a linear regression model; or we might say, we want “to regress \\(Y\\) on \\(X\\).”\nA simple regression model is of the form:\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i.\\]\nIn this equation, \\(\\beta_0\\) and \\(\\beta_1\\) are the y-intercept and slope parameters respectively, and the \\(e_i\\)s are the errors (the part of each \\(Y_i\\) left unexplained by the model). It is conventionally assumed that the \\(e_i\\) values are from a normal distribution with mean of zero and variance \\(\\sigma^2\\); the math-y way of saying this is \\(e_i \\sim \\mathcal{N}(0, \\sigma^2)\\), where \\(\\sim\\) is read as “distributed according to” and \\(\\mathcal{N}(0, \\sigma^2)\\) means “Normal distribution (\\(\\mathcal{N}\\)) with mean of 0 and variance of \\(\\sigma^2\\)”.\n\n2.2.1 Scatterplot\nLet’s return to the iris data. Say we want to predict Petal.Length from Petal.Width. Before proceeding, it is a good idea to make a scatterplot of the data using ggplot2. We can use the colour aesthetic to capture what species a datapoint belongs to.\n\nlibrary(\"tidyverse\")\n\nggplot(iris,\n       aes(x = Petal.Width, y = Petal.Length, colour = Species)) +\n  geom_point()\n\n\n\n\n\n\n\nFigure 2.1: Scatterplot of the iris data.\n\n\n\n\n\nThe plot looks like we could nicely fit a line through the cloud of points. Let’s do that.\n\n\n2.2.2 Fitting a model using lm()\nWe use the lm() function (linear model) to fit regression models. It’s a good idea to look at the help file before using a function we’re unfamiliar with (type help(\"lm\") in the console).\nlm                    package:stats                    R Documentation\n\nFitting Linear Models\n\nDescription:\n\n     ‘lm’ is used to fit linear models, including multivariate ones.\n     It can be used to carry out regression, single stratum analysis of\n     variance and analysis of covariance (although aov may provide a\n     more convenient interface for these).\n\nUsage:\n\n     lm(formula, data, subset, weights, na.action,\n        method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,\n        singular.ok = TRUE, contrasts = NULL, offset, ...)\n     \n     ## S3 method for class 'lm'\n     print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)\n     \nArguments:\n\n formula: an object of class ‘\"formula\"’ (or one that can be coerced to\n          that class): a symbolic description of the model to be\n          fitted.  The details of model specification are given under\n          ‘Details’.\n\n    data: an optional data frame, list or environment (or object\n          coercible by as.data.frame to a data frame) containing the\n          variables in the model.  If not found in ‘data’, the\n          variables are taken from ‘environment(formula)’, typically\n          the environment from which ‘lm’ is called.\nUsually the first few arguments are the important ones we need to pay attention to. Here, the key ones are formula and data. We need to provide a symbolic description of the model. Under Details we see\nDetails:\n\n     Models for ‘lm’ are specified symbolically.  A typical model has\n     the form ‘response ~ terms’ where ‘response’ is the (numeric)\n     response vector and terms is a series of terms which specifies a\n     linear predictor for ‘response’.\nSo we specify the model using response ~ terms, but we omit the intercept and the error term of the model because they are always implied. We also don’t name the regression coefficients; they get given the same names as the predictor variables. Thus our formula \\(Y_i = \\beta_0 + \\beta_1 X_i + e_i\\) just becomes Petal.Length ~ Petal.Width. The data argument tells the function the name of the data frame where the variables can be found. The result of the call to lm() is a fitted model object. We want to store the result so that we can perform further computations on the object. So altogether, our call might look like the following.\n\nmod &lt;- lm(Petal.Length ~ Petal.Width, data = iris)\n\nNote that we chose the name mod, but this is arbitrary; we could have used any other variable name.\nTo see the results of the function, we use summary() on the model object.\n\nsummary(mod)\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.33542 -0.30347 -0.02955  0.25776  1.39453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.08356    0.07297   14.85   &lt;2e-16 ***\nPetal.Width  2.22994    0.05140   43.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4782 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n\nWe are most interested in the table of coefficients here, which is the table of the regression coefficients for our model, the y-intercept \\(\\beta_0\\) and the slope \\(\\beta_1\\). We get \\(\\hat{\\beta}_0 = 1.084\\) and \\(\\hat{\\beta}_1 = 2.230\\). So with every 1 unit increase in Petal.Width, Petal.Length increases by 2.230.\nWe can see these value in the output, but we can also pull them out using code with the coef() function, which returns a vector with their values.\n\ncoef(mod)\n\n(Intercept) Petal.Width \n   1.083558    2.229940 \n\n\nRecall that the errors in our model are from a normal distribution with a mean of zero and variance \\(\\sigma^2\\), stated mathematically as \\(e_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\). The “residual standard error” for the model tells us \\(\\hat{\\sigma}\\), the estimated value of \\(\\sigma\\). We can pull this out of the model object using the sigma() function.\n\nsigma(mod)\n\n[1] 0.4782058\n\n\n\n\n2.2.3 Plotting the model fit against the data\nIt’s usually a good idea to plot the model fit against the data to see how well we are doing. Let’s re-create our scatterplot but use geom_abline() to add in a line with the slope and y-intercept for the model, which we can get using coef().\n\nggplot(iris,\n       aes(x = Petal.Width, y = Petal.Length, colour = Species)) +\n  geom_point() +\n  geom_abline(slope = coef(mod)[\"Petal.Width\"],\n              intercept = coef(mod)[\"(Intercept)\"],\n              colour = \"pink\")\n\n\n\n\n\n\n\n\n\n\n2.2.4 Getting other properties of fitted model objects\nThere are three other functions that useful to know. Each of these takes the fitted model object as the first argument.\n\n\n\nfunction\ndescription\n\n\n\n\npredict()\ngenerate predictions from the model\n\n\nfitted()\nget fitted values from the model\n\n\nresiduals()\ncalculate residuals\n\n\n\nFitted values, denoted by \\(\\hat{Y}_i\\), are the predicted values for all the \\(X_i\\) in the data. Predicted values can be for any \\(X_i\\) values, even those not seen in the dataset.\nResiduals represent the error of prediction, and are defined as \\(Y_i - \\hat{Y}_i\\); i.e., the observed value for case \\(i\\) minus the fitted value for case \\(i\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html#relationship-between-correlation-and-regression",
    "href": "corr-and-reg.html#relationship-between-correlation-and-regression",
    "title": "2  Correlation and Regression",
    "section": "2.3 Relationship between correlation and regression",
    "text": "2.3 Relationship between correlation and regression\nWe can calculate regression coefficients from correlation statistics and vice versa. To get regression statistics from correlation statistics, along with the correlation coefficient we need means of X and Y (denoted by \\(\\mu_x\\) and \\(\\mu_y\\) respectively) and their standard deviations (\\(\\hat{\\sigma}_x\\) and \\(\\hat{\\sigma}_y\\)). Let’s see how we can compute regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\).\nFirst, the slope of the regression line \\(\\beta_1\\) equals the correlation coefficient \\(\\rho\\) times the ratio of the standard deviations of \\(Y\\) and \\(X\\).\n\\[\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\]\nIf you play around with the bivariate web app you can verify for yourself that this is the case.\nThe next thing to note is that for mathematical reasons, the regression line is guaranteed to go through the point corresponding to the mean of \\(X\\) and the mean of \\(Y\\), i.e., the point \\((\\mu_x, \\mu_y)\\). (You can think of the regression line “pivoting” around that point depending on the slope). You also know that \\(\\beta_0\\) is the y-intercept, the point where the line crosses the vertical axis at \\(X = 0\\). From this information, and the estimates above, can you figure out the value of \\(\\beta_0\\)?\nWell, for each unit increase in \\(X\\) you have a corresponding change of \\(\\beta_1\\) for \\(Y\\), and you know that the line goes through the points \\((\\mu_x, \\mu_y)\\) as well as the y-intercept \\((0, \\beta_0)\\).\nThink about stepping back unit-by-unit from \\(X = \\mu_x\\) to \\(X = 0\\). At \\(X = \\mu_x\\), \\(Y = \\mu_y\\). Each unit step you take backward in the X dimension, \\(Y\\) will drop by \\(\\beta_1\\) units. When you get to zero, \\(Y\\) will have dropped from \\(\\mu_y\\) to \\(\\mu_y - \\mu_x\\beta_1\\).\nSo the general solution is: \\(\\beta_0 = \\mu_y - \\mu_x\\beta_1\\).\nTo close, here are a few implications from the relationship between correlation and regression.\n\n\\(\\beta_1 = 0\\) is the same as \\(\\rho = 0\\).\n\\(\\beta_1 &gt; 0\\) implies \\(\\rho &gt; 0\\), since standard deviations can’t be negative.\n\\(\\beta_1 &lt; 0\\) implies \\(\\rho &lt; 0\\), for the same reason.\nRejecting the null hypothesis that \\(\\beta_1 = 0\\) is the same as rejecting the null hypothesis that \\(\\rho = 0\\). The p-values that you get for \\(\\beta_1\\) in lm() will be the same as the one you get for \\(\\rho\\) from cor.test().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html#two-approaches-for-simulating-bivariate-data",
    "href": "corr-and-reg.html#two-approaches-for-simulating-bivariate-data",
    "title": "2  Correlation and Regression",
    "section": "2.4 Two approaches for simulating bivariate data",
    "text": "2.4 Two approaches for simulating bivariate data\n\n2.4.1 Approach 1: Simulating from the covariance matrix\nYou can simulate data from the normal distribution using the function rnorm(). The function rnorm() allows you to specify the mean and standard deviation of a single variable. How do we simulate correlated variables?\nIt should be clear that you can’t just run rnorm() twice and combine the variables, because then you end up with two variables that are unrelated, i.e., with a correlation of zero.\nThe package MASS provides a function mvrnorm() which is the ‘multivariate’ version of rnorm (hence the function name, mv + rnorm, which makes it easy to remember.\n\n\n\n\n\n\nCaution\n\n\n\nThe MASS package comes pre-installed with R. But the only function you’ll probably ever want to use from MASS is mvrnorm(), so rather than load in the package using library(\"MASS\"), it is preferable to use MASS::mvrnorm(), especially as MASS and the dplyr package from tidyverse don’t play well together, due to both packages having the function select(). So if you load in MASS after you load in tidyverse, you’ll end up getting the MASS version of select() instead of the dplyr version. It will do your head in trying to figure out what is wrong with your code, so always use MASS::mvrnorm() without loading library(\"MASS\").\n\n\nHave a look at the documentation for the mvrnorm() function (type ?MASS::mvrnorm in the console).\nThere are three arguments to take note of:\n\n\n\n\n\n\n\narg\ndescription\n\n\n\n\nn\nthe number of samples required\n\n\nmu\na vector giving the means of the variables\n\n\nSigma\na positive-definite symmetric matrix specifying the covariance matrix of the variables.\n\n\n\nThe Sigma argument to MASS::mvrnorm() plays an analogous role to the sd argument in rnorm(); it specifies the spread for the two variables.\n\n\n\n\n\n\nExercise: Simulate bivariate data using MASS::mvrnorm().\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4.2 Approach 2: Simulating from the regression model\nAnother way you can simulated data is based on a regression model. This way is pretty straightforward because we just follow the \\(Y_i = \\beta_0 + \\beta_1 X_i + e_i\\) formula.\n\nfirst, simulate the \\(X_i\\) values\ncalculate the predicted value from the model\nadd random error to the prediction using rnorm() to yield \\(Y_i\\)\n\nLet’s use this approach to simulate data from the iris dataset. First, let’s get the coefficients and the estimate of \\(\\sigma\\).\n\niris_coef &lt;- coef(mod)\niris_sig &lt;- sigma(mod) \n\niris_coef\n\n(Intercept) Petal.Width \n   1.083558    2.229940 \n\niris_sig\n\n[1] 0.4782058\n\n\nWe want to start by creating a data frame containing the \\(X_i\\) values. But for this we need to know \\(\\mu_x\\) and \\(\\sigma_x\\).\n\niris_mu_x &lt;- mean(iris$Petal.Width)\niris_sd_x &lt;- sd(iris$Petal.Width)\n\nWe’ll create the \\(X_i\\) values in a vector first. We have to decide on the number of paired observations we want; let’s say 150, the same number as in the original dataset. Let’s set the seed before we do any random number generation.\n\nset.seed(1451)\n\niris_x &lt;- rnorm(150, mean = iris_mu_x, sd = iris_sd_x)\n\nNow let’s use the tibble() function (from tidyverse) to create the \\(X_i\\) values in a data frame, followed by the predicted value, y_hat, followed by the errors, e_i\n\nsim_iris &lt;- tibble(x_i = iris_x,\n                   y_hat = iris_coef[1] + iris_coef[2] * x_i,\n                   e_i = rnorm(150, mean = 0, sd = iris_sig),\n                   y_i = y_hat + e_i)\n\nLet’s have a look.\n\nggplot(sim_iris,\n       aes(x = x_i, y = y_i)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis looks fairly similar to the original data, but note that we haven’t taken into account the species of flower, so the points are spread evenly across the x dimension, unlike in Figure Figure 2.1 above. So our model is good, but not really complete.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Correlation and Regression</span>"
    ]
  },
  {
    "objectID": "multiple-reg.html",
    "href": "multiple-reg.html",
    "title": "3  Multiple regression",
    "section": "",
    "text": "3.1 An example: How to get a good grade in statistics\nLet’s look at some (made up, but realistic) data to see how we can use multiple regression to answer various study questions. In this hypothetical study, you have a dataset for 100 statistics students, which includes their final course grade (grade), the number of lectures each student attended (lecture, an integer ranging from 0-10), how many times each student clicked to download online materials (nclicks) and each student’s grade point average prior to taking the course, GPA, which ranges from 0 (fail) to 4 (highest possible grade).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiple-reg.html#an-example-how-to-get-a-good-grade-in-statistics",
    "href": "multiple-reg.html#an-example-how-to-get-a-good-grade-in-statistics",
    "title": "3  Multiple regression",
    "section": "",
    "text": "3.1.1 Data import and visualization\nLet’s load in the data grades.csv and have a look.\n\nlibrary(\"corrr\") # correlation matrices\nlibrary(\"tidyverse\")\n\ngrades &lt;- read_csv(\"data/grades.csv\", col_types = \"ddii\")\n\ngrades\n\n# A tibble: 100 × 4\n   grade   GPA lecture nclicks\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;\n 1  4     2.52      10     108\n 2  3.02  2.73       7      93\n 3  1.47  1.55       4      71\n 4  1.21  2.55      10     101\n 5  1.90  2.46       9      84\n 6  3.38  2.25       6      93\n 7  4     3.45       8     135\n 8  3.47  2.96       6     126\n 9  2.59  3.22       7     109\n10  1.87  2.64       7      74\n# ℹ 90 more rows\n\n\nFirst let’s look at all the pairwise correlations.\n\ngrades |&gt;\n  correlate() |&gt;\n  shave() |&gt;\n  fashion()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n     term grade  GPA lecture nclicks\n1   grade                           \n2     GPA   .44                     \n3 lecture   .15  .30                \n4 nclicks   .52  .61     .21        \n\n\n\npairs(grades)\n\n\n\n\n\n\n\nFigure 3.1: All pairwise relationships in the grades dataset.\n\n\n\n\n\n\n\n3.1.2 Estimation and interpretation\nTo estimate the regression coefficients (the \\(\\beta\\)s), we will use the lm() function. For a GLM with \\(m\\) predictors:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_m X_{mi} + e_i\n\\]\nThe call to base R’s lm() is\nlm(Y ~ X1 + X2 + ... + Xm, data)\nThe Y variable is your response variable, and the X variables are the predictor variables. Note that you don’t need to explicitly specify the intercept or residual terms; those are included by default.\nFor the current data, let’s predict grade from lecture and nclicks.\n\nmy_model &lt;- lm(grade ~ lecture + nclicks, grades)\n\nsummary(my_model)\n\n\nCall:\nlm(formula = grade ~ lecture + nclicks, data = grades)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.77648 -0.45633  0.04778  0.49755  1.54089 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.100047   0.609603  -1.805   0.0743 .  \nlecture      0.024888   0.045623   0.546   0.5867    \nnclicks      0.033941   0.005903   5.750 1.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7729 on 97 degrees of freedom\nMultiple R-squared:  0.272, Adjusted R-squared:  0.257 \nF-statistic: 18.12 on 2 and 97 DF,  p-value: 2.061e-07\n\n\nWe’ll often write the parameter symbol with a little hat on top to make clear that we are dealing with estimates from the sample rather than the (unknown) true population values. From above:\n\n\\(\\hat{\\beta}_0\\) = -1.1\n\\(\\hat{\\beta}_1\\) = 0.02\n\\(\\hat{\\beta}_2\\) = 0.03\n\nThis tells us that a person’s predicted grade is related to their lecture attendance and download rate by the following formula:\ngrade = -1.1 + 0.02 \\(\\times\\) lecture + 0.03 \\(\\times\\) nclicks\nBecause \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) are both positive, we know that higher values of lecture and nclicks are associated with higher grades.\nSo if you were asked, what grade would you predict for a student who attends 3 lectures and downloaded 70 times, you could easily figure that out by substituting the appropriate values.\ngrade = -1.1 + 0.02 \\(\\times\\) 3 + 0.03 \\(\\times\\) 70\nwhich equals\ngrade = -1.1 + 0.06 + 2.1\nand reduces to\ngrade = 1.06\n\n\n3.1.3 Predictions from the linear model using predict()\nIf we want to predict response values for new predictor values, we can use the predict() function in base R.\npredict() takes two main arguments. The first argument is a fitted model object (i.e., my_model from above) and the second is a data frame (or tibble) containing new values for the predictors.\n\n\n\n\n\n\nCaution\n\n\n\nYou need to include all of the predictor variables in the new table. You’ll get an error message if your tibble is missing any predictors. You also need to make sure that the variable names in the new table exactly match the variable names in the model.\n\n\nLet’s create a tibble with new values and try it out.\n\n## a 'tribble' is a way to make a tibble by rows,\n## rather than by columns. This is sometimes useful\nnew_data &lt;- tribble(~lecture, ~nclicks,\n                    3, 70,\n                    10, 130,\n                    0, 20,\n                    5, 100)\n\n\n\n\n\n\n\nNote\n\n\n\nThe tribble() function provides a way to build a tibble row by row, whereas with tibble() the table is built column by column.\nThe first row of the tribble() contains the column names, each preceded by a tilde (~).\nThis is sometimes easier to read than doing it row by row, although the result is the same. Consider that we could have made the above table using\n\nnew_data &lt;- tibble(lecture = c(3, 10, 0, 5),\n                   nclicks = c(70, 130, 20, 100))\n\n\n\nNow that we’ve created our table new_data, we just pass it to predict() and it will return a vector with the predictions for \\(Y\\) (grade).\n\npredict(my_model, new_data)\n\n        1         2         3         4 \n 1.350520  3.561225 -0.421218  2.418540 \n\n\nThat’s great, but maybe we want to line it up with the predictor values. We can do this by just adding it as a new column to new_data.\n\nnew_data |&gt;\n  mutate(predicted_grade = predict(my_model, new_data))\n\n# A tibble: 4 × 3\n  lecture nclicks predicted_grade\n    &lt;dbl&gt;   &lt;dbl&gt;           &lt;dbl&gt;\n1       3      70           1.35 \n2      10     130           3.56 \n3       0      20          -0.421\n4       5     100           2.42 \n\n\nWant to see more options for predict()? Check the help at ?predict.lm.\n\n\n3.1.4 Visualizing partial effects\nAs noted above the parameter estimates for each regression coefficient tell us about the partial effect of that variable; it’s effect holding all of the others constant. Is there a way to visualize this partial effect? Yes, you can do this using the predict() function, by making a table with varying values for the focal predictor, while filling in all of the other predictors with their mean values.\nFor example, let’s visualize the partial effect of lecture on grade holding nclicks constant at its mean value.\n\nnclicks_mean &lt;- grades |&gt; pull(nclicks) |&gt; mean()\n\n## new data for prediction\nnew_lecture &lt;- tibble(lecture = 0:10,\n                      nclicks = nclicks_mean)\n\n## add the predicted value to new_lecture\nnew_lecture2 &lt;- new_lecture |&gt;\n  mutate(grade = predict(my_model, new_lecture))\n\nnew_lecture2\n\n# A tibble: 11 × 3\n   lecture nclicks grade\n     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1       0    99.0  2.26\n 2       1    99.0  2.28\n 3       2    99.0  2.31\n 4       3    99.0  2.33\n 5       4    99.0  2.36\n 6       5    99.0  2.38\n 7       6    99.0  2.41\n 8       7    99.0  2.43\n 9       8    99.0  2.46\n10       9    99.0  2.48\n11      10    99.0  2.51\n\n\nNow let’s plot.\n\nggplot(grades, aes(lecture, grade)) + \n  geom_point() +\n  geom_line(data = new_lecture2)\n\n\n\n\n\n\n\nFigure 3.2: Partial effect of ‘lecture’ on grade, with nclicks at its mean value.\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nPartial effect plots only make sense when there are no interactions in the model between the focal predictor and any other predictor.\nThe reason is that when there are interactions, the partial effect of focal predictor \\(X_i\\) will differ across the values of the other variables it interacts with.\n\n\nNow can you visualize the partial effect of nclicks on grade?\nSee the solution at the bottom of the page.\n\n\n3.1.5 Standardizing coefficients\nOne kind of question that we often use multiple regression to address is, Which predictors matter most in predicting Y?\nNow, you can’t just read off the \\(\\hat{\\beta}\\) values and choose the one with the largest absolute value, because the predictors are all on different scales. To answer this question, you need to center and scale the predictors.\nRemember \\(z\\) scores?\n\\[\nz = \\frac{X - \\bar{X}}{S_x}\n\\]\nA \\(z\\) score represents the distance of a score \\(X\\) from the sample mean (\\(\\bar{X}\\)) in standard deviation units (\\(S_x\\)). So a \\(z\\) score of 1 means that the score is one standard deviation about the mean; a \\(z\\)-score of -2.5 means 2.5 standard deviations below the mean. \\(Z\\)-scores give us a way of comparing things that come from different populations by calibrating them to the standard normal distribution (a distribution with a mean of 0 and a standard deviation of 1).\nSo we re-scale our predictors by converting them to \\(z\\)-scores. This is easy enough to do.\n\ngrades2 &lt;- grades |&gt;\n  mutate(lecture_z = (lecture - mean(lecture)) / sd(lecture),\n         nclicks_z = (nclicks - mean(nclicks)) / sd(nclicks))\n\ngrades2\n\n# A tibble: 100 × 6\n   grade   GPA lecture nclicks lecture_z nclicks_z\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  4     2.52      10     108     1.72      0.670\n 2  3.02  2.73       7      93     0        -0.444\n 3  1.47  1.55       4      71    -1.72     -2.08 \n 4  1.21  2.55      10     101     1.72      0.150\n 5  1.90  2.46       9      84     1.15     -1.11 \n 6  3.38  2.25       6      93    -0.574    -0.444\n 7  4     3.45       8     135     0.574     2.68 \n 8  3.47  2.96       6     126    -0.574     2.01 \n 9  2.59  3.22       7     109     0         0.745\n10  1.87  2.64       7      74     0        -1.86 \n# ℹ 90 more rows\n\n\nNow let’s re-fit the model using the centered and scaled predictors.\n\nmy_model_scaled &lt;- lm(grade ~ lecture_z + nclicks_z, grades2)\n\nsummary(my_model_scaled)\n\n\nCall:\nlm(formula = grade ~ lecture_z + nclicks_z, data = grades2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.77648 -0.45633  0.04778  0.49755  1.54089 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.43370    0.07729  31.489  &lt; 2e-16 ***\nlecture_z    0.04332    0.07942   0.546    0.587    \nnclicks_z    0.45665    0.07942   5.750 1.04e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7729 on 97 degrees of freedom\nMultiple R-squared:  0.272, Adjusted R-squared:  0.257 \nF-statistic: 18.12 on 2 and 97 DF,  p-value: 2.061e-07\n\n\nThis tells us that nclicks_z has a relatively larger influence; for each standard deviation increase in this variable, grade increases by about 0.46.\nAnother common approach to standardization involves standardizing the response variable as well as the predictors, i.e., \\(z\\)-scoring the \\(Y\\) values as well as the \\(X\\) values. The relative rank order of the regression coefficients will be the same under this approach. The main difference would be that the coefficients will be expressed in standard deviation (\\(SD\\)) units of the response variable, rather than in raw units.\n\n\n\n\n\n\nMulticollinearity\n\n\n\nIn discussions about multiple regression you may hear concerns expressed about “multicollinearity”, which is a fancy way of referring to the existence of intercorrelations among the predictor variables. This is only a potential problem insofar as it potentially affects the interpretation of the effects of individual predictor variables. When predictor variables are correlated, \\(\\beta\\) values can change depending upon which predictors are included or excluded from the model, sometimes even changing signs. The key things to keep in mind about this are:\n\ncorrelated predictors are probably unavoidable in observational studies;\nregression does not assume that your predictor variables are independent from one another (in other words, finding correlations amongst your predictors is not itself a reason to question your model);\nwhen strong correlations are present, use caution in interpreting individual regression coefficients;\nthere is no known “remedy” for it, nor is it clear that any such remedy is desireable, and many so-called remedies do more harm than good.\n\nFor more information and guidance, see Vanhove (2021).\n\n\n\n\n3.1.6 Model comparison\nAnother common kind of question multiple regression is also used to address is of the form: Does some predictor or set of predictors of interest significantly impact my response variable over and above the effects of some control variables?\nFor example, we saw above that the model including lecture and nclicks was statistically significant, \\(F(2,\n97) =\n18.118\\), \\(p &lt; \\\\.001\\).\nThe null hypothesis for a regression model with \\(m\\) predictors is\n\\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_m = 0;\\]\nin other words, that all of the coefficients (except the intercept) are zero. If the null hypothesis is true, then the null model\n\\[Y_i = \\beta_0\\]\ngives just as good of a prediction as the model including all of the predictors and their coefficients. In other words, your best prediction for \\(Y\\) is just its mean (\\(\\mu_y\\)); the \\(X\\) variables are irrelevant. We rejected this null hypothesis, which implies that we can do better by including our two predictors, lecture and nclicks.\nBut you might ask: maybe its the case that better students get better grades, and the relationship between lecture, nclicks, and grade is just mediated by student quality. After all, better students are more likely to go to lecture and download the materials. So we can ask, are attendance and downloads associated with better grades above and beyond student ability, as measured by GPA?\nThe way we can test this hypothesis is by using model comparison. The logic is as follows. First, estimate a model containing any control predictors but excluding the focal predictors of interest. Second, estimate a model containing the control predictors as well as the focal predictors. Finally, compare the two models, to see if there is any statistically significant gain by including the predictors.\nHere is how you do this:\n\nm1 &lt;- lm(grade ~ GPA, grades) # control model\nm2 &lt;- lm(grade ~ GPA + lecture + nclicks, grades) # bigger model\n\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: grade ~ GPA\nModel 2: grade ~ GPA + lecture + nclicks\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     98 64.030                                \n2     96 56.114  2    7.9165 6.7718 0.001774 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe null hypothesis is that we are just as good predicting grade from GPA as we are predicting it from GPA plus lecture and nclicks. We will reject the null if adding these two variables leads to a substantial enough reduction in the residual sums of squares (RSS); i.e., if they explain away enough residual variance.\nWe see that this is not the case: \\(F(2, 96 ) = 6.772\\), \\(p = 0.002\\). So we don’t have evidence that lecture attendance and downloading the online materials is associated with better grades above and beyond student ability, as measured by GPA.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiple-reg.html#dealing-with-categorical-predictors",
    "href": "multiple-reg.html#dealing-with-categorical-predictors",
    "title": "3  Multiple regression",
    "section": "3.2 Dealing with categorical predictors",
    "text": "3.2 Dealing with categorical predictors\nA regression formula characterizes the response variable as the sum of weighted predictors. What if one of the predictors is categorical (e.g., representing groups such as “rural” or “urban”) rather than numeric? Many variables are nominal: a categorical variable containing names, for which there is no inherent ordering among the levels of the variable. Pet ownership (cat, dog, ferret) is a nominal variable; preferences aside, owning a cat is not greater than owning a dog, and owning a dog is not greater than owning a ferret.\n\n\n\n\n\n\nRepresenting nominal data using numeric predictors\n\n\n\nRepresenting a nominal variable with \\(k\\) levels in a regression model requires \\(k-1\\) numeric predictors; for instance, if you have four levels, you need three predictors. Most numerical coding schemes require that you choose one of these \\(k\\) levels as a baseline level. Each of your \\(k-1\\) variables contrasts one of the other levels level with the baseline.\nExample: You have a variable, pet_type with three levels (cat, dog, ferret).\nYou choose cat as the baseline, and create two numeric predictor variables:\n\ndog_v_cat to encode the contrast between dog and cat, and\nferret_v_cat to encode the contrast between ferret and cat.\n\n\n\nNominal variables are typically represented in a data frame as type character or factor.\nThe difference between a character and a factor variable is that factors contain information about the levels and their order, while character vectors lack this information.\nWhen you specify a model using the R formula syntax, R will check the data types of the predictors on the right hand side of the formula. For example, if your model regresses income on pet_type (e.g., income ~ pet_type), R checks the data type of pet_type.\nFor any variable of type character or factor, R will implicitly create a numeric predictor (or a set of predictors) to represent that variable in the model. There are different schemes available for creating numeric representations of nominal variables. The default in R is to use dummy (or ‘treatment’) coding (see below). Unfortunately, this default is unsuitable for many types of study designs in psychology, so I am going to recommend that you learn how to code your own predictor variables “by hand,” and that you make a habit of doing so.\n\n\n\n\n\n\nDon’t represent levels of a categorical variable with numbers!\n\n\n\nIn the above example, we had a variable pet_type with levels cat, dog, and ferret. Sometimes people represent the levels of a nominal variable with numbers, like so:\n\n1 for cat,\n2 for dog,\n3 for ferret.\n\nThis is a bad idea.\nFirst, the labeling is arbitrary and opaque and anyone attempting to use your data would not know which number goes with which category (and you could also forget!).\nEven worse, if you were to put this variable in as a predictor in a regression model, R would have no way of knowing your intention to use 1, 2, and 3 as arbitrary labels for groups, and would instead assume that pet_type is a measurement for which dogs are 1 unit greater than cats, and ferrets are 2 units greater than cats and 1 unit greater than dogs, which is nonsense!\nIt is far too easy to make this mistake, and difficult to catch if authors do not share their data and code. In 2016, a paper on religious affiliation and altruism in children that was published in Current Biology had to be retracted for just this kind of mistake.\nSo, don’t represent the levels of a nominal variable with numbers, except of course when you deliberately create predictor variables encoding the \\(k-1\\) contrasts needed to properly represent a nominal variable in a regression model.\nIf you get data where someone has done this, you can convert the problematic variable into a factor by using the factor() function.\n\n\n\n3.2.1 Dummy (a.k.a. “treatment”) coding\nFor a nominal variable with only two levels, choose one level as baseline, and create a new variable that is 0 whenever the level is baseline and 1 when it is the other level. The choice of baseline is arbitrary, and will affect only whether the coefficient is positive or negative, but not its magnitude, its standard error nor the associated p-value.\nTo illustrate, let’s gin up some fake data with a single two level categorical predictor.\n\nfake_data &lt;- tibble(Y = rnorm(10),\n                    group = rep(c(\"A\", \"B\"), each = 5))\n\nfake_data\n\n# A tibble: 10 × 2\n        Y group\n    &lt;dbl&gt; &lt;chr&gt;\n 1  1.05  A    \n 2 -0.599 A    \n 3  0.786 A    \n 4  0.704 A    \n 5 -2.17  A    \n 6  0.441 B    \n 7  0.466 B    \n 8  0.233 B    \n 9  0.901 B    \n10  1.57  B    \n\n\nNow let’s add a new variable, group_d, which is the dummy coded group variable. We will use the dplyr::if_else() function to define the new column.\n\nfake_data2 &lt;- fake_data |&gt;\n  mutate(group_d = if_else(group == \"B\", 1, 0))\n\nfake_data2\n\n# A tibble: 10 × 3\n        Y group group_d\n    &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1  1.05  A           0\n 2 -0.599 A           0\n 3  0.786 A           0\n 4  0.704 A           0\n 5 -2.17  A           0\n 6  0.441 B           1\n 7  0.466 B           1\n 8  0.233 B           1\n 9  0.901 B           1\n10  1.57  B           1\n\n\nNow we just run it as a regular regression model.\n\nsummary(lm(Y ~ group_d, fake_data2))\n\n\nCall:\nlm(formula = Y ~ group_d, data = fake_data2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.12378 -0.43725 -0.03857  0.81175  1.09425 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.04603    0.45834  -0.100    0.922\ngroup_d      0.76824    0.64819   1.185    0.270\n\nResidual standard error: 1.025 on 8 degrees of freedom\nMultiple R-squared:  0.1494,    Adjusted R-squared:  0.04303 \nF-statistic: 1.405 on 1 and 8 DF,  p-value: 0.2699\n\n\nLet’s reverse the coding. We get the same result, just the sign is different.\n\nfake_data3 &lt;- fake_data |&gt;\n  mutate(group_d = if_else(group == \"A\", 1, 0))\n\nsummary(lm(Y ~ group_d, fake_data3))\n\n\nCall:\nlm(formula = Y ~ group_d, data = fake_data3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.12378 -0.43725 -0.03857  0.81175  1.09425 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   0.7222     0.4583   1.576    0.154\ngroup_d      -0.7682     0.6482  -1.185    0.270\n\nResidual standard error: 1.025 on 8 degrees of freedom\nMultiple R-squared:  0.1494,    Adjusted R-squared:  0.04303 \nF-statistic: 1.405 on 1 and 8 DF,  p-value: 0.2699\n\n\nThe interpretation of the intercept is the estimated mean for the group coded as zero. You can see by plugging in zero for X in the prediction formula below. Thus, \\(\\beta_1\\) can be interpreted as the difference between the mean for the baseline group and the group coded as 1.\n\\[\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i \\]\nNote that if we just put the character variable group as a predictor in the model, R will automatically create a dummy variable (or variables) for us as needed.\n\nlm(Y ~ group, fake_data) |&gt;\n  summary()\n\n\nCall:\nlm(formula = Y ~ group, data = fake_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.12378 -0.43725 -0.03857  0.81175  1.09425 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.04603    0.45834  -0.100    0.922\ngroupB       0.76824    0.64819   1.185    0.270\n\nResidual standard error: 1.025 on 8 degrees of freedom\nMultiple R-squared:  0.1494,    Adjusted R-squared:  0.04303 \nF-statistic: 1.405 on 1 and 8 DF,  p-value: 0.2699\n\n\nThe lm() function examines group and figures out the unique levels of the variable, which in this case are A and B. It then chooses as baseline the level that comes first alphabetically, and encodes the contrast between the other level (B) and the baseline level (A). (In the case where group has been defined as a factor, the baseline level is the first element of levels(fake_data$group)).\nThis new variable that it created shows up with the name groupB in the output.\n\n\n3.2.2 Dummy coding when \\(k &gt; 2\\)\nWhen a nominal predictor variable has more than two levels (\\(k &gt; 2\\)), one numeric predictor is no longer sufficient; we need \\(k-1\\) predictors. If the nominal predictor has four levels, we’ll need to define three predictors. Let’s simulate some data to work with, season_wt, which represents a person’s bodyweight (in kg) over the four seasons of the year.\n\nseason_wt &lt;- tibble(season = rep(c(\"winter\", \"spring\", \"summer\", \"fall\"),\n                                 each = 5),\n                    bodyweight_kg = c(rnorm(5, 105, 3),\n                                      rnorm(5, 103, 3),\n                                      rnorm(5, 101, 3),\n                                      rnorm(5, 102.5, 3)))\n\nseason_wt\n\n# A tibble: 20 × 2\n   season bodyweight_kg\n   &lt;chr&gt;          &lt;dbl&gt;\n 1 winter          99.9\n 2 winter         103. \n 3 winter         111. \n 4 winter         102. \n 5 winter         106. \n 6 spring          98.3\n 7 spring         103. \n 8 spring         106. \n 9 spring         109. \n10 spring         101. \n11 summer         103. \n12 summer         102. \n13 summer          97.0\n14 summer          96.4\n15 summer         103. \n16 fall           104. \n17 fall           101. \n18 fall           103. \n19 fall            96.3\n20 fall           101. \n\n\nNow let’s add three predictors to code the variable season. Try it out and see if you can figure out how it works.\n\n## baseline value is 'winter'\nseason_wt2 &lt;- season_wt |&gt;\n  mutate(spring_v_winter = if_else(season == \"spring\", 1, 0),\n         summer_v_winter = if_else(season == \"summer\", 1, 0),\n         fall_v_winter = if_else(season == \"fall\", 1, 0))\n\nseason_wt2\n\n# A tibble: 20 × 5\n   season bodyweight_kg spring_v_winter summer_v_winter fall_v_winter\n   &lt;chr&gt;          &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1 winter          99.9               0               0             0\n 2 winter         103.                0               0             0\n 3 winter         111.                0               0             0\n 4 winter         102.                0               0             0\n 5 winter         106.                0               0             0\n 6 spring          98.3               1               0             0\n 7 spring         103.                1               0             0\n 8 spring         106.                1               0             0\n 9 spring         109.                1               0             0\n10 spring         101.                1               0             0\n11 summer         103.                0               1             0\n12 summer         102.                0               1             0\n13 summer          97.0               0               1             0\n14 summer          96.4               0               1             0\n15 summer         103.                0               1             0\n16 fall           104.                0               0             1\n17 fall           101.                0               0             1\n18 fall           103.                0               0             1\n19 fall            96.3               0               0             1\n20 fall           101.                0               0             1\n\n\n\n\n\n\n\n\nReminder: Always look at your data\n\n\n\nWhenever you write code that potentially changes your data, you should double check that the code works as intended by looking at your data. This is especially the case when you are hand-coding nominal variables for use in regression, because sometimes the code will be wrong, but won’t throw an error.\nConsider the code chunk above, where we defined three contrasts to represent the nominal variable season, with winter as our baseline.\nWhat would happen if you accidently misspelled one of the levels (summre for summer) and didn’t notice?\n\nseason_wt3 &lt;- season_wt |&gt;\n  mutate(spring_v_winter = if_else(season == \"spring\", 1, 0),\n         summer_v_winter = if_else(season == \"summre\", 1, 0),\n         fall_v_winter = if_else(season == \"fall\", 1, 0))\n\nWhile the above code chunk runs, we get confusing output when we run the regression; namely, the coefficent for summer_v_winter is NA (not available).\n\nlm(bodyweight_kg ~ spring_v_winter + summer_v_winter + fall_v_winter,\n   season_wt3)\n\n\nCall:\nlm(formula = bodyweight_kg ~ spring_v_winter + summer_v_winter + \n    fall_v_winter, data = season_wt3)\n\nCoefficients:\n    (Intercept)  spring_v_winter  summer_v_winter    fall_v_winter  \n       102.4585           0.7484               NA          -1.5751  \n\n\nWhat happened? Let’s look at the data to find out. We will use distinct to find the distinct combinations of our original variable season with the three variables we created (see ?dplyr::distinct for details).\n\nseason_wt3 |&gt;\n  distinct(season, spring_v_winter, summer_v_winter, fall_v_winter)\n\n# A tibble: 4 × 4\n  season spring_v_winter summer_v_winter fall_v_winter\n  &lt;chr&gt;            &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n1 winter               0               0             0\n2 spring               1               0             0\n3 summer               0               0             0\n4 fall                 0               0             1\n\n\nBecause of our misspelling, the predictor summer_v_winter is not 1 when season == \"summer\"; instead, it is always zero. The if_else() above literally says ‘set summer_v_winter to 1 if season == \"summre\", otherwise 0’. Of course, season is never equal to summre, because summre is a typo. We could have caught this easily by running the above check with distinct(). Get in the habit of doing this when you create your own numeric predictors.\n\n\n\n\n\n\n\n\nA closer look at R’s defaults\n\n\n\nIf you’ve ever used point-and-click statistical software like SPSS, you probably never had to learn about coding categorical predictors. Normally, the software recognizes when a predictor is categorical and, behind the scenes, it takes care of recoding it into a numerical predictor. R is no different: if you supply a predictor of type character or factor to a linear modeling function, it will create numerical dummy-coded predictors for you, as shown in the code below.\n\nlm(bodyweight_kg ~ season, season_wt) |&gt;\n  summary()\n\n\nCall:\nlm(formula = bodyweight_kg ~ season, data = season_wt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9267 -2.8710 -0.1012  2.6219  6.6669 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  100.8834     1.6950  59.519   &lt;2e-16 ***\nseasonspring   2.3235     2.3971   0.969    0.347    \nseasonsummer  -0.4633     2.3971  -0.193    0.849    \nseasonwinter   3.6135     2.3971   1.507    0.151    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.79 on 16 degrees of freedom\nMultiple R-squared:  0.1956,    Adjusted R-squared:  0.04483 \nF-statistic: 1.297 on 3 and 16 DF,  p-value: 0.3096\n\n\nHere, R implicitly creates three dummy variables to code the four levels of season, called seasonspring, seasonsummer and seasonwinter. The unmentioned season, fall, has been chosen as baseline because it comes earliest in the alphabet. These three predictors have the following values:\n\nseasonspring: 1 if spring, 0 otherwise;\nseasonsummer: 1 if summer, 0 otherwise;\nseasonwinter: 1 if winter, 0 otherwise.\n\nThis seems like a handy thing to have R do for us, but dangers lurk in relying on the default. We’ll learn more about these dangers in the next chapter when we talk about interactions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "multiple-reg.html#equivalence-between-multiple-regression-and-one-way-anova",
    "href": "multiple-reg.html#equivalence-between-multiple-regression-and-one-way-anova",
    "title": "3  Multiple regression",
    "section": "3.3 Equivalence between multiple regression and one-way ANOVA",
    "text": "3.3 Equivalence between multiple regression and one-way ANOVA\nIf we wanted to see whether our bodyweight varies over season, we could do a one way ANOVA on season_wt2 like so.\n\n## make season into a factor with baseline level 'winter'\nseason_wt3 &lt;- season_wt2 |&gt;\n  mutate(season = factor(season, levels = c(\"winter\", \"spring\",\n                                            \"summer\", \"fall\")))\n\nmy_anova &lt;- aov(bodyweight_kg ~ season, season_wt3)\nsummary(my_anova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nseason       3   55.9   18.63   1.297   0.31\nResiduals   16  229.8   14.37               \n\n\nOK, now can we replicate that result using the regression model below?\n\\[Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\]\n\nsummary(lm(bodyweight_kg ~ spring_v_winter +\n             summer_v_winter + fall_v_winter,\n           season_wt2))\n\n\nCall:\nlm(formula = bodyweight_kg ~ spring_v_winter + summer_v_winter + \n    fall_v_winter, data = season_wt2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.9267 -2.8710 -0.1012  2.6219  6.6669 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      104.497      1.695  61.651   &lt;2e-16 ***\nspring_v_winter   -1.290      2.397  -0.538    0.598    \nsummer_v_winter   -4.077      2.397  -1.701    0.108    \nfall_v_winter     -3.614      2.397  -1.507    0.151    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.79 on 16 degrees of freedom\nMultiple R-squared:  0.1956,    Adjusted R-squared:  0.04483 \nF-statistic: 1.297 on 3 and 16 DF,  p-value: 0.3096\n\n\nNote that the \\(F\\) values and \\(p\\) values are identical for the two methods!\n\n\n\n\n\n\nSolution to partial effect plot\n\n\n\n\n\nFirst create a tibble with new predictors. We might also want to know the range of values that nclicks varies over.\n\nlecture_mean &lt;- grades |&gt; pull(lecture) |&gt; mean()\nmin_nclicks &lt;- grades |&gt; pull(nclicks) |&gt; min()\nmax_nclicks &lt;- grades |&gt; pull(nclicks) |&gt; max()\n\n## new data for prediction\nnew_nclicks &lt;- tibble(lecture = lecture_mean,\n                      nclicks = min_nclicks:max_nclicks)\n\n## add the predicted value to new_lecture\nnew_nclicks2 &lt;- new_nclicks |&gt;\n  mutate(grade = predict(my_model, new_nclicks))\n\nnew_nclicks2\n\n# A tibble: 66 × 3\n   lecture nclicks grade\n     &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;\n 1       7      71  1.48\n 2       7      72  1.52\n 3       7      73  1.55\n 4       7      74  1.59\n 5       7      75  1.62\n 6       7      76  1.65\n 7       7      77  1.69\n 8       7      78  1.72\n 9       7      79  1.76\n10       7      80  1.79\n# ℹ 56 more rows\n\n\nNow plot.\n\nggplot(grades, aes(nclicks, grade)) +\n  geom_point() +\n  geom_line(data = new_nclicks2)\n\n\n\n\n\n\n\nFigure 3.3: Partial effect plot of nclicks on grade.\n\n\n\n\n\n\n\n\n\n\n\n\nVanhove, J. (2021). Collinearity isn’t a disease that needs curing. Meta-Psychology, 5.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple regression</span>"
    ]
  },
  {
    "objectID": "interactions-placeholder.html",
    "href": "interactions-placeholder.html",
    "title": "4  Interactions",
    "section": "",
    "text": "(under construction)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactions</span>"
    ]
  },
  {
    "objectID": "mixed-effects-placeholder.html",
    "href": "mixed-effects-placeholder.html",
    "title": "5  Multilevel models",
    "section": "",
    "text": "(under construction)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Multilevel models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Stevens, S. S. (1946). On the theory of scales of measurement.\nScience, 103(2684), 677–680. https://doi.org/10.1126/science.103.2684.677\n\n\nVanhove, J. (2021). Collinearity isn’t a disease that needs curing.\nMeta-Psychology, 5.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "criterion\nThe :response variable, usually used in the regression context to refer to the variable whose value we are trying to predict. Conventionally, this variable appears on the left side of the regression equation. For example, in the simple regression equation\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\]\nthe variable \\(Y_i\\) is the response variable.\nSee also :predictor variable.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#dependent-variable",
    "href": "glossary.html#dependent-variable",
    "title": "Glossary",
    "section": "dependent variable",
    "text": "dependent variable\nThe variable in an analysis corresponding to an outcome that we are interested in, often abbreviated as “DV”. The value of the DV is assumed to be related in some way to the value of an :independent variable or set of independent variables.\nConventionally, researchers use the more general term :response variable or :criterion, with “dependent variable” usually referring to the response variable in an experiment.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#ide",
    "href": "glossary.html#ide",
    "title": "Glossary",
    "section": "integrated development environment (IDE)",
    "text": "integrated development environment (IDE)\nAn integrated development environment or IDE is software that you use to develop computer programs. In the context of data analysis we often refer to the plain text files containing the programming instructions as a “script”. The IDE includes things like a script editor and features like syntax highlighting that make it easier to develop scripts.\nIt is always important to distinguish the software that provides the programming environment (e.g., RStudio Desktop, created by the for-profit company Posit) from the software that performs the computations (e.g., R, created for free by numerous volunteers). The scripts you develop are (ideally) independent of the IDE.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#independent-variable",
    "href": "glossary.html#independent-variable",
    "title": "Glossary",
    "section": "independent variable",
    "text": "independent variable\nA variable whose value is assumed to affect the value of a :dependent variable, abbreviated as “IV”. In a randomized experiment, the value of the IV is manipulated by the experimenter. Similar to a :predictor variable, but usually used in the context of randomized experiments.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#deviation-score",
    "href": "glossary.html#deviation-score",
    "title": "Glossary",
    "section": "deviation score",
    "text": "deviation score\nA transformed score, where the mean is subtracted from the original score. If \\(X\\) is a variable, then the deviation score is \\(X - \\bar{X}\\), where \\(\\bar{X}\\) is the :mean of \\(X\\).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#fixed-factor",
    "href": "glossary.html#fixed-factor",
    "title": "Glossary",
    "section": "fixed factor",
    "text": "fixed factor\nTODO\nSee also :random factor",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#interval-scale",
    "href": "glossary.html#interval-scale",
    "title": "Glossary",
    "section": "interval scale of measurement",
    "text": "interval scale of measurement\nOne of the four basic :scales of measurement. Values measured on an interval scale are numerical, quantitative values where differences between measurements represent meaningful ‘distances’ on the scale. Unlike the :ratio scale, interval scales lack a true zero corresponding to the absence of the quantity being measured. Instead, the zero point is determined by convention. A common example is temperature, where zero does not reflect an absence of temperature but is defined more arbitrarily (e.g., zero degrees Centigrade is different from zero degrees Farenheit).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#linear-mixed-effects-model",
    "href": "glossary.html#linear-mixed-effects-model",
    "title": "Glossary",
    "section": "linear mixed-effects model",
    "text": "linear mixed-effects model\nTODO",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#mean",
    "href": "glossary.html#mean",
    "title": "Glossary",
    "section": "mean",
    "text": "mean\nA measure of the central tendency of a variable, calculated by summing up the values and then dividing by \\(N\\), the number of values. The mean of a set of observations is often notated by putting a horizontal bar over the symbol for the variable; e.g., for variable \\(X\\), the mean would be notated as \\(\\bar{X}\\). In contrast, population means are often denoted using the greek letter \\(\\mu\\) (“mu”) with the variable name as a subscript, e.g., \\(\\mu_x\\).\nThe mean is calculated using the formula:\n\\[\\bar{X} = \\frac{\\Sigma X}{N}\\]\nwhere \\(N\\) is the number of observations, and \\(\\Sigma\\) is an instruction to sum together all of the \\(X\\) values.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#multilevel",
    "href": "glossary.html#multilevel",
    "title": "Glossary",
    "section": "multilevel",
    "text": "multilevel\nWe say that a dataset is multilevel when there are multiple measurements on the dependent variable for a given sampling unit. Usually, the sampling unit is a participant in a research study.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#multivariate",
    "href": "glossary.html#multivariate",
    "title": "Glossary",
    "section": "multivariate",
    "text": "multivariate\nWe say that a dataset is multivariate when there are multiple dependent variables rather than a single dependent variable.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#nominal-scale",
    "href": "glossary.html#nominal-scale",
    "title": "Glossary",
    "section": "nominal scale of measurement",
    "text": "nominal scale of measurement\nOne of the four basic :scales of measurement. Values measured on a nominal scale consist of discrete, unordered categories. “Scottish political party affiliation” is an example of a nominal variable, which (in 2025) might be given the following levels: Conservative, Labour, SNP, Lib Dem, Green, Reform, Alba, Other. Note that the levels correspond to discrete categories rather than numeric values, and that there is no intrinsic ordering among them. An easy way to remember this is that “nominal” comes from the Latin “nomen” which means name. Nominal data contrasts with :ordinal data, where in the later case there is an intrinsic ordering to the categories.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#Nutshell",
    "href": "glossary.html#Nutshell",
    "title": "Glossary",
    "section": "Nutshell",
    "text": "Nutshell\nNutshell is a web tool developed by Nicky Case to make expandable, embeddable explanations within a web page, like this one. A Nutshell definition appears with a dashed underline and two dots to the upper left. When you click on the link, the definition of the term will appear embedded within the page. Click the link again (or the little X at the bottom) to remove the definition.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#ordinal-scale",
    "href": "glossary.html#ordinal-scale",
    "title": "Glossary",
    "section": "ordinal scale",
    "text": "ordinal scale\nOne of the four basic :scales of measurement. Ordinal data is like :nominal data, except there is an intrinsic rank ordering among the categories, but the distance between the ranks may not be (psychologically) equal, unlike with :interval data. An example would be a Likert agreement scale with five categories: Strongly Agree, Somewhat Agree, Neither Agree Nor Disagree, Somewhat Disagree, Strongly Disagree.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#predictor-variable",
    "href": "glossary.html#predictor-variable",
    "title": "Glossary",
    "section": "predictor variable",
    "text": "predictor variable\nA variable in a regression whose value is used to predict the value of the :response variable, often in tandem with other predictor variables. For example, in the simple regression equation\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\]\nthe variable \\(X_i\\) is a predictor variable.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#p-value",
    "href": "glossary.html#p-value",
    "title": "Glossary",
    "section": "\\(p\\)-value",
    "text": "\\(p\\)-value\nA probability associated with a test statistic; specifically, the probability, assuming the null hypothesis is true, of obtaining a test statistic at least as extreme as, or more extreme than, the one observed.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#power",
    "href": "glossary.html#power",
    "title": "Glossary",
    "section": "power",
    "text": "power\nThe probability of rejecting the null hypothesis when it is false, for a specific analysis, effect size, sample size, and significance level.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#random-factor",
    "href": "glossary.html#random-factor",
    "title": "Glossary",
    "section": "random factor",
    "text": "random factor\nTODO\nSee also :fixed factor.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#ratio-scale-of-measurement-ratio-scale",
    "href": "glossary.html#ratio-scale-of-measurement-ratio-scale",
    "title": "Glossary",
    "section": "ratio scale of measurement (#ratio-scale)",
    "text": "ratio scale of measurement (#ratio-scale)\nOne of the four basic :scales of measurement. Ratio data, like :interval data is quantitative in nature and with a consistent distance between units, but unlike interval data, it has a true zero representing the absence of the quantity being measure. Response time, which is a measure of duration, is an example of ratio scale data, where ‘zero’ is a theoretically possible value, meaning that a response was made instantaneously (although such a fast response time is unattainable in practice).",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#regression-coefficient",
    "href": "glossary.html#regression-coefficient",
    "title": "Glossary",
    "section": "regression coefficient",
    "text": "regression coefficient\nA parameter in a regression equation, whose true (i.e., population) value is usually estimated from the sample. Each predictor in a model is weighted by an associated regression coefficient. For example, in the simple regression equation\n\\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\]\n\\(\\beta_0\\) and \\(\\beta_1\\) are both regression coefficients.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#response-variable",
    "href": "glossary.html#response-variable",
    "title": "Glossary",
    "section": "response variable",
    "text": "response variable\nThe outcome variable in a regression. Used interchangeably with :criterion variable.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#scales-of-measurement",
    "href": "glossary.html#scales-of-measurement",
    "title": "Glossary",
    "section": "scales of measurement",
    "text": "scales of measurement\nA typology introduced by Stevens (1946) of four types of measurement scales found in psychology: :nominal, :interval, :ratio, and :ordinal.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "glossary.html#standard-deviation",
    "href": "glossary.html#standard-deviation",
    "title": "Glossary",
    "section": "standard deviation",
    "text": "standard deviation\nA measure of variability, defined as the mean of the :deviation scores.\n\n\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677–680. https://doi.org/10.1126/science.103.2684.677",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Notation",
    "section": "",
    "text": "blah!!",
    "crumbs": [
      "Notation"
    ]
  }
]